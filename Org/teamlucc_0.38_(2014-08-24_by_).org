+ Package: teamlucc
+ Version: 0.38
+ Date: 2014-08-24
+ Title: TEAM land use and cover change data processing toolkit
+ Authors@R: c(person("Alex", "Zvoleff", email="azvoleff@conservation.org",
+     role=c("aut", "cre")), person("Sarah", "Goslee", role = "ctb"),
+     person("Xiaolin", "Zhu", role = "ctb"))
+ Maintainer: Alex Zvoleff <azvoleff@conservation.org>
+ Depends:
+     R (>= 2.10.0),
+     Rcpp (>= 0.11.0),
+     methods,
+     raster (>= 2.2-6)
+ Imports:
+     sp,
+     foreach,
+     iterators,
+     glcm,
+     wrspathrow,
+     rgeos,
+     rgdal,
+     mgcv,
+     dplyr,
+     lmodel2,
+     reshape2,
+     ggplot2,
+     stringr,
+     kernlab,
+     e1071,
+     randomForest,
+     SDMTools,
+     grid,
+     spatial.tools,
+     RCurl,
+     gdalUtils,
+     caret,
+     maptools,
+     mclust,
+     lubridate,
+     XML
+ Suggests:
+     testthat,
+     landsat
+ LinkingTo: Rcpp, RcppArmadillo
+ SystemRequirements: To perform gap filling of Landsat 7 SLC-off images or
+     Landsat scenes with heavy clouds using the IDL code by Xiaolin Zhu at the
+     Ohio State University requires licenses for both EXELIS IDL and ENVI, and
+     ENVI version 5 or greater.
+ Description: teamlucc is a set of routines to support analyzing land use and
+     cover change (LUCC) in R. The package was designed to support analyzing
+     LUCC in the Zone of Interaction (ZOIs) of monitoring sites in the Tropical
+     Ecology Assessment and Monitoring (TEAM) Network.
+ License: GPL (>= 3)
+ URL: https://www.azvoleff.com/teamlucc
+ BugReports: https://github.com/azvoleff/teamlucc/issues
+ LazyData: true
* accuracy.R
 #+BEGIN_SRC R 
#' A class for representing accuracy assessment results
#' @exportClass accuracy
#' @rdname accuracy-class
#' @slot ct a simple sample contingency table
#' @slot pop_ct a population contingency table (if \code{pop} was provided - 
#' see \code{\link{accuracy}})
#' @slot Q quantity disagreement
#' @slot A allocation disagreement
#' @slot n_test the number of samples
#' @slot pop the population of each class as a numeric
#' @import methods
setClass('accuracy', slots=c(ct='table', pop_ct='table', Q='numeric', 
                             A='numeric', n_test='numeric',
                             pop='numeric')
)
#' @export
#' @method summary accuracy
summary.accuracy <- function(object, ...) {
    obj = list()
    obj[['class']] <- class(object)
    obj[['Q']] <- object@Q
    obj[['A']] <- object@A
    obj[['ct']] <- object@ct
    obj[['pop_ct']] <- object@pop_ct
    obj[['n_test']] <- object@n_test
    margined_pop_ct <- .add_ct_margins(object@pop_ct)
    obj[['overall_acc']] <- margined_pop_ct[length(margined_pop_ct)]
    class(obj) <- 'summary.accuracy'
    obj
}
#' @export
#' @method print summary.accuracy
print.summary.accuracy <- function(x, ...) {
    cat(paste('Object of class ', x[['class']], '\n', sep = ''))
    cat('\n')
    cat(paste('Testing samples:\t', x[['n_test']], '\n', sep = ''))
    cat('\n')
    cat('Sample contingency table:\n')
    print(.add_ct_margins(x[['ct']]))
    cat('\n')
    cat('Population contingency table:\n')
    print(.add_ct_margins(x[['pop_ct']]))
    cat('\n')
    cat(paste(Overall accuracy:\t, round(x[['overall_acc']], digits=4), \n, sep = ))
    cat('\n')
    cat(paste('Quantity disagreement:\t\t', round(x[['Q']], digits=4), '\n', sep = ''))
    cat(paste('Allocation disagreement:\t', round(x[['A']], digits=4), '\n', sep = ''))
    invisible(x)
}
#' @export
#' @method print accuracy
print.accuracy <- function(x, ...) {
    print(summary(x, ...))
}
setMethod(show, signature(object=accuracy), function(object) print(object))
#' A class for error adjusted class areas
#'
#' @seealso \code{\link{adj_areas}}.
#' @import methods
#' @exportClass error_adj_area
setClass('error_adj_area', slots=c(adj_area_mat='matrix'))
#' Calculated adjusted class areas for an image classification
#'
#' Calculates the adjusted areas of each class in an image after taking account 
#' of omission and commission errors. For unbiased adjustments, error rates 
#' should be calculated using a population sample matrix (see 
#' \code{\link{accuracy}}.
#'
#' Standard errors for the adjusted areas are calculated as in Olofsson et al.  
#' (2013).
#' @export adj_areas
#' @param x an \code{accuracy} object or a list of populations as a 
#' \code{numeric}
#' @param y missing, or a contingency table
#' @references Olofsson, P., G. M. Foody, S. V. Stehman, and C. E. Woodcock.  
#' 2013. Making better use of accuracy data in land change studies: Estimating 
#' accuracy and area and quantifying uncertainty using stratified estimation.  
#' Remote Sensing of Environment 129:122-131.
setGeneric(adj_areas, function(x, y) standardGeneric(adj_areas))
#' @rdname adj_areas
#' @aliases adj_areas,numeric,table-method
setMethod(adj_areas, signature(x=numeric, y=table),
function(x, y) {
    pop <- x
    ct <- y
    Wi <- pop / sum(pop)
    adj_area_est <- sum(pop) * colSums(Wi * (ct / rowSums(ct)))
    # Calculate standard errors of the proportions
    std_err_p <- sqrt(colSums(Wi^2 *
                              (((ct / rowSums(ct))*(1 - ct / rowSums(ct))) /
                               (rowSums(ct) - 1))))
    # Now calculate standard error of adjusted area estimate
    std_err_area <- sum(pop) * std_err_p
    adj_area_mat <- cbind(pop, adj_area_est, std_err_area, 1.96 * std_err_area)
    adj_area_mat <- round(adj_area_mat, 0)
    dimnames(adj_area_mat)[[1]] <- dimnames(ct)[[1]]
    dimnames(adj_area_mat)[[2]] <- c('Mapped area', 'Adj. area', 'S.E.', 
                                     '1.96 * S.E.')
    return(new('error_adj_area', adj_area_mat=adj_area_mat))
})
#' @rdname adj_areas
#' @aliases adj_areas,numeric,matrix-method
setMethod(adj_areas, signature(x=numeric, y=matrix),
function(x, y) {
    class(y) <- table
    adj_areas(x, y)
})
#' @rdname adj_areas
#' @aliases adj_areas,numeric,missing-method
setMethod(adj_areas, signature(x=accuracy, y='missing'),
function(x) {
    pop <- x@pop
    ct <- x@ct
    adj_areas(pop, ct)
})
setMethod(show, signature(object=error_adj_area),
function(object) {
    cat('Object of class: error_adj_area\n')
    cat('Accuracy-adjusted area table:\n')
    print(object@adj_area_mat)
})
plot.error_adj_area <- function(x, ...) {
    classes <- dimnames(x@adj_area_mat)[[1]]
    areas <- x@adj_area_mat[, 2]
    se <- x@adj_area_mat[, 3]
    plt_data <- data.frame(x=classes, y=areas, se=se)
    y <- NULL # Fix for R CMD check
    ggplot(plt_data, aes(x, y)) + geom_bar(stat=identity) + 
        geom_errorbar(aes(ymin=y - 1.96 * se, ymax=y + 1.96 * se), width=.25) +
        xlab(Class) + ylab(Area)
}
.calc_pop_ct <- function(ct, pop) {
    # Below uses the notation of Pontius and Millones (2011)
    nijsum <- matrix(rowSums(ct), nrow=nrow(ct), ncol=ncol(ct))
    Ni <- matrix(pop, nrow=nrow(ct), ncol=ncol(ct))
    # pop_ct is the population contigency table
    pop_ct <- (ct / nijsum) * (Ni / sum(pop))
    dimnames(pop_ct)[[1]] <- dimnames(ct)[[1]]
    dimnames(pop_ct)[[2]] <- dimnames(ct)[[2]]
    class(pop_ct) <- 'table'
    return(pop_ct)
}
.calc_Q <- function(pop_ct) {
    # Calculate quantity disagreement (Pontius and Millones, 2011, eqns 2-3)
    qg_mat = abs(rowSums(pop_ct) - colSums(pop_ct))
    return(sum(qg_mat) / 2)
}
.calc_A <- function(pop_ct) {
    # Calculate allocation disagreement (Pontius and Millones, 2011, eqns 4-5)
    diag_indices <- which(diag(nrow(pop_ct)) == TRUE)
    ag_mat = 2 * apply(cbind(rowSums(pop_ct) - pop_ct[diag_indices],
                             colSums(pop_ct) - pop_ct[diag_indices]), 1, min)
    return(sum(ag_mat) / 2)
}
# Adds margins to contingency table
.add_ct_margins <- function(ct, digits=4) {
    # For user's, producer's, and overall accuracy formulas, see Table 
    # 21.3 in Foody, G.M., Stehman, S.V., 2009. Accuracy Assessment, in: 
    # Warner, T.A., Nellis, M.D., Foody, G.M. (Eds.), The SAGE Handbook of 
    # Remote Sensing. SAGE.
    diag_indices <- which(diag(nrow(ct)) == TRUE)
    users_acc <- ct[diag_indices] / colSums(ct)
    prod_acc <- ct[diag_indices] / rowSums(ct)
    overall_acc <- sum(ct[diag_indices]) / sum(ct)
    ct <- addmargins(ct)
    dimnames(ct)[[1]][nrow(ct)] <- Total
    dimnames(ct)[[2]][nrow(ct)] <- Total
    ct <- rbind(ct, Producers=c(users_acc, NA))
    ct <- cbind(ct, Users=c(prod_acc, NA, overall_acc))
    ct <- round(ct, digits=digits)
    dimnames(ct) <- list(predicted=dimnames(ct)[[1]],
                         observed=dimnames(ct)[[2]])
    class(ct) <- 'table'
    return(ct)
}
#' Calculate statistics summarizing classification accuracy
#'
#' Calculates a contingency table and various statistics for use in image 
#' classification accuracy assessment and map comparison. Contingency table 
#' includes user's, producer's, and overall accuracies for an image 
#' classification, and quantity disagreement \code{Q} and allocation 
#' disagreement \code{A}. \code{Q} and \code{A} are calculated based on Pontius 
#' and Millones (2011). Standard errors for 95 percent confidence intervals for 
#' the user's, producer's and overall accuracies are calculated as in Foody and 
#' Stehman (2009) Table 21.3. To avoid bias due to the use of a sample 
#' contingency table, the contingency table will be converted to a population 
#' contingency table if the variable 'pop' is provided. For an accuracy 
#' assessment using testing data from a simple random sample, 'pop' does not 
#' need to be provided (see Details).
#'
#' \code{x} can be one of:
#' \enumerate{
#'
#'   \item A prediction model as output from one of the \code{teamlucc} 
#'   \code{classify} functions. If \code{x} is a model, and testing data 
#'   is included in the model, \code{pop} and \code{test_data} can both be 
#'   missing, and accuracy will still run (though the output will in this case 
#'   be biased unless the testing data is from a simple random sample). If 
#'   \code{x} is a \code{RasterLayer}, then \code{test_data} must be supplied.
#'
#'   \item A \code{RasterLayer} with a predicted map.
#' }
#'
#' \code{test_data} can be one of:
#' \enumerate{
#'   \item \code{NULL}. If test_data is \code{NULL}, \code{accuracy} will try to use 
#'         testing data included in \code{x}. This will only work if \code{x}
#'         is a model of class \code{train} from the \code{caret} package, and 
#'         if the model was run using the one of the \code{teamlucc} 
#'         \code{classify} functions.
#'
#'   \item A \code{SpatialPolygonsDataFrame} object, in which case \code{accuracy} 
#'         will extract the predicted classes within each polygon from \code{x}.  
#'         This will only work if \code{x} is a \code{RasterLayer}.
#'
#'   \item A \code{pixel_data} object, in which case \code{accuracy} will use the 
#'         included \code{training_flag} indicator to separate testing and 
#'         training data.
#' }
#'
#' \code{pop} can be one of:
#' \enumerate{
#'   \item NULL, in which case the sample frequencies will be used as estimates 
#'         of the population frequencies of each class.
#'
#'   \item A list of length equal to the number of classes in the map giving 
#'         the total number of pixels in the population for each class.
#'
#'   \item A predicted cover map from as a \code{RasterLayer}, from which the 
#'         class frequencies will be tabulated and used as the population 
#'         frequencies.
#' }
#' @export accuracy
#' @param x either a classification model with a \code{predict} method or a 
#' \code{RasterLayer} (see Details)
#' @param test_data a \code{link{pixel_data}} object, 
#' \code{SpatialPolygonsDataFrame}, or NULL (see Details).
#' @param pop A \code{RasterLayer}, \code{numeric} of length equal to the 
#' number of clasess, or NULL (see Details).
#' @param class_col required if \code{test_data} is a 
#' \code{SpatialPolygonsDataFrame}. Defines the name of the column containing 
#' the observed cover class IDs
#' @param reclass_mat a reclassification matrix to be used in the case of a 
#' model fit by \code{classify} with the \code{do_split} option selected
#' @return \code{\link{accuracy-class}} instance
#' @references Pontius, R. G., and M. Millones. 2011. Death to Kappa: birth of 
#' quantity disagreement and allocation disagreement for accuracy assessment.  
#' International Journal of Remote Sensing 32:4407-4429.
#'
#' Olofsson, P., G. M. Foody, S. V. Stehman, and C. E. Woodcock.  2013. Making 
#' better use of accuracy data in land change studies: Estimating accuracy and 
#' area and quantifying uncertainty using stratified estimation.  Remote 
#' Sensing of Environment 129:122-131.
#'
#' Foody, G.M., Stehman, S.V., 2009. Accuracy Assessment, in: Warner, T.A., 
#' Nellis, M.D., Foody, G.M. (Eds.), The SAGE Handbook of Remote Sensing. SAGE.
#' @examples
#' \dontrun{
#' train_data <- get_pixels(L5TSR_1986, L5TSR_1986_2001_training, class_1986, 
#'                          training=.6)
#' model <- train_classifier(train_data)
#' accuracy(L5TSR_1986_rfmodel)
#' }
setGeneric(accuracy, function(x, test_data, pop, class_col, reclass_mat) 
           standardGeneric(accuracy))
#' @rdname accuracy
#' @aliases accuracy,train,ANY,ANY,missing,ANY-method
setMethod(accuracy, signature(x=train, test_data=ANY, pop=ANY, class_col=missing, reclass_mat=ANY),
    function(x, test_data, pop, class_col, reclass_mat) {
        if (missing(test_data)) {
            test_data <- x$trainingData
            names(test_data)[names(test_data) == '.outcome'] <- 'y'
        } else {
            test_data <- cbind(y=test_data@y, 
                               test_data@x,
                               training_flag=test_data@training_flag)
        }
        if (!('training_flag' %in% names(test_data))) {
            warning('no training_flag variable found - assuming none of test_data was used for model training')
        } else if (sum(test_data$training_flag == 1) == length(test_data$training_flag)) {
            stop('cannot conduct accuracy assessment without independent testing data')
        }
        test_data <- test_data[!test_data$training_flag, ]
        complete_rows <- complete.cases(test_data)
        if (sum(complete_rows) != nrow(test_data)) {
            warning(paste('ignored', nrow(test_data) - sum(complete_rows), 
                          'rows because of missing data'))
            test_data <- test_data[complete.cases(test_data), ]
        }
        predicted <- predict(x, test_data)
        observed <- test_data$y
        calc_accuracy(predicted, observed, pop, reclass_mat)
    }
)
#' @rdname accuracy
#' @aliases accuracy,RasterLayer,pixel_data,ANY,missing,ANY-method
setMethod(accuracy, signature(x=RasterLayer, test_data=pixel_data, pop=ANY, class_col=missing, reclass_mat=ANY),
    function(x, test_data, pop, class_col, reclass_mat) {
        if (all(test_data@training_flag == 1)) {
            stop('cannot conduct accuracy assessment without independent testing data')
        } else if (all(test_data@training_flag == 0)) {
            # All the test_data is for testing
            predicted <- extract(x, test_data@polys, small=TRUE, df=TRUE)[, 2]
            observed <- test_data@y
        } else {
            # Mix of testing and validation data
            predicted <- extract(x, test_data@polys[!test_data@training_flag], 
                                 small=TRUE, df=TRUE)[, 2]
            observed <- test_data@y[!test_data@training_flag]
        }
        predicted <- factor(predicted, labels=levels(observed))
        calc_accuracy(predicted, observed, pop, reclass_mat)
    }
)
#' @rdname accuracy
#' @aliases accuracy,RasterLayer,SpatialPolygonsDataFrame,ANY,character,ANY-method
setMethod(accuracy, signature(x=RasterLayer, test_data=SpatialPolygonsDataFrame, pop=ANY, class_col=character, reclass_mat=ANY),
    function(x, test_data, pop, class_col, reclass_mat) {
        ext <- get_pixels(x, test_data, class_col=class_col)
        # Since x is the predicted image, the output of get_pixels gives 
        # the predicted value in slot x, and the observed value in slot y.  
        # However x is converted to a numeric from a factor, so it needs to be 
        # converted back to a factor with the same levels as y.
        observed <- ext@y
        predicted <- factor(ext@x[, ], labels=levels(ext@y))
        calc_accuracy(predicted, observed, pop, reclass_mat)
    }
)
calc_accuracy <- function(predicted, observed, pop, reclass_mat) {
    if (!missing(reclass_mat)) {
        stop('reclass_mat not yet supported')
    }
    # ct is the sample contigency table
    ct <- table(predicted, observed)
    if (missing(pop)) {
        warning('pop was not provided - assuming sample frequencies equal population frequencies')
        pop <- rowSums(ct)
    } else if (class(pop) == 'RasterLayer') {
        pop <- freq(pop, useNA='no')[, 2]
        if (length(pop) != nrow(ct)) {
            stop('number of classes in pop must be equal to nrow(ct)')
        }
    } else if (class(pop) %in% c('integer', 'numeric')) {
        if (length(pop) != nrow(ct)) {
            stop('length(pop) must be equal to number of classes in the predicted data')
        }
    } else { 
        stop('pop must be a numeric vector or integer vector of length equal to the number of classes in x, or a RasterLayer, or NULL')
    }
    pop_ct <- .calc_pop_ct(ct, pop)
    Q <- .calc_Q(pop_ct)
    A <- .calc_A(pop_ct)
    return(new(accuracy, ct=ct, pop_ct=pop_ct, Q=Q, A=A, 
               n_test=length(observed), pop=pop))
}
  #+END_SRC
* apply_windowed.R
 #+BEGIN_SRC R 
#' Apply a raster function with edge effects over a series of blocks
#'
#' This function can be useful when applying windowed functions over a raster, 
#' as with \code{glcm}. This function allows windows functions that have edge 
#' effects to be applied over a raster in block-by-block fashion.  
#' \code{apply_windowed} avoids the striping that would result if the edge 
#' effects were ignored.
#'
#' @export
#' @param x a \code{Raster*}
#' @param fun the function to apply
#' @param edge length 2 numberic with number of rows on top and bottom with 
#' edge effects, defined as c(top, bottom)
#' @param chunksize the number of rows to read per block (passed to 
#' \code{raster} \code{blockSize} function.
#' @param filename file on disk to save \code{Raster*} to (optional)
#' @param overwrite whether to overwrite any existing files (otherwise an error 
#' will be raised)
#' @param datatype the \code{raster} datatype to use
#' @param ... additional arguments to pass to \code{fun}
#' @examples
#' \dontrun{
#' L5TSR_1986_b1 <- raster(L5TSR_1986, layer=1)
#' min_x <- cellStats(L5TSR_1986_b1, 'min')
#' max_x <- cellStats(L5TSR_1986_b1, 'max')
#' apply_windowed(L5TSR_1986_b1, glcm, edge=c(1, 3), min_x=min_x, max_x=max_x)
#' }
apply_windowed <- function(x, fun, edge=c(0, 0), chunksize=NULL, filename='', 
                          overwrite=FALSE, datatype='FLT4S', ...) {
    if ((length(edge) != 2) || (class(edge) != 'numeric') || any(edge < 0)) {
        stop('edge must be a length 2 positive numeric')
    }
    if (is.null(chunksize)) {
        bs <- blockSize(x)
    } else {
        bs <- blockSize(x, chunksize)
    }
    n_blocks <- bs$n
    # bs_mod is the blocksize that will contain blocks that have been expanded 
    # to avoid edge effects
    bs_mod <- bs
    # Expand blocks to account for edge effects on the top:
    bs_mod$row[2:n_blocks] <- bs_mod$row[2:n_blocks] - edge[1]
    # Need to read additional rows from these blocks to avoid an offset
    bs_mod$nrows[2:n_blocks] <- bs_mod$nrows[2:n_blocks] + edge[1]
    # Read additional bottom rows to account for edge effects on the bottom:
    bs_mod$nrows[1:(n_blocks - 1)] <- bs_mod$nrows[1:(n_blocks - 1)] + edge[2]
    if (any(bs_mod$row < 1)) {
        stop('too many blocks to read without edge effects - try increasing chunksize')
    } else if (any((bs_mod$nrows + bs_mod$row - 1) > nrow(x))) {
        stop('too many blocks to read without edge effects - try increasing chunksize')
    }
    
    started_writes <- FALSE
    for (block_num in 1:bs$n) {
        this_block <- getValues(x, row=bs_mod$row[block_num], 
                                nrows=bs_mod$nrows[block_num],
                                format='matrix')
        out_block <- fun(this_block, ...)
        layer_names <- dimnames(out_block)[[3]]
        # Drop the padding added to top to avoid edge effects, unless we are 
        # really on the top of the image, where top edge effects cannot be 
        # avoided
        if ((block_num != 1) && (edge[1] > 0)) {
            out_block <- out_block[-(1:edge[1]), , ]
            # The below line is needed to maintain a 3 dimensional array, 
            # even when an n x m x 1 array is returned from 
            # calc_texture_full_image because a single statistic was chosen. 
            # Without the below line, removing a row will coerce the 3d array 
            # to a 2d matrix, and the bottom padding removal will fail as it 
            # references a 3d matrix).
            if (length(dim(out_block)) < 3) dim(out_block) <- c(dim(out_block), 1)
        }
        # Drop the padding added to bottom to avoid edge effects, unless we are 
        # really on the bottom of the image, where bottom edge effects cannot 
        # be avoided
        if ((block_num != n_blocks) && (edge[2] > 0)) {
            out_block <- out_block[-((nrow(out_block)-edge[2]+1):nrow(out_block)), , ]
            if (length(dim(out_block)) < 3) dim(out_block) <- c(dim(out_block), 1)
        }
        if (!started_writes) {
            # Setup an output raster with number of layers equal to the number 
            # of layers in out_block, and extent/resolution equal to extent and 
            # resolution of x
            if (dim(out_block)[3] == 1) {
                out <- raster(x)
            } else {
                out <- brick(stack(rep(c(x), dim(out_block)[3])), values=FALSE)
            }
            if (filename == '') filename <- rasterTmpFile()
            out <- writeStart(out, filename=filename, overwrite=overwrite, 
                              datatype=datatype)
            names(out) <- layer_names
            started_writes <- TRUE
        }
        # To write to a RasterBrick the out_block needs to be structured as 
        # a 2-d matrix with bands in columns and columns as row-major vectors
        if (dim(out_block)[3] == 1) {
            out_block <- aperm(out_block, c(3, 2, 1))
            out_block <- matrix(out_block, ncol=nrow(out_block))
        } else {
            out_block <- aperm(out_block, c(3, 2, 1))
            out_block <- matrix(out_block, ncol=nrow(out_block), byrow=TRUE)
        }
        out <- writeValues(out, out_block, bs$row[block_num])
    }
    out <- writeStop(out)
    return(out)
}
calc_glcm_edge <- function(shift, window) {
    if ((length(shift) == 2) && is.numeric(shift)) shift <- list(shift)
    if ((!(is.vector(shift) && all(lapply(shift, length) == 2)) &&
         !(is.matrix(shift) && ncol(shift) == 2)) ||
        !(all(floor(unlist(shift)) == unlist(shift)))) {
        stop('shift must be a list of length 2 integer vectors, or a 2 column matrix')
    }
    if (!is.matrix(shift)) {
        shift <- matrix(unlist(shift), ncol=2, byrow=TRUE)
    }
    neg_shifts <- shift[, 2][shift[, 2] < 0]
    pos_shifts <- shift[, 2][shift[, 2] > 0]
    if (length(neg_shifts) == 0) neg_shifts <- 0
    if (length(pos_shifts) == 0) pos_shifts <- 0
    return(c(abs(min(neg_shifts)) + ceiling(window[2] / 2) - 1,
             abs(max(pos_shifts)) + ceiling(window[2] / 2) - 1))
}
  #+END_SRC
* auto_calc_predictors.R
 #+BEGIN_SRC R 
#' Calculate predictor layers for a classification
#'
#' This function automates the calculation of a layer stack of predictor layers 
#' to use in a land use and/or land cover classification. See Details for the 
#' output layers.
#' 
#' The layers in the output layer stack are listed below. Note that all the 
#' layers are rescaled so that they range between -32,767 and 32,767 (allowing 
#' them to be stored as 16 bit unsigned integers).
#'
#' \bold{Predictor layer stack:}
#' \tabular{ll}{
#'     Layer 1: \tab Band 1 reflectance \cr
#'     Layer 2: \tab Band 2 reflectance \cr
#'     Layer 3: \tab Band 3 reflectance \cr
#'     Layer 4: \tab Band 4 reflectance \cr
#'     Layer 5: \tab Band 5 reflectance \cr
#'     Layer 6: \tab Band 7 reflectance \cr
#'     Layer 7: \tab MSAVI2 \cr
#'     Layer 8: \tab GLCM mean (from MSAVI2) \cr
#'     Layer 9: \tab GLCM variance (from MSAVI2) \cr
#'     Layer 10: \tab GLCM dissimilarity (from MSAVI2) \cr
#'     Layer 11: \tab Elevation \cr
#'     Layer 12: \tab Slope (radians X 10000) \cr
#'     Layer 13: \tab Aspect (see below) \cr
#' }
#'
#' The aspect is recoded as:
#'
#' \bold{Aspect coding:}
#' \tabular{ll}{
#'     1: \tab north facing (0-45 degrees, 315-360 degrees) \cr
#'     2: \tab east facing (45-135 degrees) \cr
#'     3: \tab south facing (135-225 degrees) \cr
#'     4: \tab west facing (225-315 degrees) \cr
#' }
#' @export
#' @importFrom glcm glcm
#' @importFrom stringr str_extract
#' @param x path to a preprocessed image as output by 
#' \code{auto_preprocess_landsat} or \code{auto_cloud_fill}.
#' @param dem DEM \code{RasterLayer} as output by \code{auto_setup_dem}
#' @param slopeaspect \code{RasterStack} as output by \code{auto_setup_dem}
#' @param output_path the path to use for the output (optional - if NULL then 
#' output images will be saved alongside the input images in the same folder).
#' @param ext file extension to use when saving output rasters (determines 
#' output file format).
#' @param overwrite whether to overwrite existing files (otherwise an error 
#' will be raised)
#' @param ...  additional arguments passed to \code{\link{glcm}}, such as
#' \code{n_grey}, \code{window}, or \code{shift}
#' @param notify notifier to use (defaults to \code{print} function). See the 
#' \code{notifyR} package for one way of sending notifications from R. The 
#' \code{notify} function should accept a string as the only argument.
auto_calc_predictors <- function(x, dem, slopeaspect, output_path=NULL, 
                                 ext='tif', overwrite=FALSE, notify=print,
                                 ...) {
    if (!file_test(-f, x)) {
        stop(paste(input image, x, does not exist))
    }
    if (!is.null(output_path) && !file_test(-d, output_path)) {
        stop(paste(output_path, does not exist))
    }
    ext <- gsub('^[.]', '', ext)
    timer <- Track_time(notify)
    timer <- start_timer(timer, label='Predictor calculation')
    # Setup a regex to identify preprocessed images
    preproc_regex <- '^[a-zA-Z]{2,3}_[0-9]{3}-[0-9]{3}_[0-9]{4}-[0-9]{3}_L[457][ET]SR(_tc)?'
    # Update the basename to refer the chosen file
    image_basename <- basename(file_path_sans_ext(x))
    image_stack <- brick(x)
    if (is.null(output_path)) {
        output_path <- dirname(x)
    }
    mask_stack_file <- paste0(file_path_sans_ext(x), '_masks.', ext)
    if (!file_test('-f', mask_stack_file)) {
        mask_stack_file <- gsub(paste0('(_tc)?.', ext, '$'), paste0('_masks.', ext), x)
        if (file_test('-f', mask_stack_file)) {
            warning('using masks file with old format (pre v0.5) teamlucc naming')
        } else {
            stop('could not find masks file')
        }
    }
    mask_stack <- brick(mask_stack_file)
    image_mask <- calc(mask_stack[[2]], function(maskvals) {
        # Mask clouds, cloud shadow, and fill
        (maskvals == 2) | (maskvals == 4) | (maskvals == 255)
    })
    ######################################################################
    # Calculate additional predictor layers (MSAVI and textures)
    timer <- start_timer(timer, label='Calculating MSAVI2')
    MSAVI2_filename <- file.path(output_path,
                                 paste0(image_basename, '_MSAVI2.', ext))
    MSAVI2_layer <- MSAVI2(red=raster(image_stack, layer=3),
                           nir=raster(image_stack, layer=4))
    # Truncate MSAVI2 to range between 0 and 1, and scale by 10,000 so it 
    # can be saved as a INT2S
    MSAVI2_layer <- calc(MSAVI2_layer, fun=function(vals) {
            vals[vals > 1] <- 1
            vals[vals < 0] <- 0
            vals <- round(vals * 10000)
        }, filename=MSAVI2_filename, overwrite=overwrite, datatype=INT2S)
    timer <- stop_timer(timer, label='Calculating MSAVI2')
    timer <- start_timer(timer, label='Calculating GLCM textures')
    MSAVI2_glcm_filename <- file.path(output_path,
                                      paste0(image_basename, 
                                            '_MSAVI2_glcm.', ext))
    glcm_statistics <- c('mean', 'variance', 'homogeneity', 'contrast', 
                         'dissimilarity', 'entropy', 'second_moment', 
                         'correlation')
    MSAVI2_layer[image_mask] <- NA
    # Need to know window and shift to calculate edge for apply_windowed. So if 
    # they are not in the dotted args, assume the defaults (since glcm will use 
    # the defaults if these parameters are not supplied).
    dots <- list(...)
    if (!(window %in% names(dots))) {
        dots$window <- c(3, 3)
    }
    if (!(shift %in% names(dots))) {
        dots$shift <- c(1, 1)
    }
    edge <- calc_glcm_edge(dots$shift, dots$window)
    # Note the min_x and max_x are given for MSAVI2 that has been scaled by 
    # 10,000
    apply_windowed_args <- list(x=MSAVI2_layer, fun=glcm, edge=edge, min_x=0, 
                             max_x=10000, filename=MSAVI2_glcm_filename, 
                             overwrite=overwrite, statistics=glcm_statistics, 
                             na_opt='center')
    apply_windowed_args <- c(apply_windowed_args, dots)
    MSAVI2_glcm <- do.call(apply_windowed, apply_windowed_args)
    names(MSAVI2_glcm) <- paste('glcm', glcm_statistics, sep='_')
    timer <- stop_timer(timer, label='Calculating GLCM textures')
    if (!missing(slopeaspect)) {
        timer <- start_timer(timer, label='Processing slopeaspect')
        names(slopeaspect) <- c('slope', 'aspect')
        # Classify aspect into north facing, east facing, etc., recalling 
        # that the aspect is stored in radians scaled by 1000.
        #     1: north facing (0-45, 315-360)
        #     2: east facing (45-135)
        #     3: south facing (135-225)
        #     4: west facing (225-315)
        aspect_cut <- raster::cut(slopeaspect$aspect/1000,
                                  c(-1, 45, 135, 225, 315, 361)*(pi/180))
        # Code both 0-45 and 315-360 aspect as North facing (1)
        aspect_cut[aspect_cut == 5] <- 1
        names(aspect_cut) <- 'aspect'
        timer <- stop_timer(timer, label='Processing slopeaspect')
    }
    ######################################################################
    # Layer stack predictor layers:
    timer <- start_timer(timer, label='Writing predictors')
    predictors <- stack(raster(image_stack, layer=1),
                        raster(image_stack, layer=2),
                        raster(image_stack, layer=3),
                        raster(image_stack, layer=4),
                        raster(image_stack, layer=5),
                        raster(image_stack, layer=6),
                        MSAVI2_layer,
                        scale_raster(MSAVI2_glcm$glcm_mean),
                        scale_raster(MSAVI2_glcm$glcm_variance),
                        scale_raster(MSAVI2_glcm$glcm_dissimilarity))
    predictor_names <- c('b1', 'b2', 'b3', 'b4', 'b5', 'b7', 'msavi', 
                         'msavi_glcm_mean', 'msavi_glcm_variance', 
                         'msavi_glcm_dissimilarity')
    if (!missing(dem)) {
        predictors <- stack(predictors, dem)
        predictor_names <- c(predictor_names, 'elev')
    }
    if (!missing(slopeaspect)) {
        predictors <- stack(predictors, slopeaspect$slope, aspect_cut)
        predictor_names <- c(predictor_names, 'slope', 'aspect')
    }
    predictors_filename <- file.path(output_path,
                                     paste0(image_basename, '_predictors.', 
                                            ext))
    names(predictors) <- predictor_names
    predictors <- mask(predictors, image_mask, maskvalue=1, 
                       filename=predictors_filename, 
                       overwrite=overwrite, datatype='INT2S')
    names(predictors) <- predictor_names
    # Save a copy of the original masks file along with the predictors file, so 
    # the masks can be easily located later.
    predictors_mask_filename <- file.path(output_path,
                                          paste0(image_basename, 
                                                 '_predictors_masks.', ext))
    mask_stack <- writeRaster(mask_stack, filename=predictors_mask_filename, 
                              overwrite=overwrite, 
                              datatype=dataType(mask_stack)[1])
    timer <- stop_timer(timer, label='Writing predictors')
    timer <- stop_timer(timer, label='Predictor calculation')
    return(predictors)
}
  #+END_SRC
* auto_chg_detect.R
 #+BEGIN_SRC R 
#' Perform change detection for two Landsat CDR surface reflectance images
#'
#' This image automates the change detection process using the Change Vector 
#' Analysis in Posterior Probability Space (CVAPS) algorithm. The threshold for 
#' change/no-change mapping is determined using Huang's algorithm (see 
#' \code{\link{threshold}} or can be specified manually. First the images 
#' should be classified using the \code{auto_classify} function (or any other 
#' classification approach that yields per-pixel probabilities of class 
#' membership).
#'
#' @export
#' @param t1_classes cover classes as output from \code{auto_classify_image} 
#' for time 1 image
#' @param t1_probs per class probabilities as output from 
#' \code{auto_classify_image} for time 1 image
#' @param t2_probs per class probabilities as output from 
#' \code{auto_classify_image} for time 2 image
#' @param output_path the path to use for the output
#' @param output_basename the base filename for output files from 
#' \code{auto_chg_detect} (without an extension)
#' @param ext file extension to use when saving output rasters (determines 
#' output file format).
#' @param overwrite whether to overwrite existing files (otherwise an error
#' will be raised)
#' @param chg_threshold the threshold to use determining change and no-change 
#' areas from the change magnitude image (see \code{\link{chg_mag}}. If 
#' \code{NULL}, then \code{\link{threshold}} will be used to dermine this 
#' threshold value automatically. A threshold in the range of .75-1 is 
#' recommended as a starting point.
#' @param notify notifier to use (defaults to \code{print} function).  See the 
#' \code{notifyR} package for one way of sending notifications from R.  The 
#' \code{notify} function should accept a string as the only argument.
#' @return nothing - used for the side effect of performing change detection
#' @references Chen, J., X. Chen, X. Cui, and J. Chen. 2011. Change vector 
#' analysis in posterior probability space: a new method for land cover change 
#' detection.  IEEE Geoscience and Remote Sensing Letters 8:317-321.
auto_chg_detect <- function(t1_classes, t1_probs, t2_probs, output_path, 
                            output_basename, ext='tif', overwrite=FALSE, 
                            chg_threshold=NULL, notify=print) {
    if (!file_test(-d, output_path)) {
        stop(paste(output_path, does not exist))
    }
    ext <- gsub('^[.]', '', ext)
    timer <- Track_time(notify)
    timer <- start_timer(timer, label='Change detection')
    ###########################################################################
    # Calculate change magnitude and direction
    ###########################################################################
    timer <- start_timer(timer, label='Change magnitude and direction')
    chg_dir_filename <- file.path(output_path, paste0(output_basename, 
                                                     '_chgdir.', ext))
    chg_dir_image <- chg_dir(t1_probs, t2_probs, filename=chg_dir_filename, 
                             overwrite=overwrite)
    chg_mag_filename <- file.path(output_path, paste0(output_basename, 
                                                     '_chgmag.', ext))
    chg_mag_image <- chg_mag(t1_probs, t2_probs, filename=chg_mag_filename, 
                             overwrite=overwrite)
    timer <- stop_timer(timer, label='Change magnitude and direction')
    ###########################################################################
    # Calculate change trajectories
    ###########################################################################
    timer <- start_timer(timer, label='Change trajectories')
    if (is.null(chg_threshold)) chg_threshold <- threshold(chg_mag_image)
    
    notify(paste0('Using threshold=', chg_threshold))
    chg_traj_filename <- file.path(output_path,
                                   paste0(output_basename, '_chgtraj.', ext))
    chg_traj_out <- chg_traj(chg_mag_image, chg_dir_image, 
                             chg_threshold=chg_threshold, overwrite=overwrite, 
                             filename=chg_traj_filename)
    timer <- stop_timer(timer, label='Change trajectories')
    timer <- stop_timer(timer, label='Change detection')
}
  #+END_SRC
* auto_classify.R
 #+BEGIN_SRC R 
#' Classify a preprocessed surface reflectance image
#'
#' First the image should be preprocessed using the \code{auto_preprocess} 
#' function. For Landsat CDR imagery, predictor layers can be generated using 
#' the \code{auto_generate_predictors} function.
#'
#' @export
#' @importFrom rgdal readOGR
#' @importFrom sp spTransform
#' @importFrom tools file_path_sans_ext
#' @param predictor_file a \code{Raster*} of predictor layers output by the 
#' \code{auto_preprocess} function or path to an image stack in a format 
#' readable by the \code{raster} package.
#' @param train_shp a file readable by readOGR with training polygons
#' @param output_path the path to use for the output
#' @param class_col the name of the column containing the response variable 
#' (for example the land cover type of each pixel)
#' @param training indicator of which polygons to use in training. Can be: 1) a 
#' string giving the name of a column indicating whether each polygon is to be 
#' used in training (column equal to TRUE) or in testing (column equal to 
#' FALSE), or 2) a logical vector of length equal to length(polys), or 3) a 
#' number between 0 and 1 indicating the fraction of the polygons to be 
#' randomly selected for use in training.
#' @param overwrite whether to overwrite existing files (otherwise an error 
#' will be raised)
#' @param notify notifier to use (defaults to \code{print} function). See the 
#' \code{notifyR} package for one way of sending notifications from R. The 
#' \code{notify} function should accept a string as the only argument.
#' @examples
#' #TODO: Add example
auto_classify <- function(predictor_file, train_shp, output_path, 
                          class_col=Poly_Type, training=.6, overwrite=FALSE, 
                          notify=print) {
    if (!file_test(-f, train_shp)) {
        stop(paste(train_shp, does not exist))
    }
    if (!file_test(-f, predictor_file)) {
        stop(paste(predictor_file, does not exist))
    }
    if (!file_test(-d, output_path)) {
        stop(paste(output_path, does not exist))
    }
    timer <- Track_time(notify)
    timer <- start_timer(timer, label='Running auto_classify')
    predictors <- brick(predictor_file)
    pred_rast_basename <- basename(file_path_sans_ext(predictor_file))
    train_polys <- readOGR(dirname(train_shp), basename(file_path_sans_ext(train_shp)))
    train_polys <- spTransform(train_polys, crs(predictors))
    train_data <- get_pixels(predictors, train_polys, class_col=class_col, 
                             training=training)
    timer <- start_timer(timer, label='Running classification')
    classification <- classify(predictors, train_data)
    model <- classification$model
    save(model, file=file.path(output_path, paste(pred_rast_basename, 
                                                  'predmodel.RData', sep='_')))
    writeRaster(classification$pred_classes,
                filename=file.path(output_path, paste(pred_rast_basename, 
                                                      'predclasses.tif', 
                                                      sep='_')),
                datatype='INT2S', overwrite=overwrite)
    writeRaster(scale_raster(classification$pred_probs),
                filename=file.path(output_path, paste(pred_rast_basename, 
                                                      'predprobs.tif', 
                                                      sep='_')),
                datatype='INT2S', overwrite=overwrite)
    timer <- stop_timer(timer, label='Running classification')
    # cls <- levels(train_data$y) 
    # cls <- data.frame(code=seq(1:length(cls)), name=cls)
    # color_image(classification$predclasses, cls,
    #             file.path(output_path, paste(pred_rast_basename, 
    #             'predclasses_colored.tif', sep='_')))
    # Perform accuracy assessment using an independent dataset:
    timer <- start_timer(timer, label='Running accuracy assessment')
    acc <- accuracy(classification$model, 
                    pop=classification$pred_classes)
    capture.output(summary(acc),
                   file=file.path(output_path, paste(pred_rast_basename, 'predacc.txt', sep='_')))
    timer <- stop_timer(timer, label='Running accuracy assessment')
    timer <- stop_timer(timer, label='Running auto_classify')
}
  #+END_SRC
* auto_cloud_fill.R
 #+BEGIN_SRC R 
pct_clouds <- function(cloud_mask) {
    num_clouds <- cellStats(cloud_mask == 1, stat='sum', na.rm=TRUE)
    num_clear <- cellStats(cloud_mask == 0, stat='sum', na.rm=TRUE)
    return((num_clouds / (num_clouds + num_clear)) * 100)
}
#' Automated removal of clouds from Landsat CDR imagery
#'
#' Uses one of four cloud reomval algorithms (see \code{\link{cloud_remove}}) 
#' to remove thick clouds from Landsat imagery. In hilly areas, topographic 
#' correction should be done before cloud fill.
#'
#' The \code{auto_cloud_fill} function allows an analyst to automatically 
#' construct a cloud-filled image after specifying: \code{data_dir} (a folder 
#' of Landsat images), \code{wrspath} and \code{wrsrow} (the WRS-2 path/row to 
#' use), and \code{start_date} and \code{end_date} (a start and end date 
#' limiting the images to use in the algorithm).  The analyst can also 
#' optionally specify a \code{base_date}, and the \code{auto_cloud_fill} 
#' function will automatically pick the image closest to that date to use as 
#' the base image.
#' 
#' As the \code{auto_cloud_fill} function automatically chooses images for 
#' inclusion in the cloud fill process, it relies on having images stored on 
#' disk in a particular way, and currently only supports cloud fill for Landsat 
#' CDR surface reflectance images. To ensure that images are correctly stored 
#' on your hard disk, use the \code{\link{auto_preprocess_landsat}} function to 
#' extract the original Landsat CDR hdf files from the USGS archive. The 
#' \code{auto_preprocess_landsat} function will ensure that images are 
#' extracted and renamed properly so that they can be used with the 
#' \code{auto_cloud_fill} script.
#'
#' @export
#' @importFrom tools file_path_sans_ext
#' @importFrom lubridate as.duration new_interval
#' @importFrom stringr str_extract
#' @importFrom SDMTools ConnCompLabel
#' @param data_dir folder where input images are located, with filenames as 
#' output by the \code{\link{auto_preprocess_landsat}} function. This folder 
#' will be searched recursively for images (taking the below path/row, date, 
#' and topographic correction options into account).
#' @param wrspath World Reference System (WRS) path
#' @param wrsrow World Reference System (WRS) row
#' @param start_date start date of period from which images will be chosen to 
#' fill cloudy areas in the base image (as \code{Date} object)
#' @param end_date end date of period from which images will be chosen to fill 
#' cloudy areas in the the base image (as \code{Date} object)
#' @param base_date ideal date for base image (base image will be chosen as the 
#' image among the available images that is closest to this date). If NULL, 
#' then the base image will be the image with the lowest cloud cover.
#' @param out_name base filename (without an extension - see \code{ext} 
#' argument) for cloud filled image.  The mask file for the cloud filled image 
#' will be saved with the same name, with the added suffix _mask.
#' @param tc if \code{TRUE}, use topographically corrected imagery as output by 
#' \code{auto_preprocess_landsat}. IF \code{FALSE} use bands 1-5 and 7 surface 
#' reflectance as output by \code{unstack_ledaps} or 
#' \code{auto_preprocess_landsat} (if \code{auto_preprocess_landsat} was also 
#' run with tc=FALSE).
#' @param ext file extension to use when searching for input rasters and when 
#' saving output rasters (determines output file format). Should match file 
#' extension of input rasters (and should most likely match the value chosen 
#' for \code{ext} when \code{auto_preprocess_landsat} was run).
#' @param sensors choose the sensors to include when selecting images (useful 
#' for excluding images from a particular satellite if desired). Can be any of 
#' L4T, L5T, L7E, and/or L8C.
#' @param img_type type of Landsat imagery to preprocess. Can be CDR for 
#' Landsat Climate Data Record (CDR) imagery in HDR format, or L1T for 
#' Standard Terrain Correction (Level 1T) imagery. Note that if L1T imagery is 
#' used, fmask must be run locally (see https://code.google.com/p/fmask) prior 
#' to using \code{auto_preprocess_landsat}.
#' @param threshold maximum percent cloud cover allowable in base image. Cloud 
#' fill will iterate until percent cloud cover in base image is below this 
#' value, or until \code{max_iter} iterations have been run
#' @param max_iter maximum number of times to run cloud fill script
#' @param notify notifier to use (defaults to \code{print} function).  See the 
#' \code{notifyR} package for one way of sending notifications from R.  The 
#' \code{notify} function should accept a string as the only argument.
#' @param verbose whether to print detailed status messages. Set to FALSE or 0 
#' for no status messages. Set to 1 for basic status messages. Set to 2 for 
#' detailed status messages.
#' @param overwrite whether to overwrite \code{out_name} if it already exists
#' @param ...  additional arguments passed to \code{\link{cloud_remove}}, such 
#' as \code{DN_min}, \code{DN_max}, \code{algorithm}, \code{byblock}, 
#' \code{verbose}, etc. See \code{\link{cloud_remove}} for details
#' @return a list with two elements: filled, a \code{Raster*} object with 
#' cloud filled image, and mask, a \code{RasterLayer} object with the cloud 
#' mask for the cloud filled image.
#' @references Zhu, X., Gao, F., Liu, D., Chen, J., 2012. A modified 
#' neighborhood similar pixel interpolator approach for removing thick clouds 
#' in Landsat images.  Geoscience and Remote Sensing Letters, IEEE 9, 521--525.  
#' doi:10.1109/LGRS.2011.2173290
auto_cloud_fill <- function(data_dir, wrspath, wrsrow, start_date, end_date, 
                            out_name, base_date=NULL, tc=TRUE, ext='tif',
                            sensors=c('L4T', 'L5T', 'L7E', 'L8C'), 
                            img_type=CDR, threshold=1, max_iter=5, 
                            notify=print, verbose=1, overwrite=FALSE, ...) {
    if (!file_test('-d', data_dir)) {
        stop('data_dir does not exist')
    }
    if (!file_test('-d', dirname(out_name))) {
        stop('output folder does not exist')
    }
    if (file_path_sans_ext(out_name) != out_name) {
        stop('out_name should not have a file extension')
    }
    ext <- gsub('^[.]', '', ext)
    output_file <- paste0(out_name, '.', ext)
    if (file_test('-f', output_file) & !overwrite) {
        stop(paste0('output file ', output_file, ' already exists'))
    }
    if (!all(sensors %in% c('L4T', 'L5T', 'L7E', 'L8C'))) {
        stop('sensors must be a list of one or more of: L4T, L5T, L7E, L8C')
    }
    log_file <- file(paste0(out_name, '_log.txt'), open=wt)
    msg <- function(txt) {
        cat(paste0(txt, '\n'), file=log_file, append=TRUE)
        print(txt)
    }
    timer <- Track_time(msg)
    timer <- start_timer(timer, label='Cloud fill')
    stopifnot(class(start_date) == 'Date')
    stopifnot(class(end_date) == 'Date')
    wrspath <- sprintf('%03i', wrspath)
    wrsrow <- sprintf('%03i', wrsrow)
    # Find image files based on start and end dates
    prefix_re <- ^([a-zA-Z]*_)?
    #pathrow_re <-[012][0-9]{2}-[012][0-9]{2}
    pathrow_re <- paste(wrspath, wrsrow, sep='-')
    date_re <-((19)|(2[01]))[0-9]{2}-[0123][0-9]{2}
    if (img_type == CDR) {
        sensor_re <- paste0('(', paste0(paste0('(', sensors,')'), collapse='|'), ')', SR)
    } else if (img_type == L1T) {
        sensor_re <- paste0('(', paste0(paste0('(', sensors,')'), collapse='|'), ')', L1T)
    } else {
        stop(paste(img_type, is not a recognized img_type))
    }
    if (tc) {
        suffix_re <- paste0('_tc.', ext, '$')
    } else {
        suffix_re <- paste0('.', ext, '$')
    }
    file_re <- paste0(prefix_re, paste(pathrow_re, date_re, sensor_re, 
                                       sep='_'), suffix_re)
    img_files <- dir(data_dir, pattern=file_re, recursive=TRUE)
    img_dates <- str_extract(basename(img_files), date_re)
    img_dates <- as.Date(img_dates, '%Y-%j')
    which_files <- which((img_dates >= start_date) &
                          (img_dates < end_date))
    img_dates <- img_dates[which_files]
    img_files <- file.path(data_dir, img_files[which_files])
    if (length(img_files) == 0) {
        stop('no images found - check date_dir, check wrspath, wrsrow, start_date, and end_date')
    } else if (length(img_files) < 2) {
        stop(paste('Only', length(img_files),
                   'image(s) found. Need at least two images to perform cloud fill'))
    }
    if (verbose > 0) {
        msg(paste('Found', length(img_files), 'image(s)'))
        timer <- start_timer(timer, label='Analyzing cloud cover in input images')
    }
    # Run QA stats
    fmasks <- list()
    fill_QAs <- list()
    imgs <- list()
    for (img_file in img_files) {
        masks_file <- paste0(file_path_sans_ext(img_file), '_masks.', ext)
        if (!file_test('-f', masks_file)) {
            masks_file <- gsub(suffix_re, paste0('_masks.', ext), img_file)
            if (file_test('-f', masks_file)) {
                warning('using masks file with old format (pre v0.5) teamlucc naming')
            } else {
                stop('could not find masks file')
            }
        }
        this_fill_QA <- raster(masks_file, band=1)
        fill_QAs <- c(fill_QAs, this_fill_QA)
        this_fmask <- raster(masks_file, band=2)
        fmasks <- c(fmasks, this_fmask)
        this_img <- stack(img_file)
        imgs <- c(imgs, stack(this_img))
    }
    
    compareRaster(imgs, res=TRUE, orig=TRUE)
    compareRaster(fmasks, res=TRUE, orig=TRUE)
    freq_table <- freq(stack(fmasks), useNA='no', merge=TRUE)
    # Convert frequency table to fractions
    freq_table[-1] <- freq_table[-1] / colSums(freq_table[-1], na.rm=TRUE)
    if (verbose > 0) {
        timer <- stop_timer(timer, label='Analyzing cloud cover in input images')
    }
    if (verbose > 0) {
        timer <- start_timer(timer, label='Calculating cloud masks')
    }
    # Find image that is either closest to base date, or has the maximum 
    # percent clear
    if (is.null(base_date)) {
        clear_row <- which(freq_table$value == 0)
        base_img_index <- which(freq_table[clear_row, -1] == 
                                max(freq_table[clear_row, -1]))
    } else {
        base_date_diff <- lapply(img_dates, function(x) 
                                 as.duration(new_interval(x, base_date)))
        base_date_diff <- abs(unlist(base_date_diff))
        base_img_index <- which(base_date_diff == min(base_date_diff))
        # Handle ties - two images that are the same distance from base date.  
        # Default to earlier image.
        if (length(base_img_index) > 1) {
            base_img_index <- base_img_index[1]
        }
    }
    # Save the original base image fmask so it can be used to recode the final 
    # cloud mask at the end of cloud filling
    base_fmask <- fmasks[[base_img_index]]
    base_fill_QA <- fill_QAs[[base_img_index]]
    # Convert masks to indicate: 0 = clear; 1 = cloud or shadow; 2 = fill
    #
    #   fmask_band key:
    #       0 = clear
    #       1 = water
    #       2 = cloud_shadow
    #       3 = snow
    #       4 = cloud
    #       255 = fill value
    calc_cloud_mask <- function(fmask, img) {
        # Code clouds and cloud shadows as 1
        ret <- (fmask == 2) | (fmask == 4)
        # Code fill as 2
        ret[fmask == 255] <- 2
        # Code other missing data that is not in fill areas as NA. The (ret != 
        # 1) test is necessary to ensures that only NAs that are NOT in clouds 
        # will be copied to the mask images (the assumption being that NAs in 
        # clouds should be marked as cloud and fill should be attempted).  This 
        # is necessary in case clouded areas in img are mistakenly coded NA 
        # (they should not be).
        ret[(ret != 1) & (ret != 2) & is.na(img)] <- NA
        return(ret)
    }
    for (n in 1:length(fmasks)) {
        fmasks[n] <- overlay(fmasks[[n]], imgs[[n]][[1]], fun=calc_cloud_mask, 
                             datatype=dataType(fmasks[[n]]))
    }
    base_img <- imgs[[base_img_index]]
    imgs <- imgs[-base_img_index]
    base_mask <- fmasks[[base_img_index]]
    fmasks <- fmasks[-base_img_index]
    base_img_date <- img_dates[base_img_index]
    img_dates <- img_dates[-base_img_index]
    if (verbose > 0) {
        msg(paste('Using image from', base_img_date, 'as base image.'))
    }
    if (verbose > 0) {
        timer <- stop_timer(timer, label='Calculating cloud masks')
    }
    if (verbose > 0) {
        timer <- start_timer(timer, label='Masking base image')
    }
    # Mask out clouds in base image. Save this image to disk so it is available 
    # even if no cloud fill is done (if the pct_clouds in this image is below 
    # the threshold).
    base_img <- overlay(base_img, base_mask,
        fun=function(base_vals, mask_vals) {
            # Set clouds/shadows to 0
            base_vals[mask_vals == 1] <- 0
            # Allow fill to be attempted in NA areas
            base_vals[is.na(base_vals)] <- 0
            # Set slc-off gaps and areas outside scene to NA
            base_vals[mask_vals == 2] <- NA
            return(base_vals)
        }, datatype=dataType(base_img[[1]]), 
        filename=extension(rasterTmpFile(), ext), overwrite=overwrite)
    cur_pct_clouds <- pct_clouds(base_mask)
    if (verbose > 0) {
        msg(paste0('Base image has ', round(cur_pct_clouds, 2), '% cloud cover before fill'))
    }
    if (verbose > 0) {
        timer <- stop_timer(timer, label='Masking base image')
    }
    n <- 0
    while ((cur_pct_clouds > threshold) & (n < max_iter) & (length(imgs) >= 1)) {
        if (verbose > 0) {
            timer <- start_timer(timer, label=paste('Fill iteration', n + 1))
        }
        # Calculate a raster indicating the pixels in each potential fill image 
        # that are available for filling pixels of base_img that are missing 
        # due to cloud contamination. Areas coded 1 are missing due to cloud or 
        # shadow in the base image and are available in the merge image. This 
        # will return a stack with number of layers equal to number of masks.
        fill_areas <- overlay(base_mask, stack(fmasks),
            fun=function(base_mask_vals, fill_mask_vals) {
                ret <- rep(NA, length(base_mask_vals))
                # Code cloudy in base, clear in fill as 1
                ret[(base_mask_vals == 1) & (fill_mask_vals == 0)] <- 1
                # Code clear in base, clear in fill as 0
                ret[(base_mask_vals == 0) & (fill_mask_vals == 0)] <- 0
                # Code NA in base, clear in fill as clouded, so these NAs will 
                # be filled if possible.
                ret[is.na(base_mask_vals) & (fill_mask_vals == 0)] <- 1
                # Ensure SLC-off gaps and background areas in each image are 
                # not filled:
                ret[(base_mask_vals == 2) | (fill_mask_vals == 2)] <- NA
                return(ret)
            }, datatype=dataType(base_mask))
        fill_areas_freq <- freq(fill_areas, useNA='no', merge=TRUE)
        # Below is necessary as for some reason when fill_areas is of length 
        # one, freq returns a matrix rather than a data.frame
        fill_areas_freq <- as.data.frame(fill_areas_freq)
        # Select the fill image with the maximum number of available pixels 
        # (counting only pixels in the fill image that are not ALSO clouded in 
        # the fill image)
        avail_fill_row <- which(fill_areas_freq$value == 1)
        if (length(avail_fill_row) == 0) {
            msg(paste('No fill pixels available. Stopping fill.'))
            break
        }
        # Remove the now unnecessary value column
        fill_areas_freq <- fill_areas_freq[!(names(fill_areas_freq) == 'value')]
        fill_img_index <- which(fill_areas_freq[avail_fill_row, ] == 
                                max(fill_areas_freq[avail_fill_row, ], na.rm=TRUE))
        if ((length(fill_img_index) == 0) ||
            (fill_areas_freq[avail_fill_row, fill_img_index] == 0)) {
            msg(paste('No fill pixels available. Stopping fill.'))
            break
        }
        fill_img <- imgs[[fill_img_index]]
        imgs <- imgs[-fill_img_index]
        base_img_mask <- fill_areas[[fill_img_index]]
        fmasks <- fmasks[-fill_img_index]
        fill_img_date <- img_dates[fill_img_index]
        img_dates <- img_dates[-fill_img_index]
        # Add numbered IDs to the cloud patches
        base_img_mask <- ConnCompLabel(base_img_mask)
        # Ensure dataType is properly set prior to handing off to IDL
        dataType(base_img_mask) <- 'INT2S'
        if (verbose > 0) {
            msg(paste0('Filling image from ', base_img_date,
                          ' with image from ', fill_img_date, '.'))
            timer <- start_timer(timer, label=Performing fill)
        }
        base_img <- cloud_remove(base_img, fill_img, base_img_mask, 
                                 out_name=extension(rasterTmpFile(), ext), 
                                 verbose=verbose, overwrite=TRUE, ...)
        # base_img <- cloud_remove(base_img, fill_img, base_img_mask, 
        #                          out_name=extension(rasterTmpFile(), ext), 
        #                          verbose=verbose, overwrite=TRUE, 
        #                          DN_min=DN_min, DN_max=DN_max, 
        #                          algorithm=algorithm, byblock=byblock)
        if (verbose > 0) {
            timer <- stop_timer(timer, label=Performing fill)
        }
        # Revise base mask to account for newly filled pixels
        base_mask <- overlay(base_mask, base_img[[1]],
            fun=function(mask_vals, filled_vals) {
                mask_vals[(mask_vals == 1) & (filled_vals != 0)] <- 0
                return(mask_vals)
            }, datatype=dataType(base_mask), 
            filename=extension(rasterTmpFile(), ext), overwrite=TRUE)
        cur_pct_clouds <- pct_clouds(base_mask)
        if (verbose > 0) {
            msg(paste0('Base image has ', round(cur_pct_clouds, 2),
                          '% cloud cover remaining'))
            timer <- stop_timer(timer, label=paste('Fill iteration', n + 1))
        }
        n <- n + 1
    }
    base_img <- writeRaster(base_img, filename=output_file, datatype=INT2S, 
                            overwrite=overwrite)
    # Recode base mask so final coding matches that of fmask (though cloud and 
    # cloud shadow are no longer differentiated)
    #   fmask_band key:
    #       0 = clear
    #       1 = water
    #       2 = cloud_shadow
    #       3 = snow
    #       4 = cloud
    #       255 = fill value
    #   base_mask key:
    #   	0 = clear
    #   	1 = cloud
    #   	2 = fill
    mask_output_file <- paste0(out_name, '_masks.', ext)
    filled_fmask <- overlay(base_mask, base_fmask,
        fun=function(after_fill, before_fill) {
            ret <- after_fill
            # Code clear after filling but water in fmask as water (1 in fmask)
            ret[(after_fill == 0) & (before_fill == 1)] <- 1
            # Code clear after filling but snow in fmask as snow (3 in fmask)
            ret[(after_fill == 0) & (before_fill == 3)] <- 3
            # Code cloudy after filling as cloud (4 in fmask)
            ret[after_fill == 1] <- 4
            # Code gap in fmask as gap (3 in fmask)
            ret[before_fill == 255] <- 255
            return(ret)
        }, datatype=dataType(base_mask))
    final_masks <- stack(base_fill_QA, filled_fmask)
    names(final_masks) <- c(fill_QA, fmask)
    final_masks <- writeRaster(final_masks, datatype=dataType(base_mask), 
                               filename=mask_output_file, overwrite=TRUE)
    timer <- stop_timer(timer, label='Cloud fill')
    close(log_file)
    return(list(filled=base_img, mask=final_masks))
}
  #+END_SRC
* auto_gap_fill.R
 #+BEGIN_SRC R 
pct_gap <- function(gap_mask) {
    num_gap <- cellStats(gap_mask == 1, stat='sum', na.rm=TRUE)
    num_clear <- cellStats(gap_mask == 0, stat='sum', na.rm=TRUE)
    return((num_gap / num_clear) * 100)
}
#' Automated removal of gaps in SLC-off images using GNSPI
#'
#' Uses the GNSPI algorithm from Zhu et al. See \code{\link{fill_gaps}} for 
#' details.  In hilly areas, gap fill should be done after topographic 
#' correction.
#'
#' The \code{auto_gap_fill} function allows an analyst to automatically 
#' construct a gap-filled image after specifying: \code{data_dir} (a folder of 
#' Landsat images), \code{wrspath} and \code{wrsrow} (the WRS-2 path/row to 
#' use), and \code{start_date} and \code{end_date} (a start and end date 
#' limiting the images to use in the algorithm).  The analyst can also 
#' optionally specify a \code{base_date}, and the \code{auto_gap_fill} function 
#' will automatically pick the image closest to that date to use as the base 
#' image.
#' 
#' As the \code{auto_gap_fill} function automatically chooses images for 
#' inclusion in the gap fill process, it relies on having images stored on disk 
#' in a particular way. To ensure that images are correctly stored on your hard 
#' disk, use the \code{\link{auto_preprocess_landsat}} function to extract the 
#' original Landsat CDR hdf files from the USGS archive. The 
#' \code{auto_preprocess_landsat} function will ensure that images are 
#' extracted and renamed properly so that they can be used with the 
#' \code{auto_gap_fill} script.
#'
#' @export
#' @importFrom spatial.tools sfQuickInit sfQuickStop
#' @importFrom lubridate as.duration new_interval
#' @importFrom stringr str_extract
#' @importFrom SDMTools ConnCompLabel
#' @param data_dir folder where input images are located, with filenames as 
#' output by the \code{\link{auto_preprocess_landsat}} function. This folder 
#' will be searched recursively for images (taking the below path/row, date, 
#' and topographic correction options into account).
#' @param wrspath World Reference System (WRS) path
#' @param wrsrow World Reference System (WRS) row
#' @param start_date start date of period from which images will be chosen to 
#' fill cloudy areas in the base image (as \code{Date} object)
#' @param end_date end date of period from which images will be chosen to fill 
#' cloudy areas in the the base image (as \code{Date} object)
#' @param base_date ideal date for base image (base image will be chosen as the 
#' image among the available images that is closest to this date). If NULL, 
#' then the base image will be the image with the lowest cloud cover.
#' @param tc if \code{TRUE}, use topographically corrected imagery as output by 
#' \code{auto_preprocess_landsat}. IF \code{FALSE} use bands 1-5 and 7 surface 
#' reflectance as output by \code{unstack_ledaps} or 
#' \code{auto_preprocess_landsat} (if \code{auto_preprocess_landsat} was also 
#' run with tc=FALSE).
#' @param threshold maximum percent gap allowable in base image. Gap fill will 
#' not occur unless percent gap in base image is greater than this value.
#' @param n_cpus the number of CPUs to use for processes that can run in 
#' parallel
#' @param notify notifier to use (defaults to \code{print} function).  See the 
#' \code{notifyR} package for one way of sending notifications from R.  The 
#' \code{notify} function should accept a string as the only argument.
#' @param verbose whether to print detailed status messages
#' @param ... additional arguments passed to \code{\link{fill_gaps}}, such as 
#' \code{DN_min}, \code{DN_max}, \code{use_IDL}, \code{verbose}, etc. See 
#' \code{\link{fill_gaps}}.
#' @return \code{Raster*} object with gap filled image.
#' @references Zhu, X., Liu, D., Chen, J., 2012. A new geostatistical approach 
#' for filling gaps in Landsat ETM+ SLC-off images. Remote Sensing of 
#' Environment 124, 49--60.
auto_gap_fill <- function(data_dir, wrspath, wrsrow, start_date, end_date, 
                          base_date=NULL, tc=TRUE, threshold=1, n_cpus=1, 
                          notify=print, verbose=TRUE, ...) {
    stop('auto_gap_fill not yet supported')
    if (!file_test('-d', data_dir)) {
        stop('data_dir does not exist')
    }
    timer <- Track_time(notify)
    timer <- start_timer(timer, label='Gap fill')
    if (n_cpus > 1) sfQuickInit(n_cpus)
    wrspath <- sprintf('%03i', wrspath)
    wrsrow <- sprintf('%03i', wrsrow)
    # Find image files based on start and end dates
    prefix_re <- ^([a-zA-Z]*_)?
    #pathrow_re <-[012][0-9]{2}-[012][0-9]{2}
    pathrow_re <- paste(wrspath, wrsrow, sep='-')
    date_re <-((19)|(2[01]))[0-9]{2}-[0123][0-9]{2}
    sensor_re <-((L[45]T)|(L7E)|(L8C))SR
    if (tc) {
        suffix_re <- '_tc.tif$'
    } else {
        suffix_re <- '.tif$'
    }
    file_re <- paste0(prefix_re, paste(pathrow_re, date_re, sensor_re, 
                                       sep='_'), suffix_re)
    img_files <- dir(data_dir, pattern=file_re, recursive=TRUE)
    img_dates <- str_extract(basename(img_files), date_re)
    img_dates <- as.Date(img_dates, '%Y-%j')
    which_files <- which((img_dates >= start_date) &
                          (img_dates < end_date))
    img_dates <- img_dates[which_files]
    img_files <- file.path(data_dir, img_files[which_files])
    if (length(img_files) == 0) {
        stop('no images found - check date_dir, check wrspath, wrsrow, start_date, and end_date')
    } else if (length(img_files) <= 2) {
        stop(paste('Only', length(img_files),
                   'image(s) found. Need at least two images to perform gap fill'))
    }
    if (verbose) {
        notify(paste('Found', length(img_files), 'image(s)'))
        timer <- start_timer(timer, label='Analyzing cloud cover and gaps in input images')
    }
    # Run QA stats - remember band 1 is fmask band, and band 2 is fill_QA
    masks <- list()
    imgs <- list()
    for (img_file in img_files) {
        masks_file <- gsub(suffix_re, '_masks.tif', img_file)
        this_mask <- raster(masks_file, band=2)
        masks <- c(masks, this_mask)
        this_img <- stack(img_file)
        imgs <- c(imgs, stack(this_img))
    }
    freq_table <- freq(stack(masks), merge=TRUE)
    # Convert frequency table to fractions
    freq_table[-1] <- freq_table[-1] / colSums(freq_table[-1], na.rm=TRUE)
    if (verbose) {
        timer <- stop_timer(timer, label='Analyzing cloud cover and gaps in input images')
    }
    # Find image that is either closest to base date, or has the maximum 
    # percent not in cloud or gap
    if (is.null(base_date)) {
        clear_row <- which(is.na(freq_table$value))
        base_img_index <- which(freq_table[clear_row, -1] == 
                                max(freq_table[clear_row, -1]))
    } else {
        base_date_diff <- lapply(img_dates, function(x) 
                                 as.duration(new_interval(x, base_date)))
        base_date_diff <- abs(unlist(base_date_diff))
        base_img_index <- which(base_date_diff == min(base_date_diff))
    }
    # Convert masks to binary indicating: 0=other; 1=gap, shadow, or cloud.  
    # Note that gaps are coded as NAs in the fmask band.
    #
    #   band1: fmask_band
    #       0 = clear
    #       1 = water
    #       2 = cloud_shadow
    #       3 = snow
    #       4 = cloud
    #       NA = gap or background
    #   band 2: fill_QA
    #      	0 = not fill
    #    	255 = fill
    for (n in 1:length(masks)) {
        masks[n] <- (is.na(masks[[n]])) | (masks[[n]] == 2) | (masks[[n]] == 4)
    }
    # Code areas of imgs that are background, gap, cloud, or shadow as 0
    for (n in 1:length(imgs)) {
        imgs[n][masks[[1]] == 1] <- 0
    }
    base_img <- imgs[[base_img_index]]
    imgs <- imgs[-base_img_index]
    base_mask <- masks[[base_img_index]]
    masks <- masks[-base_img_index]
    base_img_date <- img_dates[base_img_index]
    img_dates <- img_dates[-base_img_index]
    # Save base_img in filled so it will be returned if base_img already has 
    # pct_gap below threshold
    start_pct_gap <- pct_gap(base_mask)
    if (verbose) {
        notify(paste0('Base image has ', round(start_pct_gap, 2), '% gap before fill'))
    }
    if (start_pct_gap > threshold) {
        if (verbose) {
            timer <- start_timer(timer, label='Performing gap fill')
        }
        # Calculate a raster indicating the pixels in each potential fill image 
        # that are available for filling pixels of base_img that are missing 
        # due to SLC-off gaps. Areas coded 1 are missing due to gaps in the 
        # base image and are available (i.e. are not gaps or clouds) in the 
        # merge image.
        fill_areas <- list()
        for (mask_img in masks) {
            fill_areas <- c(fill_areas, list(base_mask == 1 & mask_img == 0))
        }
        fill_areas_freq <- freq(stack(fill_areas), useNA='no', merge=TRUE)
        # Select the fill image with the maximum number of available pixels 
        # (counting only pixels in the fill image that are not ALSO in gaps or 
        # clouded in the fill image)
        avail_fill_row <- which(fill_areas_freq$value == 1)
        fill_img_index <- which(fill_areas_freq[avail_fill_row, -1] == 
                                max(fill_areas_freq[avail_fill_row, -1]))
        fill_img <- imgs[[fill_img_index]]
        imgs <- imgs[-fill_img_index]
        cloud_mask <- fill_areas[[fill_img_index]]
        fill_img_mask <- masks[[fill_img_index]]
        masks <- masks[-fill_img_index]
        fill_img_date <- img_dates[fill_img_index]
        img_dates <- img_dates[-fill_img_index]
        # Mark areas of the cloud_mask where fill_img is blank (clouded) with 
        # -1
        coded_cloud_mask[fill_img_mask] <- -1
        NAvalue(coded_cloud_mask) <- -2
        if (verbose) {
            notify(paste0('Filling image from ', base_img_date,
                          ' with image from ', fill_img_date, 'as input image...'))
        }
        filled <- fill_gaps(base_img, fill_img, imgs, verbose=verbose, ...)
        if (verbose) {
            notify('Fill complete.')
        }
        # Revise base mask to account for newly filled pixels
        # TODO: Fix this
        base_mask[coded_cloud_mask >= 1] <- 0
        max_iter <- max_iter + 1
        if (verbose) {
            final_pct_gap <- pct_gap(base_mask)
            notify(paste0('Base image has ', round(final_pct_gap, 2), '% gap remaining'))
            timer <- stop_timer(timer, label='Performing gap fill')
        }
    } else {
        notify('Percent gap < threshold. Skipping gap fill.')
    }
    timer <- stop_timer(timer, label='Gap fill')
    if (n_cpus > 1) sfQuickStop(n_cpus)
    return(filled)
}
  #+END_SRC
* auto_normalize.R
 #+BEGIN_SRC R 
#' Normalize a set of preprocessed CDR images to a base image
#'
#' This function uses model II regression to perform relative normalization to 
#' match a set of Landsat CDR surface reflectance images. The function assumes 
#' the images were preprocessed using the \code{auto_preprocess_landsat} 
#' function. A base image can be optionally supplied. If a base image is not 
#' supplied, then the function will calculate the percent cloud cover of each 
#' input image, and automatically choose the image with the least cloud cover 
#' as the base image. This function assumes that the images (and image masks) 
#' were preprocessed using the \code{auto_preprocess_landsat} function.
#'
#' This function will run in parallel if a parallel backend is registered with 
#' \code{\link{foreach}}.
#'
#' @export
#' @import foreach
#' @importFrom tools file_path_sans_ext
#' @param image_files list of filenames for images to normalize
#' @param base (optional) filename of base image. If not supplied, the base 
#' image will be automatically chosen from the images in \code{image_files}, as 
#' the image with the lowest percent cloud cover.
#' @param overwrite whether to overwrite existing files
#' @return nothing - used for side effect of normalizing imagery
auto_normalize <- function(image_files, base, overwrite=FALSE) {
    stopifnot(length(image_files) >= 1)
    image_stacks <- lapply(image_files, stack)
    mask_files <- paste0(file_path_sans_ext(image_files), '_masks', 
                         extension(image_files))
    mask_stacks <- lapply(mask_files, stack)
    if (!missing(base)) {
        base_img_file <- base
    } else if (missing(base) & (length(image_files) == 1)) {
        stop('length of image_files is 1 but no base image was supplied')
    } else {
        # Figure out which image has lowest percent cloud cover - use that 
        # image as the base image
        pct_clouds <- function(cloud_mask) {
            clouded_pixels <- calc(cloud_mask, fun=function(vals) {
                # For fmask layer, 2 is cloud shadow, and 4 is cloud
                (vals == 2) | (vals == 4)
            })
            num_clouds <- cellStats(clouded_pixels, stat='sum', na.rm=TRUE)
            # For fmask layer, 255 is fill
            num_clear <- cellStats(cloud_mask != 255, stat='sum', na.rm=TRUE)
            return((num_clouds / (num_clouds + num_clear)) * 100)
        }
        cloud_cover <- foreach(mask_stack=iter(mask_stacks),
                 .packages=c('teamlucc', 'stringr', 'rgdal'),
                 .combine=c) %dopar% {
            # Note that fmask layer is 2nd layer in stack
            pct_clouds(mask_stack[[2]])
        }
        base_index <- which(cloud_cover == min(cloud_cover))
        
        base_img <- image_stacks[[base_index]]
        image_stacks <- image_stacks[-base_index]
        base_img_file <- image_files[[base_index]]
        image_files <- image_files[-base_index]
        base_mask <- mask_stacks[[base_index]]
        mask_stacks <- mask_stacks[-base_index]
    }
    # Copy the base image to a new file with the _base.tif extension
    base_copy_filename <- paste0(file_path_sans_ext(base_img_file), 
                                 '_normbase.tif')
    base_img <- writeRaster(base_img, filename=base_copy_filename, 
                            datatype=dataType(base_img)[1], 
                            overwrite=overwrite)
    base_mask_copy_filename <- paste0(file_path_sans_ext(base_img_file), 
                                      '_normbase_masks.tif')
    base_mask <- writeRaster(base_mask, filename=base_mask_copy_filename, 
                             datatype=dataType(base_mask)[1], 
                             overwrite=overwrite)
    stopifnot(length(image_files) == length(image_stacks))
    stopifnot(length(image_files) == length(mask_stacks))
    image_file=image_stack=NULL
    # Now normalize each remaining image to the _base.tif file
    foreach (image_file=iter(image_files), image_stack=iter(image_stacks), 
             mask_stack=iter(mask_stacks),
             .packages=c('teamlucc', 'stringr', 'tools')) %dopar% {
        message(paste('Preprocessing ', image_file))
        output_normed_file <- paste0(file_path_sans_ext(image_file), 
                                     '_normalized.tif')
        output_normed_masks_file <- paste0(file_path_sans_ext(image_file), 
                                           '_normalized_masks.tif')
        # Note that fmask layer is 2nd layer in stack
        missing_vals <- overlay(base_mask[[2]], mask_stack[[2]],
                            fun=function(base_vals, this_vals) {
            # Only use clear pixels when normalizing (0 in fmask)
            (base_vals != 0) & (this_vals != 0)
        }, datatype=dataType(base_mask))
        if (ncell(image_stack) > 500000) {
            size <- 500000
        } else {
            size <- ncell(image_stack)
        }
        normed_image <- normalize(base_img, image_stack, missing_vals, size=size)
        normed_image <- writeRaster(normed_image, filename=output_normed_file, 
                                    datatype=dataType(base_img)[1], 
                                    overwrite=overwrite)
        mask_stack <- writeRaster(mask_stack, 
                                  filename=output_normed_masks_file, 
                                  datatype=dataType(mask_stack)[1], 
                                  overwrite=overwrite)
    }
}
  #+END_SRC
* auto_preprocess_landsat.R
 #+BEGIN_SRC R 
get_gdalinfo_item <- function(item, gdalinfo_text) {
    gdalinfo_text <- gdalinfo_text[grepl(paste0('^[ ]*', item), gdalinfo_text)]
    if (length(gdalinfo_text) > 1) stop('more than one item found')
    gdalinfo_text <- gsub(paste0('[ ]*', item, '='), '', gdalinfo_text)
    return(gdalinfo_text)
}
get_mtl_item <- function(item, mtl_txt) {
    mtl_txt <- mtl_txt[grepl(paste0('^[ ]*', item), mtl_txt)]
    if (length(mtl_txt) > 1) stop('more than one item found')
    mtl_txt <- gsub(paste0('[ ]*', item, ' = '), '', mtl_txt)
    # Strip leading/following quotes
    mtl_txt <- gsub('^', '', mtl_txt)
    mtl_txt <- gsub('$', '', mtl_txt)
    return(mtl_txt)
}
#' @importFrom stringr str_extract
#' @importFrom gdalUtils gdalinfo
get_metadata <- function(ls_file, img_type) {
    meta <- list()
    if (img_type == CDR) {
        ls_file_gdalinfo <- gdalinfo(ls_file)
        aq_date <- get_gdalinfo_item('AcquisitionDate', ls_file_gdalinfo)
        meta$aq_date <- strptime(aq_date, format=%Y-%m-%dT%H:%M:%OSZ, tz=UTC)
        meta$WRS_Path <- sprintf('%03i', as.numeric(get_gdalinfo_item('WRS_Path', ls_file_gdalinfo)))
        meta$WRS_Row <- sprintf('%03i', as.numeric(get_gdalinfo_item('WRS_Row', ls_file_gdalinfo)))
        meta$sunelev <- 90 - as.numeric(get_gdalinfo_item('SolarZenith', ls_file_gdalinfo))
        meta$sunazimuth <- as.numeric(get_gdalinfo_item('SolarAzimuth', ls_file_gdalinfo))
        meta$short_name  <- get_gdalinfo_item('ShortName', ls_file_gdalinfo)
    } else if (img_type == L1T) {
        if (!grepl(_MTL.txt$, ls_file)) {
            stop(ls_file must be a *_MTL.txt file)
        }
        mtl_txt <- readLines(ls_file, warn=FALSE)
        aq_date <- get_mtl_item('DATE_ACQUIRED', mtl_txt)
        aq_time <- get_mtl_item('SCENE_CENTER_TIME', mtl_txt)
        meta$aq_date <- strptime(paste0(aq_date, T, aq_time), format=%Y-%m-%dT%H:%M:%OSZ, tz=UTC)
        meta$WRS_Path <- sprintf('%03i', as.numeric(get_mtl_item('WRS_PATH', mtl_txt)))
        meta$WRS_Row <- sprintf('%03i', as.numeric(get_mtl_item('WRS_ROW', mtl_txt)))
        meta$sunelev <- as.numeric(get_mtl_item('SUN_ELEVATION', mtl_txt))
        meta$sunazimuth <- as.numeric(get_mtl_item('SUN_AZIMUTH', mtl_txt))
        # Build a shortname based on satellite and img_type that is consistent 
        # with the format of the CDR image shortnames
        satellite <- str_extract(get_mtl_item('SPACECRAFT_ID', mtl_txt), '[4578]')
        sensor_string <- str_extract(basename(ls_file), '^((LT[45])|(LE7)|(LC8))')
        meta$short_name  <- paste0(substr(sensor_string, 1, 1),
                                   substr(sensor_string, 3, 3),
                                   substr(sensor_string, 2, 2), img_type)
    } else {
        stop(paste(img_type, is not a recognized img_type))
    }
    return(meta)
}
calc_cloud_mask <- function(mask_stack, mask_type, ...) {
    if (mask_type == 'fmask') {
        # Make a mask where clouds and gaps are coded as 1, clear as 0
        # fmask_band key:
        # 	0 = clear
        # 	1 = water
        # 	2 = cloud_shadow
        # 	3 = snow
        # 	4 = cloud
        # 	255 = fill value
        cloud_mask <- calc(mask_stack$fmask_band,
            fun=function(fmask) {
                return((fmask == 2) | (fmask == 4) | (fmask == 255))
            }, datatype='INT2S', ...)
    } else if (mask_type == '6S') {
        # This cloud mask includes the cloud_QA, cloud_shadow_QA, and 
        # adjacent_cloud_QA layers. Pixels in cloud, cloud shadow, or 
        # adjacent cloud are coded as 1.
        cloud_mask <- overlay(mask_stack$fill_QA,
                              mask_stack$cloud_QA, 
                              mask_stack$cloud_shadow_QA, 
                              mask_stack$adjacent_cloud_QA,
            fun=function(fill, clo, sha, adj) {
                return((fill == 255) | (clo == 255) | (sha == 255) | 
                       (adj == 255))
            }, datatype='INT2S', ...)
    } else if (mask_type == 'both') {
        cloud_mask <- overlay(mask_stack$fmask_band, 
                              mask_stack$cloud_QA, 
                              mask_stack$cloud_shadow_QA, 
                              mask_stack$adjacent_cloud_QA,
            fun=function(fmask, clo, sha, adj) {
                return((fmask == 2) | (fmask == 4) | (fmask == 255) | 
                       (clo == 255) | (sha == 255) | (adj == 255))
            }, datatype='INT2S', ...)
    } else {
        stop(paste0('unrecognized option ', cloud_mask, ' for mask_type'))
    }
    return(cloud_mask)
}

#' @importFrom gdalUtils get_subdatasets gdalbuildvrt
build_band_vrt <- function(ls_file, band_vrt_file, img_type) {
    image_bands <- c('band1', 'band2', 'band3', 'band4', 'band5', 'band7')
    if (img_type == CDR) {
        sds <- get_subdatasets(ls_file)
        band_sds <- sds[grepl(paste0(':(', paste(image_bands, collapse='|'), ')$'), sds)]
        gdalbuildvrt(band_sds, band_vrt_file, separate=TRUE)
    } else if (img_type == L1T) {
        if (!grepl(_MTL.txt$, ls_file)) {
            stop(ls_file must be a *_MTL.txt file)
        }
        ls_file_base <- gsub(_MTL.txt, ", ls_file)
        ls_files <- dir(dirname(ls_file_base),
                        pattern=paste0(basename(ls_file_base), '_B[123457].((TIF)|(tif))$'),
                        full.names=TRUE)
        gdalbuildvrt(ls_files, band_vrt_file, separate=TRUE)

    } else {
        stop(paste(img_type, is not a recognized img_type))
    }
    return(image_bands)
}

#' @importFrom gdalUtils get_subdatasets gdalbuildvrt
build_mask_vrt <- function(ls_file, mask_vrt_file, img_type) {
    if (img_type == CDR) {
        mask_bands <- c('fill_QA', 'cfmask_band', 'cloud_QA', 'cloud_shadow_QA', 
                        'adjacent_cloud_QA')
        sds <- get_subdatasets(ls_file)
        # Below is to support CDR imagery downloaded prior to late August 2014
        if (any(grepl(fmask_band, sds))) {
            warning('Using fmask_band instead of newer cfmask_band band name')
            mask_bands[grepl(^cfmask_band$, mask_bands)] <- fmask_band
        }
        mask_sds <- sds[grepl(paste0(':(', paste(mask_bands, collapse='|'), ')$'), sds)]
        stopifnot(length(mask_sds) == 5)
        gdalbuildvrt(mask_sds, mask_vrt_file, separate=TRUE, srcnodata='None')
    } else if (img_type == L1T) {
        mask_bands <- c('fill_QA', 'fmask_band')
        if (!grepl(_MTL.txt$, ls_file)) {
            stop(ls_file must be a *_MTL.txt file)
        }
        ls_file_base <- gsub(_MTL.txt, ", ls_file)

        fmask_file <- dir(dirname(ls_file_base),
                          pattern=paste0(basename(ls_file_base), '_MTLFmask$'),
                          full.names=TRUE)

        # Calculate a QA mask file from the fmask file, since teamlucc expects 
        # this file as part of the mask stack.
        qa_mask_file <- extension(rasterTmpFile(), '.tif')
        # TODO: Check if this is proper coding - should it be reversed?
        qa_mask <- calc(raster(fmask_file),
                        fun=function(x) {
                            out <- x == 255
                            out[x == 255] <- 255
                            return(out)
                        }, datatype=INT2S, filename=qa_mask_file)

        # Note that allow_projection_difference is used below as GDAL thinks 
        # the two images have different projection systems, even though they 
        # are in identical projection systems.
        gdalbuildvrt(c(qa_mask_file, fmask_file), mask_vrt_file, 
                     separate=TRUE, allow_projection_difference=TRUE,
                     srcnodata='None')
    } else {
        stop(paste(img_type, is not a recognized img_type))
    }
    return(mask_bands)
}

#' Preprocess surface reflectance imagery from the Landsat CDR archive
#'
#' This function preprocesses surface reflectance imagery from the Landsat 
#' Climate Data Record (CDR) archive. \code{auto_preprocess_landsat} can 
#' reproject CDR tiles to match the projection of a given \code{aoi}, crop the 
#' tiles to match the \code{aoi} or a common WRS-2 path/row polygon, mask 
#' missing data and clouds out of the CDR tiles, and perform topographic 
#' correction.
#'
#' \code{mask_type} chooses the cloud mask to use if topographic correction is 
#' performed (\code{tc=TRUE}). The mask can be one of three different options: 
#' 6S, fmask, or combined. Each option uses a different combination of 
#' cloud mask layers from the CDR product. The 6S masks out any areas coded 
#' as fill (fill_QA=255), cloud (cloud_QA=255), cloud shadow
#' (cloud_shadow_QA=255) or adjacent to cloud (adjacent_cloud_QA=255). The 
#' fmask option masks out any areas coded as fill (fmask=255), cloud 
#' (fmask=4) or cloud shadow (fmask=2).  The combined option combines the 6S 
#' and fmask approaches to masks out areas coded as fill, cloud, cloud 
#' shadow, or adjacent to cloud using either method. Note that fmask is the 
#' only supported option when \code{img_type} is L1T.
#'
#' Prior to running \code{auto_preprocess_landsat}, \code{\link{espa_extract}} 
#' should be used to extract the original zipfiles supplied by USGS. To perform 
#' topographic correction with \code{auto_preprocess_landsat}, first run 
#' \code{\link{auto_setup_dem}} to preprocess a set of DEM tiles. Then run 
#' \code{auto_preprocess_landsat} with the \code{tc=TRUE} option.
#'
#' If topographic correction is being performed, it will be run in parallel if 
#' a parallel backend is registered with \code{\link{foreach}}.
#'
#' @export
#' @importFrom rgeos gIntersection
#' @importFrom wrspathrow pathrow_poly
#' @importFrom tools file_path_sans_ext
#' @importFrom gdalUtils gdalwarp
#' @importFrom sp is.projected
#' @param image_dirs list of paths to a set of Landsat CDR image files in HDF 
#' format
#' @param prefix string to use as a prefix for all filenames
#' @param img_type type of Landsat imagery to preprocess. Can be CDR for 
#' Landsat Climate Data Record (CDR) imagery in HDR format, or L1T for 
#' Standard Terrain Correction (Level 1T) imagery. Note that if L1T imagery is 
#' used, fmask must be run locally (see https://code.google.com/p/fmask) prior 
#' to using \code{auto_preprocess_landsat}.
#' @param tc whether to topographically correct imagery (if \code{TRUE}, then 
#' \code{dem_path} must be specified)
#' @param dem_path path to a set of DEMs as output by \code{auto_setup_dem} 
#' (only required if tc=TRUE)
#' @param aoi area of interest (AOI), as a \code{SpatialPolygonsDataFrame}.  If 
#' supplied, this aoi is used to crop and set the projection system of the 
#' output. Must be in a projected coordinate system.
#' @param output_path the path to use for the output (optional - if NULL then 
#' output images will be saved alongside the input images in the same folder).
#' @param mask_type which cloud mask to use to mask clouds when performing 
#' topographic correction. Can be one of fmask, 6S, or both.  See 
#' Details.  (Ignored if \code{tc=FALSE)}.
#' @param mask_output if \code{TRUE}, cloud, cloud shadow, and fill areas 
#' (SLC-off gaps and areas with no data) will be set to \code{NA} in the 
#' output. Note this setting affects the final output file only - cloud, cloud 
#' shadow, and gap areas are masked out of the image during topographic 
#' correction regardless of the value of \code{mask_output}.
#' @param n_cpus the number of CPUs to use for processes that can run in 
#' parallel
#' @param cleartmp whether to clear temp files on each run through the loop
#' @param overwrite whether to overwrite existing files (otherwise an error 
#' will be raised)
#' @param of output format to use when saving output rasters. See description 
#' of \code{of} in \code{\link{gdalwarp}}.
#' @param ext file extension to use when saving output rasters (determines 
#' output file format). Should match file extension for output format chosen by 
#' \code{of}.
#' @param notify notifier to use (defaults to \code{print} function). See the 
#' \code{notifyR} package for one way of sending notifications from R. The 
#' \code{notify} function should accept a string as the only argument.
#' @param verbose whether to print detailed status messages and timing 
#' information
#' @return nothing - used for the side effect of preprocessing imagery
#' @seealso \code{\link{espa_extract}}, \code{\link{unstack_ledapscdr}}, 
#' \code{\link{auto_setup_dem}}
auto_preprocess_landsat <- function(image_dirs, prefix, img_type=CDR, 
                                    tc=FALSE, dem_path=NULL, aoi=NULL, 
                                    output_path=NULL, mask_type='fmask', 
                                    mask_output=FALSE, n_cpus=1, 
                                    cleartmp=FALSE,  overwrite=FALSE, 
                                    of=GTiff, ext='tif', notify=print, 
                                    verbose=FALSE) {
    if (grepl('_', prefix)) {
        stop('prefix cannot contain underscores (_)')
    }
    if (tc && is.null(dem_path)) {
        stop('dem_path must be supplied if tc=TRUE')
    }
    if (tc && !file_test(-d, dem_path)) {
        stop(paste(dem_path, does not exist))
    }
    if (!is.null(output_path) && !file_test(-d, output_path)) {
        stop(paste(output_path, does not exist))
    }
    if (!is.null(aoi)) {
        if (length(aoi) > 1) {
            stop('aoi should be a SpatialPolygonsDataFrame of length 1')
        }
        stopifnot(is.projected(aoi))
    }

    ext <- gsub('^[.]', '', ext)

    # Setup a regex to identify Landsat CDR images
    if (img_type == CDR) {
        ls_regex <- '^(lndsr.)?((LT4)|(LT5)|(LE7)|(LC8))[0-9]{6}[12][0-9]{6}[a-zA-Z]{3}[0-9]{2}.hdf$'
    } else if (img_type == L1T) {
        ls_regex <- '((LT[45])|(LE7)|(LC8))[0-9]{6}[12][0-9]{6}[a-zA-Z]{3}[0-9]{2}_MTL.txt$'
    } else {
        stop(paste(img_type, is not a recognized img_type))
    }

    if (img_type == CDR) {
        stopifnot(mask_type %in% c('fmask', '6S', 'both'))
    } else if (img_type == L1T) {
        stopifnot(mask_type == 'fmask')
    }

    ls_files <- c()
    for (image_dir in image_dirs) {
        if (!file_test(-d, image_dir)) {
            stop(paste(image_dir, does not exist))
        }
        ls_files <- c(ls_files, dir(image_dir, pattern=ls_regex, full.names=TRUE))
    }

    if (length(ls_files) == 0) {
        stop(paste0('No Landsat files found using img_type=', img_type, '.'))
    }

    for (ls_file in ls_files) {
        ######################################################################
        # Determine image basename for use in naming subsequent files
        meta <- get_metadata(ls_file, img_type)

        image_basename <- paste0(meta$WRS_Path, '-', meta$WRS_Row, '_',
                                 format(meta$aq_date, '%Y-%j'), '_', meta$short_name)

        if (is.null(output_path)) {
            this_output_path <- dirname(ls_file)
        } else {
            this_output_path  <- output_path
        }

        if (tc) {
            output_filename <- file.path(this_output_path,
                                         paste0(prefix, '_', image_basename, 
                                                '_tc.', ext))
        } else {
            # Skip topographic correction, so don't append _tc to filename
            output_filename <- file.path(this_output_path,
                                         paste0(prefix, '_', image_basename, 
                                                '.', ext))
        }

        log_file <- file(paste0(file_path_sans_ext(output_filename), '_log.txt'), open=wt)
        msg <- function(txt) {
            cat(paste0(txt, '\n'), file=log_file, append=TRUE)
            print(txt)
        }

        timer <- Track_time(msg)
        timer <- start_timer(timer, label=paste('Preprocessing', image_basename))

        #######################################################################
        # Crop and reproject images to match the projection being used for this 
        # image.  This is either the projection of the aoi (if aoi is 
        # supplied), or the UTM zone of the centroid of this path and row.
        if (verbose) timer <- start_timer(timer, label='cropping and reprojecting')

        band_vrt_file <- extension(rasterTmpFile(), '.vrt')
        band_names <- build_band_vrt(ls_file, band_vrt_file, img_type)
        mask_vrt_file <- extension(rasterTmpFile(), '.vrt')
        mask_band_names <- build_mask_vrt(ls_file, mask_vrt_file, img_type)

        this_pathrow_poly <- pathrow_poly(as.numeric(meta$WRS_Path), 
                                          as.numeric(meta$WRS_Row))
        if (!is.null(aoi)) {
            to_srs <- proj4string(aoi)
        } else {
            to_srs <- utm_zone(this_pathrow_poly, proj4string=TRUE)
        }

        # Calculate minimum bounding box coordinates:
        this_pathrow_poly <- spTransform(this_pathrow_poly, CRS(to_srs))
        if (!is.null(aoi)) {
            # If an aoi IS supplied, match the image extent to that of the AOI 
            # cropped to the appropriate Landsat path/row polygon.
            crop_area <- gIntersection(this_pathrow_poly, aoi, byid=TRUE)
        } else {
            # If an aoi IS NOT supplied, match the image extent to the 
            # appropriate Landsat path/row polygon.
            crop_area <- this_pathrow_poly
        }
        out_te <- as.numeric(bbox(crop_area))

        # Ensure origin is set at 0,0
        to_res <- c(30, 30)
        out_te <- normalize_extent(out_te, to_res)

        image_stack_reproj_file <- extension(rasterTmpFile(), ext)
        image_stack <- gdalwarp(band_vrt_file,
                                dstfile=image_stack_reproj_file,
                                te=out_te, t_srs=to_srs, tr=to_res, 
                                r='cubicspline', output_Raster=TRUE, of=of, 
                                multi=TRUE, wo=paste0(NUM_THREADS=, n_cpus), 
                                overwrite=overwrite, ot='Int16')
        names(image_stack) <- band_names

        mask_stack_reproj_file <- extension(rasterTmpFile(), paste0('.', ext))
        mask_stack <- gdalwarp(mask_vrt_file,
                               dstfile=mask_stack_reproj_file,
                               te=out_te, t_srs=to_srs, tr=to_res, 
                               r='near', output_Raster=TRUE, of=of, 
                               multi=TRUE, wo=paste0(NUM_THREADS=, n_cpus), 
                               overwrite=overwrite, ot='Int16')
        # Can't just directly assign mask_bands as the names since the bands 
        # may have been read in different order from the HDF file
        names(mask_stack) <- mask_band_names

        if (verbose) timer <- stop_timer(timer, label='cropping and reprojecting')

        ######################################################################
        # Perform topographic correction if tc=TRUE
        if (tc) {
            if (verbose) timer <- start_timer(timer, label='topocorr')

            ######################################################################
            # Load dem, slope, and aspect
            slopeaspect_filename <- file.path(dem_path,
                                              paste0('slopeaspect_', 
                                                     meta$WRS_Path, '-', meta$WRS_Row, '.', ext))
            slopeaspect <- brick(slopeaspect_filename)

            if (!proj4comp(proj4string(image_stack), proj4string(slopeaspect))) {
                stop(paste0('slopeaspect and image_stack projections do not match.\nslopeaspect proj4string: ', 
                            proj4string(slopeaspect), '\nimage_stack proj4string: ',
                            proj4string(image_stack)))
            } else {
                # Projection strings are functionally identical - so make sure 
                # their textual representations are the same.
                proj4string(slopeaspect) <- proj4string(image_stack)
            }

            compareRaster(slopeaspect, image_stack, orig=TRUE)

            image_stack_mask <- calc_cloud_mask(mask_stack, mask_type)

            image_stack_masked <- image_stack
            image_stack_masked[image_stack_mask] <- NA
            if (ncell(image_stack_masked) > 500000) {
                # Draw a sample for the Minnaert k regression. Note that 
                # sampleRegular with cells=TRUE returns cell numbers in the 
                # first column
                sampleindices <- sampleRegular(image_stack_masked, size=500000, 
                                               cells=TRUE)
                sampleindices <- as.vector(sampleindices[, 1])
            } else {
                sampleindices <- NULL
            }
            # Remember that slopeaspect layers are scaled to INT2S, but 
            # topographic_corr expects them as floats, so apply the scale factors 
            # used in auto_setup_dem
            slopeaspect_flt <- stack(raster(slopeaspect, layer=1) / 10000,
                                     raster(slopeaspect, layer=2) / 1000)
            image_stack_tc <- topographic_corr(image_stack_masked, 
                                               slopeaspect_flt, meta$sunelev, 
                                               meta$sunazimuth, 
                                               method='minnaert_full', 
                                               asinteger=TRUE, 
                                               sampleindices=sampleindices)
            if (!mask_output) {
                # Add back in the original values of areas that were masked out 
                # from the topographic correction:
                image_stack_tc[image_stack_mask] <- image_stack[image_stack_mask]
            }
            image_stack <- image_stack_tc
            
            if (verbose) timer <- stop_timer(timer, label='topocorr')
        }

        ######################################################################
        # Write final data
        if (verbose) timer <- start_timer(timer, label='writing data')

        mask_stack_path <- paste0(file_path_sans_ext(output_filename), 
                                  '_masks.', ext)
        mask_stack <- writeRaster(stack(mask_stack$fill_QA,
                                        mask_stack$fmask_band),
                                  filename=mask_stack_path, 
                                  overwrite=overwrite, datatype='INT2S')
        names(mask_stack) <- c('fill_QA', 'fmask_band')

        image_stack <- writeRaster(image_stack, filename=output_filename, 
                                   overwrite=overwrite, datatype='INT2S')
        if (verbose) timer <- stop_timer(timer, label='writing data')

        timer <- stop_timer(timer, label=paste('Preprocessing', image_basename))

        close(log_file)

        if (cleartmp) removeTmpFiles(h=1)
    }
}

  #+END_SRC
* auto_QA_stats.R
 #+BEGIN_SRC R 
# Function for retrieving frequencies from a raster frequency table, given a 
# band name and value. Handles the case of a given code not occurring in a 
# particular raster, in which case it will not show up as a row in the 
# frequency table.
get_freq <- function(band, value, freq_table) {
    band_col <- grep(band, names(freq_table))
    if (!(value %in% freq_table[, 1])) {
        # Return 0 if this value never shows up in the raster
        return(0)
    }
    frac <- freq_table[freq_table[1] == value, band_col]
    if (is.na(frac)) {
        return(0)
    } else {
        return(round(frac, 4))
    }
}
#' Calculate statistics on imagery within an AOI
#'
#' @export
#' @importFrom stringr str_extract
#' @param image_dirs list of paths to a set of Landsat CDR image files in 
#' GeoTIFF format as output by the \code{unstack_ledapscdr} function.
#' @param aoi an area of interest (AOI) to crop from each image
#' @return a \code{data.frame}
auto_QA_stats <- function(image_dirs, aoi) {
    lndsr_regex <- '^(lndsr.)?((LT4)|(LT5)|(LE7)|(LC8))[0-9]{6}[12][0-9]{6}[a-zA-Z]{3}[0-9]{2}'
    mask_bands <- c('fill_QA', 'fmask_band')
    out <- c()
    for (image_dir in image_dirs) {
        lndsr_files <- dir(image_dir, pattern=lndsr_regex)
        image_basenames <- unique(str_extract(lndsr_files,lndsr_regex))
        if (length(image_basenames) == 0) {
            stop(paste('no files found in', image_dir))
        }
        for (image_basename in image_basenames) {
            message(paste0('Processing ', image_basename, '...'))
            metadata_string <- str_extract(image_basename, 
                                           '((LT4)|(LT5)|(LE7)|(LC8))[0-9]{13}')
            sensor <- str_extract(metadata_string, '^((LT[45])|(LE7)|(LC8))')
            year <- substr(metadata_string, 10, 13)
            julian_day <- substr(metadata_string, 14, 16)
            img_path <- substr(metadata_string, 4, 6)
            img_row <- substr(metadata_string, 7, 9)
            mask_band_files <- c()
            for (mask_band in mask_bands) {
                mask_band_files <- c(mask_band_files,
                                     paste(file.path(image_dir, 
                                                     image_basename), 
                                           mask_band, sep='_'))
            }
            mask_band_files <- paste0(mask_band_files, '.tif')
            mask_stack <- stack(mask_band_files)
            names(mask_stack) <- mask_bands
            if (!missing(aoi)) {
                if (proj4string(aoi) != proj4string(mask_stack)) {
                    if (class(aoi) == 'Raster') {
                        aoi <- projectRaster(aoi, mask_stack)
                    } else {
                        aoi <- spTransform(aoi, CRS(proj4string(mask_stack)))
                    }
                }
                mask_stack <- crop(mask_stack, aoi)
                mask_stack <- mask(mask_stack, aoi)
            }
            freq_table <- freq(mask_stack, useNA='no', merge=TRUE)
            # Convert frequency table to fractions
            freq_table[-1] <- freq_table[-1] / colSums(freq_table[-1], na.rm=TRUE)
            out <- c(out, list(list(img_path,
                                    img_row,
                                    year,
                                    julian_day,
                                    sensor,
                                    get_freq('fill_QA', 0, freq_table),
                                    get_freq('fill_QA', 255, freq_table),
                                    get_freq('fmask_band', 0, freq_table),
                                    get_freq('fmask_band', 1, freq_table),
                                    get_freq('fmask_band', 2, freq_table),
                                    get_freq('fmask_band', 3, freq_table),
                                    get_freq('fmask_band', 4, freq_table),
                                    get_freq('fmask_band', 255, freq_table))))
        }
    }
    out <- data.frame(matrix(unlist(out), nrow=length(out), byrow=T))
    names(out) <- c('path', 'row', 'year', 'julian', 'sensor',
                    'fill_QA_notfill', 'fill_QA_fill', 'fmask_clear', 
                    'fmask_water', 'fmask_cloud_shadow', 'fmask_snow', 
                    'fmask_cloud', 'fmask_fill')
    return(out)
}
  #+END_SRC
* auto_setup_dem.R
 #+BEGIN_SRC R 
normalize_extent <- function(te, res=c(30, 30)) {
    # Setup xmin
    te[1] <- round(te[1] - te[1] %% res[1])
    # Setup ymin
    te[2] <- round(te[2] - te[2] %% res[2])
    # Setup xmax
    te[3] <- round(te[3] + res[1] - te[3] %% res[1])
    # Setup ymax
    te[4] <- round(te[4] + res[2] - te[4] %% res[2])
    stopifnot(round(te[1] / res[1]) == (te[1] / res[1]))
    stopifnot(round(te[2] / res[2]) == (te[2] / res[2]))
    stopifnot(round(te[3] / res[1]) == (te[3] / res[1]))
    stopifnot(round(te[4] / res[2]) == (te[4] / res[2]))
    return(te)
}
#' Setup the DEM mosaic for a given AOI
#'
#' This function will setup a set of DEM tiles for each the Landsat path/row 
#' needed to cover a given AOI. The tiles can optionally be cropped to cover 
#' only the portion of each path/row that is included in the AOI, or can cover 
#' the full scene for each path/row needed to cover the AOI.
#'
#' This function uses \code{gdalUtils}, which requires a local GDAL 
#' installation.  See http://trac.osgeo.org/gdal/wiki/DownloadingGdalBinaries 
#' or http://trac.osgeo.org/osgeo4w/ to download the appropriate installer for 
#' your operating system.
#'
#' @export
#' @importFrom wrspathrow pathrow_num
#' @importFrom rgdal readOGR writeOGR
#' @importFrom sp spTransform is.projected
#' @importFrom rgeos gBuffer gIntersects gUnaryUnion gIntersection
#' @importFrom tools file_path_sans_ext
#' @importFrom gdalUtils mosaic_rasters gdalwarp
#' @param aoi area of interest (AOI), as a \code{SpatialPolygonsDataFrame}, to 
#' use as as bounding box when selecting DEMs. Also used to crop and set 
#' projection of the output DEM(s) if \code{crop_to_aoi=TRUE}. Must be in a 
#' projected coordinate system.
#' @param output_path the path to use for the output
#' @param dem_extents a \code{SpatialPolygonsDataFrame} of the extents and 
#' filenames for a set of locally available DEM raster(s) that cover the 
#' \code{aoi}. See the \code{\link{get_extent_polys}} function for one means of 
#' generating this list. \code{dem_extents} must have a filename column.
#' @param of output format to use when saving output rasters. See description 
#' of \code{of} in \code{\link{gdalwarp}}.
#' @param ext file extension to use when saving output rasters (determines 
#' output file format). Should match file extension for output format chosen by 
#' \code{of}.
#' @param n_cpus the number of CPUs to use for processes that can run in 
#' parallel
#' @param overwrite whether to overwrite existing files (otherwise an error 
#' will be raised)
#' @param crop_to_aoi whether to crop the dem to the supplied AOI, or to the
#' Landsat path/row polygon for that particular path/row
#' @param notify notifier to use (defaults to \code{print} function). See the 
#' \code{notifyR} package for one way of sending notifications from R. The 
#' \code{notify} function should accept a string as the only argument.
#' @param verbose whether to print detailed status messages and timing 
#' information
#' @return nothing - used for the side effect of setting up DEMs
auto_setup_dem <- function(aoi, output_path, dem_extents, of=GTiff, 
                           ext='tif', n_cpus=1, overwrite=FALSE, 
                           crop_to_aoi=FALSE, notify=print, verbose=FALSE) {
    if (!file_test(-d, output_path)) {
        stop(paste(output_path, does not exist))
    }
    if (length(aoi) > 1) {
        stop('aoi should be a SpatialPolygonsDataFrame of length 1')
    }
    stopifnot(is.projected(aoi))
    ext <- gsub('^[.]', '', ext)
    timer <- Track_time(notify)
    pathrows <- pathrow_num(aoi, wrs_type=2, wrs_mode='D', as_polys=TRUE)
    aoi_prproj <- spTransform(aoi, CRS(proj4string(pathrows)))
    timer <- start_timer(timer, label=paste('Processing DEMS for', nrow(pathrows), 
                                            'path/rows'))
    if (crop_to_aoi) {
        # Do a rough crop of the pathrows to the AOI in the pathrow CRS 
        # (pathrow will later be cropped in the AOI CRS).
        pathrows_cropped <- gIntersection(pathrows, aoi_prproj, byid=TRUE)
        row.names(pathrows_cropped) <- row.names(pathrows)
        pathrows_cropped <- SpatialPolygonsDataFrame(pathrows_cropped, 
                                                     data=pathrows@data)
    } else {
        pathrows_cropped <- pathrows
    }
    # Add a 500 m buffer in UTM coordinate system, as 1) slope calculation 
    # requires a window of pixels, and 2) this buffer also helps avoid missing 
    # pixels on the sides of the DEM due to slight misalignments from the 
    # reprojection that will occur later. After buffering transform back to 
    # WGS84 to use for preliminary cropping of the dem mosaic. 
    pathrows_utm <- spTransform(pathrows_cropped,
                                CRS(utm_zone(pathrows_cropped, proj4string=TRUE)))
    pathrows_buffered <- spTransform(gBuffer(pathrows_utm, width=500, byid=TRUE), 
                                 CRS(proj4string(dem_extents)))
    intersecting <- as.logical(gIntersects(dem_extents, 
                                           gUnaryUnion(pathrows_buffered), byid=TRUE))
    if (sum(intersecting) == 0) {
        stop('no intersecting dem extents found')
    } else {
        dem_extents <- dem_extents[intersecting, ]
    }
    dem_list <- dem_extents$filename
    dem_rasts <- lapply(dem_list, raster)
    if (length(dem_list) > 1) {
        ######################################################################
        # Verify projections of DEMs match
        dem_prj <- projection(dem_rasts[[1]])
        if (any(lapply(dem_rasts, projection) != dem_prj)) {
            stop(each DEM in dem_list must have the same projection)
        }
        ######################################################################
        # Mosaic DEMs
        if (verbose) timer <- start_timer(timer, label='Mosaicking DEMs')
        mosaic_file <- extension(rasterTmpFile(), ext)
        # Calculate minimum bounding box coordinates:
        mosaic_te <- as.numeric(bbox(pathrows_buffered))
        # Use mosaic_rasters from gdalUtils for speed:
        mosaic_rasters(dem_list, mosaic_file, te=mosaic_te, of=of, 
                       overwrite=overwrite, ot='Int16')
        dem_mosaic <- raster(mosaic_file)
        if (verbose) timer <- stop_timer(timer, label='Mosaicking DEMs')
    } else {
        dem_mosaic <- dem_rasts[[1]]
        mosaic_file <- filename(dem_mosaic)
    }
    for (n in 1:length(pathrows)) {
        pathrow <- pathrows[n, ]
        pathrow_label <- paste(sprintf('%03i', pathrow@data$PATH), 
                               sprintf('%03i', pathrow@data$ROW), sep='-')
        timer <- start_timer(timer, label=paste0('Processing ', n, ' of ', 
                                                 nrow(pathrows), ': ', 
                                                 pathrow_label))
        if (verbose) timer <- start_timer(timer,
                                          label=paste('Cropping/reprojecting DEM mosaic crop for', 
                                          pathrow_label))
        if (crop_to_aoi) {
            to_srs <- proj4string(aoi)
            pathrow_tosrs <- spTransform(pathrow, CRS(to_srs))
            to_ext <- extent(gIntersection(pathrow_tosrs, aoi, byid=TRUE))
        } else {
            to_srs <- utm_zone(pathrow, proj4string=TRUE)
            to_ext <- projectExtent(pathrow, to_srs)
        }
        dem_te <- as.numeric(bbox(to_ext))
        # Ensure origin is set at 0,0
        to_res <- c(30, 30)
        dem_te <- normalize_extent(dem_te, to_res)
        # Calculate minimum bounding box coordinates:
        dem_mosaic_crop_filename <- file.path(output_path,
                                         paste0('dem_', pathrow_label, 
                                                '.', ext))
        dem_mosaic_crop <- gdalwarp(mosaic_file, 
                                    dstfile=dem_mosaic_crop_filename,
                                    te=dem_te, t_srs=to_srs, tr=to_res, 
                                    r='cubicspline', output_Raster=TRUE, 
                                    multi=TRUE, of=of,
                                    wo=paste0(NUM_THREADS=, n_cpus), 
                                    overwrite=overwrite, ot='Int16')
        if (verbose) timer <- stop_timer(timer,
                                         label=paste('Cropping/reprojecting DEM mosaic crop for', 
                                         pathrow_label))
        if (verbose) timer <- start_timer(timer, label=paste('Calculating slope/aspect for', 
                                                pathrow_label))
        slopeaspect_filename <- file.path(output_path,
                                          paste0('slopeaspect_',
                                                 pathrow_label, '.', ext))
        # Note that the default output of 'terrain' is in radians
        slopeaspect <- terrain(dem_mosaic_crop, opt=c('slope', 'aspect'))
        slopeaspect$aspect <- calc(slopeaspect$aspect, fun=function(vals) {
            vals[vals >= 2*pi] <- 0
            vals
            })
        # Note that slopeaspect is scaled - slope by 10000, and aspect by 1000 so 
        # that the layers can be saved as INT2S
        slopeaspect <- stack(round(raster(slopeaspect, layer=1) * 10000),
                             round(raster(slopeaspect, layer=2) * 1000))
        slopeaspect <- writeRaster(slopeaspect, filename=slopeaspect_filename, 
                                   overwrite=overwrite, datatype='INT2S')
        if (verbose) timer <- stop_timer(timer, label=paste('Calculating slope/aspect for', 
                                                pathrow_label))
        timer <- stop_timer(timer, label=paste0('Processing ', n, ' of ', 
                                                nrow(pathrows), ': ', 
                                                pathrow_label))
    }
    timer <- stop_timer(timer, label=paste('Processing DEMS for', nrow(pathrows), 
                                            'path/rows'))
}
  #+END_SRC
* browse_image.R
 #+BEGIN_SRC R 
plotprep <- function(x, maxpixels=500000, DN_min=0, DN_max=255, x_fun=NULL) {
    if (ncell(x) > maxpixels) {
        x <- sampleRegular(x, size=maxpixels, asRaster=TRUE, useGDAL=TRUE)
    }
    x <- calc(x, fun=function(vals) {
        vals[vals < DN_min] <- DN_min
        vals[vals > DN_max] <- DN_max
        vals <- ((vals - DN_min) / (DN_max - DN_min)) * 255
        if (!is.null(x_fun)) {
            vals <- x_fun(vals)
        }
        return(vals)
    }, datatype='INT1U')
    return(x)
}
#' Simple function to make small preview plot from large raster image
#'
#' @export
#' @param x input \code{RasterBrick} or \code{RasterStack} with at least three 
#' bands
#' @param m an optional mask \code{RasterLayer} to output below the browse 
#' image
#' @param maxpixels maximum number of pixels to use in plotting
#' @param DN_min minimum DN value
#' @param DN_max maximum DN value
#' @param r index in \code{x} of the band to use as the red band
#' @param g index in \code{x} of the band to use as the green band
#' @param b index in \code{x} of the band to use as the blue band
#' @param x_fun an optional function to apply to \code{x} after x is resampled 
#' according to \code{maxpixels}
#' @param m_fun an optional function to apply to \code{m} after m is resampled 
#' according to \code{maxpixels}
#' @return nothing - used for side-effect of saving browse image
browse_image <- function(x, m=NULL, maxpixels=500000, DN_min=0, DN_max=255, 
                         r=3, g=2, b=1, x_fun=NULL, m_fun=NULL) {
    if (!is.null(m)) stopifnot(nlayers(m) == 1)
    x <- plotprep(x, maxpixels=500000, DN_min=DN_min, DN_max=DN_max, 
                  x_fun=x_fun)
    if (!is.null(m) && !is.null(m_fun)) {
        m <- calc(m, fun=m_fun, datatype=dataType(m))
    }
    if (!is.null(m)) {
        m <- sampleRegular(m, size=maxpixels, asRaster=TRUE, useGDAL=TRUE)
        if (nrow(x) > ncol(x)) par(mfrow=c(1, 2))
        else par(mfrow=c(2, 1))
        plotRGB(x, r=r, g=g, b=b, maxpixels=maxpixels)
        plot(m, maxpixels=maxpixels, axes=FALSE, legend=FALSE, box=FALSE)
    } else {
        plotRGB(x, r=r, g=g, b=b, maxpixels=maxpixels)
    }
}
  #+END_SRC
* chg_dir.R
 #+BEGIN_SRC R 
#' Change Direction Image for CVAPS
#'
#' This code calculate the change direction image for the Change Vector 
#' Analysis in Posterior Probability Space (CVAPS) method of Chen et al. 2011.  
#' Use the change direction image in conjunction with the change magnitude 
#' image from \code{chg_dir}, and \code{DFPS} to use the Double Window Flexible 
#' Pace Search method (Chen et al. 2003) to determine the threshold to use to 
#' map areas of change and no-change.
#'
#' @export
#' @importFrom spatial.tools rasterEngine
#' @importFrom tools file_path_sans_ext
#' @param t1p time 0 posterior probability \code{Raster*}
#' @param t2p time 1 posterior probability \code{Raster*}
#' @param filename (optional) filename for output change direction
#' \code{RasterLayer}
#' @param overwrite whether to overwrite existing files (otherwise an error 
#' will be raised)
#' @param verbose whether to print detailed status messages
#' @param ... additional parameters to pass to rasterEngine
#' @return \code{Raster*} object with change direction image
#' @references Chen, J., P. Gong, C. He, R. Pu, and P. Shi. 2003.
#' Land-use/land-cover change detection using improved change-vector analysis.
#' Photogrammetric Engineering and Remote Sensing 69:369-380.
#' 
#' Chen, J., X. Chen, X. Cui, and J. Chen. 2011. Change vector analysis in 
#' posterior probability space: a new method for land cover change detection.  
#' IEEE Geoscience and Remote Sensing Letters 8:317-321.
#' @examples
#' \dontrun{
#' t0_train_data <- get_pixels(L5TSR_1986, L5TSR_1986_2001_training, class_1986,training=.6)
#' t0_model <- train_classifier(t0_train_data)
#' t0_preds <- classify(L5TSR_1986, t0_model)
#' t1_train_data <- get_pixels(L5TSR_2001, L5TSR_1986_2001_training, class_2001, training=.6)
#' t1_model <- train_classifier(t1_train_data)
#' t1_preds <- classify(L5TSR_2001, t1_model)
#' t0_t1_chgdir <- chg_dir(t0_preds$probs, t1_preds$probs)
#' }
chg_dir <- function(t1p, t2p, filename, overwrite=FALSE, verbose=FALSE, ...) {
    if (proj4string(t1p) != proj4string(t2p)) {
        stop('t0 and t1 coordinate systems do not match')
    }
    if (extent(t1p) != extent(t2p)) {
        stop('t0 and t1 extents do not match')
    }
    if (nlayers(t1p) != nlayers(t2p)) {
        stop('t0 and t1 probability maps have differing number of classes')
    }
    if (!missing(filename) && file_test('-f', filename) && !overwrite) {
        stop('output file already exists and overwrite=FALSE')
    }
    n_classes <- nlayers(t1p)
    if (n_classes == 1) {
        stop('cannot calculate change probabilities for only one class')
    }
    # calc_chg_dir_st <- function(t1p, t2p, n_classes, ...) {
    #     # Calculate change direction (eqns 5 and 6 in Chen 2011)
    #     dP <- array(t2p - t1p, dim=c(dim(t1p)[1], dim(t1p)[2], n_classes))
    #     unit_vecs <- array(diag(n_classes), dim=c(n_classes, n_classes))
    #     Eab <- apply(dP, c(1, 2), function(pixel) pixel %*% unit_vecs)
    #     chgdir <- apply(Eab, c(2, 3),
    #                     function(pixel) which(pixel == max(pixel)))
    #     chgdir <- array(chgdir, dim=c(dim(t1p)[1], dim(t1p)[2], 1))
    #     return(chgdir)
    # }
    # out <- rasterEngine(t1p=t1p, t2p=t2p, fun=calc_chg_dir_st, 
    #                     args=list(n_classes=n_classes), 
    #                     outbands=1, datatype='INT2S', ...)
    #
    # # spatial.tools can only output the raster package grid format - so output 
    # # to a tempfile in that format then copy over to the requested final output 
    # # format if a filename was supplied
    # if (!missing(filename)) {
    #     out <- writeRaster(out, filename=filename, dataType='INT2S', 
    #                        overwrite=overwrite)
    # }
    
    if (missing(filename)) {
        filename <- rasterTmpFile()
        overwrite <- TRUE
    }
   
    bs <- blockSize(t1p)
    out <- raster(t1p)
    out <- writeStart(out, filename=filename, overwrite=overwrite)
    for (block_num in 1:bs$n) {
        if (verbose > 0) {
            message(Processing block , block_num,  of , bs$n, ...)
        }
        dims <- c(bs$nrows[block_num], ncol(t1p), nlayers(t1p))
        t1p_bl <- array(getValuesBlock(t1p, row=bs$row[block_num], 
                                 nrows=bs$nrows[block_num]),
                        dim=c(dims[1] * dims[2], dims[3]))
        t2p_bl <- array(getValuesBlock(t2p, row=bs$row[block_num], 
                                       nrows=bs$nrows[block_num]),
                        dim=c(dims[1] * dims[2], dims[3]))
        chg_dirs <- calc_chg_dir(t1p_bl, t2p_bl)
        out <- writeValues(out, chg_dirs, bs$row[block_num])
    }
    out <- writeStop(out)
    return(out)
}
  #+END_SRC
* chg_mag.R
 #+BEGIN_SRC R 
#' Change Magnitude Image for CVAPS
#'
#' This code calculate the change magnitude image for the Change Vector 
#' Analysis in Posterior Probability Space (CVAPS) method of Chen et al. 2011.  
#' Use the change magnitude image and use it in conjunction with the change 
#' direction image from \code{chg_dir} to map areas of change and no-change.  
#' The threshold can be determined using \code{\link{DFPS}} (to use the Double 
#' Window Flexible Pace Search method, from Chen et al. 2003) or 
#' \code{\link{threshold}} (which uses an unsupervised method).
#'
#' This function will run in parallel if a parallel backend is registered with 
#' \code{\link{foreach}}.
#'
#' @export
#' @importFrom spatial.tools rasterEngine
#' @param t1p time 0 posterior probability \code{Raster*}
#' @param t2p time 1 posterior probability \code{Raster*}
#' @param filename (optional) filename for output change magnitude
#' \code{RasterLayer}
#' @param overwrite whether to overwrite existing files (otherwise an error 
#' will be raised)
#' @param ... additional parameters to pass to rasterEngine
#' @return \code{Raster*} object with change magnitude image
#' @references Chen, J., P. Gong, C. He, R. Pu, and P. Shi. 2003.
#' Land-use/land-cover change detection using improved change-vector analysis.
#' Photogrammetric Engineering and Remote Sensing 69:369-380.
#' 
#' Chen, J., X. Chen, X. Cui, and J. Chen. 2011. Change vector analysis in 
#' posterior probability space: a new method for land cover change detection.  
#' IEEE Geoscience and Remote Sensing Letters 8:317-321.
#' @examples
#' \dontrun{
#' t0_train_data <- get_pixels(L5TSR_1986, L5TSR_1986_2001_training, class_1986,training=.6)
#' t0_model <- train_classifier(t0_train_data)
#' t0_preds <- classify(L5TSR_1986, t0_model)
#' t1_train_data <- get_pixels(L5TSR_2001, L5TSR_1986_2001_training, class_2001, training=.6)
#' t1_model <- train_classifier(t1_train_data)
#' t1_preds <- classify(L5TSR_2001, t1_model)
#' t0_t1_chgmag <- chg_mag(t0_preds$probs, t1_preds$probs)
#' }
chg_mag <- function(t1p, t2p, filename, overwrite=FALSE, ...) {
    if (proj4string(t1p) != proj4string(t2p)) {
        stop('t0 and t1 coordinate systems do not match')
    }
    if (extent(t1p) != extent(t2p)) {
        stop('t0 and t1 extents do not match')
    }
    if (nlayers(t1p) != nlayers(t2p)) {
        stop('t0 and t1 probability maps have differing number of classes')
    }
    if (!missing(filename) && file_test('-f', filename) && !overwrite) {
        stop('output file already exists and overwrite=FALSE')
    }
    n_classes <- nlayers(t1p)
    calc_chg_mag <- function(t1p, t2p, n_classes, ...) {
        if (is.null(dim(t1p))) {
            # Handle RasterLayer images
            chgmag <- abs(t2p - t1p)
        } else {
            # Handle RasterStack or RasterBrick images
            chgmag <- apply(t2p - t1p, c(1, 2), function(pixel) sqrt(sum(pixel^2)))
        }
        chgmag <- array(chgmag, dim=c(dim(t1p)[1], dim(t1p)[2], 1))
        return(chgmag)
    }
    out <- rasterEngine(t1p=t1p, t2p=t2p, fun=calc_chg_mag, 
                        args=list(n_classes=n_classes), 
                        outbands=1, outfiles=1, ...)
    
    # spatial.tools can only output the raster package grid format - so output 
    # to a tempfile in that format then copy over to the requested final output 
    # format if a filename was supplied
    if (!missing(filename)) {
        out <- writeRaster(out, filename=filename, overwrite=overwrite)
    }
    return(out)
}
  #+END_SRC
* chg_traj.R
 #+BEGIN_SRC R 
#' Calculate change-trajectory lookup table
#'
#' This function will format a lookup table (lut) to allow coding change 
#' trajectories. Useful for use in conjunction with \code{\link{chg_traj}}.
#'
#' @export
#' @param class_codes a list of integer codes used to code land use/cover 
#' classes
#' @param class_names an (optional) list of class names as character vectors
#' @examples
#' lut <- traj_lut(c(1, 2), c(NonForest, Forest))
traj_lut <- function(class_codes, class_names=NULL) {
    lut <- expand.grid(t0_code=class_codes, t1_code=class_codes)
    if (!is.null(class_names)) {
        if (length(class_names) != length(class_codes)) {
            stop('class_names must be NULL or a vector of length equal to number of classes in initial image')
        }
        lut$t0_name <- class_names[match(lut$t0_code, class_codes)]
        lut$t1_name <- class_names[match(lut$t1_code, class_codes)]
    }
    # Code trajectories by summing t0 and t1 after multiplying t1 by the number 
    # of classes.
    lut$Code <- lut$t0_code + lut$t1_code * length(class_codes)
    # Exclude classes that are persistence - CVAPS doesn't directly code the 
    # class for classes that persist
    lut <- lut[!(lut$t0_code == lut$t1_code), ]
    return(lut)
}
#' Calculate change-trajectory image
#'
#' This function will calculate trajectories of land cover change using the 
#' Change Vector Analysis in Posterior Probability Space (CVAPS) approach of 
#' comparing posterior probabilities of class membership with an automatically 
#' determined threshold. Areas of no change are coded as -1. A lookup table for 
#' the codes output by \code{chg_traj} can be calculated with \code{traj_lut}.
#'
#' This function will run in parallel if a parallel backend is registered with 
#' \code{\link{foreach}}.
#'
#' @export
#' @importFrom spatial.tools rasterEngine
#' @param chg_mag change magnitude \code{RasterLayer} from \code{CVAPS}
#' @param chg_dir change direction \code{RasterLayer} from \code{CVAPS}
#' @param chg_threshold the threshold to use as a minimum when determining change 
#' areas (can use \code{DFPS} to determine this value).
#' @param filename filename to save the output \code{RasterLayer} to disk 
#' (optional)
#' @param overwrite whether to overwrite existing files (otherwise an error 
#' will be raised)
#' @param ... additional parameters to pass to rasterEngine
#' @return a {RasterLayer} of change trajectories, with change trajectories 
#' coded as in the \code{lut} output by \code{traj_lut}
#' @references Chen, J., P. Gong, C.  He, R.  Pu, and P.  Shi.  2003.
#' Land-use/land-cover change detection using improved change-vector analysis.
#' Photogrammetric Engineering and Remote Sensing 69:369-380.
#' 
#' Chen, J., X. Chen, X. Cui, and J. Chen. 2011. Change vector analysis in
#' posterior probability space: a new method for land cover change detection.
#' IEEE Geoscience and Remote Sensing Letters 8:317-321.
#' @examples
#' \dontrun{
#' t0_train_data <- get_pixels(L5TSR_1986, L5TSR_1986_2001_training, class_1986,training=.6)
#' t0_model <- train_classifier(t0_train_data)
#' t0_preds <- classify(L5TSR_1986, t0_model)
#' t1_train_data <- get_pixels(L5TSR_2001, L5TSR_1986_2001_training, class_2001, training=.6)
#' t1_model <- train_classifier(t1_train_data)
#' t1_preds <- classify(L5TSR_2001, t1_model)
#' t0_t1_chgmag <- chg_mag(t0_preds$probs, t1_preds$probs)
#' t0_t1_chgdir <- chg_dir(t0_preds$probs, t1_preds$probs)
#' 
#' lut <- traj_lut(t0_preds$codes$code, t0_preds$codes$class)
#' t0_t1_chgtraj <- chg_traj(lut, t0_t1_chgmag, t0_t1_chgdir, .5)
#' 
#' # Change areas are coded following the above lookup-table (lut):
#' plot(t0_t1_chgtraj)
#' 
#' # No change areas are -1:
#' plot(t0_t1_chgtraj == -1)
#' }
chg_traj <- function(chg_mag, chg_dir, chg_threshold, filename, 
                     overwrite=FALSE, ...) {
    if (nlayers(chg_mag) > 1) stop('chg_mag has more than 1 layer')
    if (nlayers(chg_dir) > 1) stop('chg_dir has more than 1 layer')
    compareRaster(chg_mag, chg_dir)
    if (!missing(filename) && file_test('-f', filename) && !overwrite) {
        stop('output file already exists and overwrite=FALSE')
    }
    calc_chg_traj <- function(chg_mag, chg_dir, chg_threshold, ...) {
        # Trajectories in chg_dir were coded by summing t0 and t1 classes after 
        # multiplying t1 class by the number of classes
        chg_dir[chg_mag < chg_threshold] <- -1
        chg_dir[is.na(chg_dir)] <- -2
        chg_dir <- array(chg_dir, dim=c(dim(chg_mag)[1], dim(chg_mag)[2], 1))
        return(chg_dir)
    }
    out <- rasterEngine(chg_mag=chg_mag, chg_dir=chg_dir, fun=calc_chg_traj,
                        args=list(chg_threshold=chg_threshold),
                        datatype='INT2S', ...)
    # spatial.tools doesn't properly handle NA values for integer layers, so 
    # they were coded as -2 above - now recode them as NA
    out[out == -2] <- NA
    # spatial.tools can only output the raster package grid format - so output 
    # to a tempfile in that format then copy over to the requested final output 
    # format if a filename was supplied
    if (!missing(filename)) {
        out <- writeRaster(out, filename=filename, overwrite=overwrite, 
                           datatype='INT2S')
    }
    return(out)
}
  #+END_SRC
* chg_traj_stats.R
 #+BEGIN_SRC R 
#' Calculate change-trajectory statistics
#'
#' @export
#' @param traj a list (as output by \code{chg_traj} with two elements: traj_lut 
#' (a lookup table of change trajectory codes) and chg_traj (a 
#' \code{RasterLayer} of change trajectory codes.
#' @return a \code{data.frame} object with change trajectory statistics
#' @examples
#' #TODO: Add examples
chg_traj_stats <- function(traj) {
    chg_table <- table(getValues(traj$chg_traj))
    summ_table <- data.frame(Traj_Code=traj$traj_lut$Code,
                             Trajectory=paste(traj$traj_lut$t0_name, traj$traj_lut$t1_name, 
                                              sep='-'))
    summ_table <- cbind(summ_table, n_pixels=chg_table[match(row.names(chg_table), summ_table$Traj_Code)])
    row.names(summ_table) <- NULL
    summ_table$Frac_Chg <- summ_table$n_pixels / sum(summ_table$n_pixels)
    summ_table$Frac_Tot <- summ_table$n_pixels / length(traj$chg_traj)
    return(summ_table)
}
  #+END_SRC
* classify.R
 #+BEGIN_SRC R 
#' Classify an image using a trained classifier
#'
#' This function will produce two outputs - a prediction image and a 
#' probability image. The prediction image contains the predicted classes, the 
#' and the probability image contains the per-pixel predicted probabilities of 
#' occurrence of each class.
#'
#' This function will run in parallel if a parallel backend is registered with 
#' \code{\link{foreach}} - TEMPORARILY DISABLED.
#'
#' @export
#' @import caret
#' @importFrom spatial.tools rasterEngine
#' @param x a \code{Raster*} image with the predictor layer(s) for the 
#' classification
#' @param model a trained classifier as output by 
#' \code{\link{train_classifier}}
#' @param classes_file filename for predicted classes (or missing)
#' @param prob_file filename for predicted probabilities (or missing)
#' @param factors a list of character vector giving the names of predictors 
#' (layer names from the images used to build \code{train_data}) that should be 
#' treated as factors, and specifying the levels of each factor. For example, 
#' \code{factors=list(year=c(1990, 1995, 2000, 2005, 2010))}.
#' @param overwrite whether to overwrite \code{out_name} if it already exists
#' @return a list with 2 elements: the predicted classes as a 
#' \code{RasterLayer} and the class probabilities as a \code{RasterBrick}
#' @examples
#' \dontrun{
#' train_data <- get_pixels(L5TSR_1986, L5TSR_1986_2001_training, class_1986, 
#'                          training=.6)
#' model <- train_classifier(train_data)
#' preds <- classify(L5TSR_1986, model)
#' plot(preds$classes)
#' plot(preds$probs)
#' }
classify <- function(x, model, classes_file, prob_file, factors=list(), 
                     overwrite=FALSE) {
    # TODO: Check with Jonathan why below fix is needed
    if (!(RasterBrick %in% class(x))) x <- brick(x)
    if (!missing(prob_file) && file_test('-f', prob_file) && !overwrite) {
        stop(paste('output file', prob_file, 'already exists and overwrite=FALSE'))
    }
    if (!missing(classes_file) && file_test('-f', classes_file) && !overwrite) {
        stop(paste('output file', classes_file, 'already exists and overwrite=FALSE'))
    }
    make_preds <- function(inrast, model, factors, ...) {
        # First, preserve the names:
        band_names <- dimnames(inrast)[3][[1]]
        # Flatten the array to a matrix (we lose the names here)
        inrast_mat <- inrast
        dim(inrast_mat) <- c(dim(inrast)[1]*dim(inrast)[2], dim(inrast)[3])
        inrast_df <- as.data.frame(inrast_mat)
        names(inrast_df) <- band_names
        # Make sure any factor variables are converted to factors and that the 
        # proper levels are assigned
        if (length(factors) > 0) {
            for (n in 1:length(factors)) {
                factor_var <- names(factors)[n]
                factor_col <- which(names(inrast_df) == factor_var)
                inrast_df[, factor_col] <- factor(inrast_df[, factor_col], 
                                                  levels=factors[[n]])
            }
        }
        good_obs <- complete.cases(inrast_df)
        preds <- matrix(NA, nrow=nrow(inrast_df), ncol=nlevels(model))
        if (sum(good_obs) > 0) {
            good_preds <- predict(model, inrast_df[good_obs, ], type=prob)
            preds[which(good_obs), ] <- as.matrix(good_preds)
        }
        preds_array <- array(preds, dim=c(dim(inrast)[1], dim(inrast)[2], 
                                          nlevels(model)))
        return(preds_array)
    }
    probs <- rasterEngine(inrast=x, fun=make_preds,
                          args=list(model=model, factors=factors),
                          filename=rasterTmpFile(), overwrite=overwrite, 
                          datatype=FLT4S, .packages=c(randomForest),
                          setMinMax=TRUE)
    # spatial.tools can only output the raster package grid format - so output 
    # to a tempfile in that format then copy over to the requested final output 
    # format if a filename was supplied
    if (!missing(prob_file)) {
        probs <- writeRaster(probs, filename=prob_file, overwrite=overwrite, 
                             datatype='FLT4S')
    }
    names(probs) <- levels(model)
    # Calculate the highest probability class from the class probabilities
    if (missing(classes_file)) classes_file <- rasterTmpFile()
    classes <- calc(probs, fun=function(vals) {
            # Subtract 1 below as software like ENVI starts class codes at zero
            out <- as.numeric(which(vals == max(vals))) - 1
            #TODO: Need to handle case of ties (length(out) > 1)
            if (length(out) != 1) out <- NA
            return(out)
        }, datatype='INT2S', filename=classes_file, overwrite=overwrite)
    names(classes) <- 'prediction'
    codes <- data.frame(code=seq(0, (nlevels(model) - 1)), class=levels(model))
    return(list(classes=classes, probs=probs, codes=codes))
}
  #+END_SRC
* class_statistics.R
 #+BEGIN_SRC R 
#' Exports statistics on pixels within each of a set of land cover classes
#'
#' @export
#' @importFrom dplyr group_by summarize
#' @importFrom reshape2 melt
#' @param x A \code{RasterLayer} from which class statistics will be 
#' calculated.
#' @param y A \code{SpatialPolygonsDataFrame} with cover class 
#' polygons
#' @param class_col the name of the column containing the response variable 
#' (for example the land cover type of each pixel)
#' @return A data.frame of class statistics.
#' @examples
#' class_statistics(L5TSR_1986, L5TSR_1986_2001_training, class_1986)
class_statistics <- function(x, y, class_col) {
    if (projection(x) != projection(y)) {
        stop('Coordinate systems do not match')
    }
    if (class(y) == SpatialPolygonsDataFrame) {
        pixels <- get_pixels(x, y, class_col)
    } else if (class(y) %in% c(RasterLayer, RasterBrick, 
                                         RasterStack)) {
        stop('class_statistics cannot yet handle Raster* objects')
    }
    pixels <- melt(data.frame(pixels@x, y=pixels@y), idvar='y')
    # Set y and variable to NULL to pass R CMD CHECK without notes
    value=variable=NULL
    class_stats <- summarize(group_by(pixels, y, variable), mean=mean(value), 
                             sd=sd(value), min=min(value), max=max(value), 
                             n_pixels=length(value))
    class_stats <- class_stats[order(class_stats$variable, class_stats$y), ]
    return(class_stats)
}
  #+END_SRC
* cloud_remove.R
 #+BEGIN_SRC R 
# Function to test if ENVI will load in IDL
check_ENVI_IDL <- function(idl) {
    idl_out <- system(paste(shQuote(idl), '-e e=ENVI(/HEADLESS)'), 
                      intern=TRUE)
    if (sum(grepl(Restored file: ENVI, idl_out)) > 0) {
        return(TRUE)
    } else {
        return(FALSE)
    }
}
# Function to ensure only character variables handed to IDL are quoted
format_IDL_param <- function(varname, varvalue) {
    if (is.character(varvalue)) {
        param <- paste0(varname, '=', varvalue, '\n')
    } else if (is.list(varvalue)) {
        param <- paste0(varname, '=[')
        if (length(varvalue) > 0) {
            for (n in 1:length(varvalue)) {
                if (is.numeric(varvalue[n])) {
                    param <- paste0(param, varvalue[n])
                } else {
                    param <- paste0(param, '', varvalue[n], '')
                }
                if (n != length(varvalue)) {
                    param <- paste0(param, ', ')
                }
            }
        }
        param <- paste0(param, ']\n')
    } else {
        param <- paste0(varname, '=', varvalue, '\n')
    }
    return(param)
}
#' @importFrom tools file_path_sans_ext
cloud_remove_IDL <- function(cloudy, clear, cloud_mask, out_name,
                             algorithm, num_class, min_pixel, max_pixel, 
                             cloud_nbh, DN_min, DN_max, 
                             verbose, idl, byblock, overwrite,
                             patch_long=1000) {
    if (verbose > 0) {
        warning(verbose not supported with CLOUD_REMOVE and CLOUD_REMOVE_FAST algorithms)
    }
    if (algorithm == 'CLOUD_REMOVE_FAST') {
        script_path <- system.file(idl, CLOUD_REMOVE_FAST.pro, 
                                   package=teamlucc)
        function_name <- 'CLOUD_REMOVE_FAST'
    } else if (algorithm == 'CLOUD_REMOVE') {
        script_path <- system.file(idl, CLOUD_REMOVE.pro, 
                                   package=teamlucc)
        function_name <- 'CLOUD_REMOVE'
    } else {
        stop(paste0('unrecognized cloud fill algorithm ', algorithm, ''))
    }
    
    if (!(file_test('-x', idl) || file_test('-f', idl))) {
        stop('IDL not found - check idl parameter')
    }
    if (!check_ENVI_IDL(idl)) {
        stop(Unable to load ENVI in IDL - do you have ENVI and IDL licenses, and ENVI >= 5.0?)
    }
    if (!byblock) {
        patch_long <- max(dim(cloudy)) + 1
    }
    # Save proj4string and extent to ensure the same proj4string and extent is 
    # returned even if they are changed by IDL
    orig_proj <- proj4string(cloudy)
    orig_ext <- extent(cloudy)
    # Write in-memory rasters to files for hand off to IDL. The capture.output 
    # line is used to avoid printing the rasterOptions to screen as they are 
    # temporarily reset.
    dummy <- capture.output(def_format <- rasterOptions()$format)
    rasterOptions(format='ENVI')
    cloudy <- writeRaster(cloudy, rasterTmpFile(), 
                          datatype=dataType(cloudy)[1])
    clear <- writeRaster(clear, rasterTmpFile(), datatype=dataType(clear)[1])
    cloud_mask <- writeRaster(cloud_mask, rasterTmpFile(), 
                              datatype=dataType(cloud_mask)[1])
    cloudy_file <- filename(cloudy)
    clear_file <- filename(clear)
    cloud_mask_file <- filename(cloud_mask)
    dummy <- capture.output(rasterOptions(format=def_format))
    param_names <- c(cloudy_file, clear_file, mask_file, out_name, 
                     num_class, min_pixel, extent1, DN_min, DN_max, 
                     patch_long)
    param_vals <- list(cloudy_file, clear_file, cloud_mask_file, out_name, 
                       num_class, min_pixel, cloud_nbh, DN_min, DN_max, 
                       patch_long)
    idl_params <- mapply(format_IDL_param, param_names, param_vals)
    idl_params <- paste(idl_params, collapse='')
    script_dir <- dirname(script_path)
    idl_script <- tempfile(fileext='.pro')
    idl_cmd <- paste0('CD, ', script_dir, '\n', idl_params, function_name, ',', 
                      paste(param_names, collapse=','), '\nexit')
    f <- file(idl_script, 'wt')
    writeLines(idl_cmd, f)
    close(f)
    idl_out <- system(paste(shQuote(idl), shQuote(idl_script)), intern=TRUE)
    log_file <- paste0(file_path_sans_ext(out_name), '_idllog.txt')
    idl_out <- gsub('\r', '', idl_out)
    f <- file(log_file, 'wt')
    writeLines(idl_out, f) 
    close(f)
    filled <- brick(out_name)
    filled[filled < DN_min] <- NA
    # Ensure original proj4string and extent are saved and returned
    proj4string(filled) <- orig_proj
    extent(filled) <- orig_ext
    filled <- writeRaster(filled, filename=out_name, overwrite=TRUE, 
                          datatype=dataType(filled)[1])
    return(filled)
}
# Wrapper around C++ cloud fill function, to enable calling the function with 
# rasterEngine
#' @import Rcpp
cloud_fill_rasterengine <- function(cloudy, clear, cloud_mask, algorithm, 
                                    num_class, min_pixel, max_pixel, cloud_nbh, 
                                    DN_min, DN_max, verbose, ...) {
    dims=dim(cloudy)
    # RcppArmadillo crashes when you pass it a cube, so resize and pass 
    # mats
    cloudy <- array(cloudy, dim=c(dims[1] * dims[2], dims[3]))
    clear <- array(clear, dim=c(dims[1] * dims[2], dims[3]))
    cloud_mask <- array(cloud_mask, dim=c(dims[1] * dims[2]))
    filled <- call_cpp_cloud_fill(cloudy, clear, cloud_mask, algorithm, dims, 
                                  num_class,  min_pixel, max_pixel, cloud_nbh, 
                                  DN_min, DN_max, verbose)
    # RcppArmadillo crashes when you return a cube, so resize the returned 
    # mat
    filled <- array(filled, dim=c(dims[1], dims[2], dims[3]))
    return(filled)
}
# This function decides which RcppArmadillo exported function to call: 
# cloud_fill, or cloud_fill_simple
call_cpp_cloud_fill <- function(cloudy, clear, cloud_mask, algorithm, dims, 
                                num_class, min_pixel, max_pixel, cloud_nbh, 
                                DN_min, DN_max, verbose, ...) {
    if (algorithm == teamlucc) {
        filled <- cloud_fill(cloudy, clear, cloud_mask, dims, num_class, 
                             min_pixel, max_pixel, cloud_nbh, DN_min, DN_max, 
                             verbose)
    } else if (algorithm == simple) {
        filled <- cloud_fill_simple(cloudy, clear, cloud_mask, dims, num_class, 
                                    cloud_nbh, DN_min, DN_max, verbose)
    } else {
        stop(paste0('unrecognized cloud fill algorithm ', algorithm, ''))
    }
    return(filled)
}
#' @importFrom spatial.tools rasterEngine
cloud_remove_R <- function(cloudy, clear, cloud_mask, out_name, algorithm, 
                           num_class, min_pixel, max_pixel, cloud_nbh, DN_min, 
                           DN_max, verbose, byblock, overwrite) {
    # Note that call_cpp_cloud_fill uses the algorithm to decide whether to 
    # call cloud_fill or cloud_fill_simple (and call_cpp_cloud_fill is called 
    # by cloud_fill_rasterengine)
    if (byblock) {
        bs <- blockSize(cloudy)
        out <- brick(cloudy, values=FALSE)
        out <- writeStart(out, out_name, overwrite=overwrite)
        for (block_num in 1:bs$n) {
            if (verbose > 0) {
                message(Processing block , block_num,  of , bs$n, ...)
            }
            dims <- c(bs$nrows[block_num], ncol(cloudy), nlayers(cloudy))
            cloudy_bl <- array(getValuesBlock(cloudy, row=bs$row[block_num],
                                              nrows=bs$nrows[block_num]),
                               dim=c(dims[1] * dims[2], dims[3]))
            clear_bl <- array(getValuesBlock(clear, row=bs$row[block_num],
                                            nrows=bs$nrows[block_num]),
                            dim=c(dims[1] * dims[2], dims[3]))
            cloud_mask_bl <- array(getValuesBlock(cloud_mask, 
                                                  row=bs$row[block_num], 
                                                  nrows=bs$nrows[block_num]), 
                                   dim=c(dims[1] * dims[2]))
            filled <- call_cpp_cloud_fill(cloudy_bl, clear_bl, cloud_mask_bl, 
                                          algorithm, dims, num_class, 
                                          min_pixel, max_pixel, cloud_nbh, 
                                          DN_min, DN_max, verbose>1)
            out <- writeValues(out, filled, bs$row[block_num])
        }
        out <- writeStop(out)
        # out <- rasterEngine(cloudy=cloudy, clear=clear, 
        # cloud_mask=cloud_mask,
        #                     fun=cloud_fill_rasterengine,
        #                     args=list(algorithm=algorithm, num_class=num_class, 
        #                               min_pixel=min_pixel, max_pixel=max_pixel, 
        #                               cloud_nbh=cloud_nbh, DN_min=DN_min, 
        #                               DN_max=DN_max, verbose=verbose),
        #                     processing_unit='chunk',
        #                     outbands=nlayers(cloudy), outfiles=1,
        #                     verbose=verbose,
        #                     filename=out_name)
    } else {
        dims <- dim(cloudy)
        out_datatype <- dataType(cloudy)[1]
        out <- brick(cloudy, values=FALSE, filename=out_name)
        # RcppArmadillo crashes when you pass it a cube, so resize and pass 
        # mats
        cloudy <- array(getValues(cloudy), dim=c(dims[1] * dims[2], dims[3]))
        clear <- array(getValues(clear), dim=c(dims[1] * dims[2], dims[3]))
        cloud_mask <- array(getValues(cloud_mask), dim=c(dims[1] * dims[2]))
        filled <- call_cpp_cloud_fill(cloudy, clear, cloud_mask, algorithm, 
                                      dims, num_class, min_pixel, max_pixel, 
                                      cloud_nbh, DN_min, DN_max, verbose>1)
        out <- setValues(out, filled)
        out <- writeRaster(out, out_name, datatype=out_datatype, 
                           overwrite=overwrite)
    }
    return(out)
}
#' Remove clouds From Landsat imagery
#'
#' This code uses one of several different algorithms (depending on the 
#' settings, see Details) to fill heavy clouds in a Landsat image.
#'
#' The \code{algorithm} parameter determines what algorithm is used for the 
#' cloud fill. \code{algorithm} must be one of: CLOUD_REMOVE, 
#' CLOUD_REMOVE_FAST, teamlucc, or simple (the default). If set to 
#' CLOUD_REMOVE the script uses a (slightly modified to be called from R) 
#' version of  Xiaolin Zhu's NSPI IDL code. If set to CLOUD_REMOVE_FAST, the 
#' algorithm uses the fast version of Xiaolin's code. Both of these two 
#' algorithms require an IDL license to run (and therefore \code{idl_path} must 
#' be set).  The teamlucc algorithm uses a version of the NSPI algorithm 
#' (based on the CLOUD_REMOVE code) that is coded in C++ and  can be run from R 
#' without an IDL license. The simple algorithm uses a cloud fill model that 
#' is based on fitting a linear model to the surface reflectance from the clear 
#' image in a window around each cloud, and using this linear model to predict 
#' reflectance in unobserved (cloudy) areas.
#'
#' @export
#' @param cloudy the cloudy image (base image) as a \code{Raster*}
#' @param clear the clear image as a \code{Raster*} to use for filling 
#' \code{img_cloudy}
#' @param cloud_mask the cloud mask as a \code{RasterLayer}, with each cloud 
#' patch assigned a unique integer code. Areas that are clear in both 
#' \code{cloudy_rast} and \code{clear_rast} should be coded 0, while areas that 
#' are clouded in \code{clear_rast} should be coded -1.
#' @param out_name filename for cloud filled image
#' @param algorithm must be one of: CLOUD_REMOVE, CLOUD_REMOVE_FAST, 
#' teamlucc, or simple. Default is simple. See Details.
#' @param num_class set the estimated number of classes in image
#' @param min_pixel the sample size of similar pixels (ignored when 
#' \code{algorithm==TRUE})
#' @param max_pixel the maximum sample size to search for similar pixels 
#' (ignored when \code{algorithm==TRUE})
#' @param cloud_nbh the range of cloud neighborhood (in pixels)
#' @param DN_min the minimum valid DN value (default of 0)
#' @param DN_max the maximum valid DN value (default of 10000 assumes 2 byte 
#' integer imagery)
#' @param idl path to the IDL binary on your machine (on Windows, the path to 
#' idl.exe)
#' @param verbose whether to print detailed status messages. Set to FALSE or 0 
#' for no status messages. Set to 1 for basic status messages. Set to 2 for 
#' detailed status messages.
#' @param byblock whether to process images block by block 
#' (\code{byblock=TRUE}) or all at once (\code{byblock=FALSE}). Use 
#' \code{byblock=FALSE} with caution, as this option will cause the cloud fill 
#' routine to consume a large amount of memory.
#' @param overwrite whether to overwrite \code{out_name} if it already exists
#' @param ... additional arguments passed to the chosen cloud fill routine
#' @return \code{Raster*} with cloud-filled image
#' @references Zhu, X., Gao, F., Liu, D., Chen, J., 2012. A modified
#' neighborhood similar pixel interpolator approach for removing thick clouds 
#' in Landsat images. Geoscience and Remote Sensing Letters, IEEE 9, 521--525.
#' @examples
#' \dontrun{
#' cloudy <- raster(system.file('tests', 'testthat_idl', 'cloud_remove', 
#' 'L20080724_cloudy', package='teamlucc'))
#' clear <- raster(system.file('tests', 'testthat_idl', 'cloud_remove', 
#' 'L20080606', package='teamlucc'))
#' cloud_mask <- raster(system.file('tests', 'testthat_idl', 'cloud_remove', 
#' 'cloud_mask', package='teamlucc'))
#' filled <- cloud_remove(cloudy, clear, cloud_mask, fast=TRUE)
#' }
cloud_remove <- function(cloudy, clear, cloud_mask, out_name=NULL, 
                         algorithm='simple',
                         num_class=4, min_pixel=20, max_pixel=1000, 
                         cloud_nbh=10, DN_min=0, DN_max=10000, 
                         idl=C:/Program Files/Exelis/IDL83/bin/bin.x86_64/idl.exe,
                         verbose=FALSE, byblock=TRUE, overwrite=FALSE, ...) {
    if (!(algorithm %in% c('CLOUD_REMOVE', 'CLOUD_REMOVE_FAST', 'teamlucc', 
                           'simple'))) {
        stop('algorithm must be one of CLOUD_REMOVE, CLOUD_REMOVE_FAST, teamlucc, or simple')
    }
    if (verbose > 0) {
        message('Using ', algorithm, ' algorithm.')
    }
    
    if (!(class(cloudy) %in% c(RasterLayer, RasterStack, RasterBrick))) {
        stop('cloudy must be a Raster* object')
    }
    if (!(class(clear) %in% c(RasterLayer, RasterStack, RasterBrick))) {
        stop('clear must be a Raster* object')
    }
    if (!(class(cloud_mask) %in% c(RasterLayer))) {
        stop('cloud_mask must be a RasterLayer object')
    }
    compareRaster(cloudy, clear)
    if (nlayers(cloudy) != nlayers(clear)) {
        stop('number of layers in cloudy must match number of layers in clear')
    }
    if (nlayers(cloud_mask) != 1) {
        stop('cloud_mask should have only one layer')
    }
    if (is.null(out_name)) {
        out_name <- rasterTmpFile()
    } else {
        out_name <- normalizePath(out_name, mustWork=FALSE)
        if (!file_test('-d', dirname(out_name))) {
            stop('output folder does not exist')
        }
        if (file_test('-f', out_name) & !overwrite) {
            stop('output file already exists - use a different out_name')
        }
    }
    
    if (algorithm %in% c('CLOUD_REMOVE', 'CLOUD_REMOVE_FAST')) {
        filled <- cloud_remove_IDL(cloudy, clear, cloud_mask, out_name,
                                   algorithm, num_class, min_pixel, max_pixel, 
                                   cloud_nbh, DN_min, DN_max, verbose, idl, 
                                   byblock, overwrite, ...)
    } else if (algorithm %in% c('teamlucc', 'simple')) {
        filled <- cloud_remove_R(cloudy, clear, cloud_mask, out_name, 
                                 algorithm, num_class, min_pixel, max_pixel, 
                                 cloud_nbh, DN_min, DN_max, verbose, byblock, 
                                 overwrite, ...)
    } else {
        stop(paste0('unrecognized cloud fill algorithm ', algorithm, ''))
    }
    names(filled) <- names(cloudy)
    return(filled)
}
  #+END_SRC
* color_image.R
 #+BEGIN_SRC R 
#' Assign class labels to classification file
#'
#' @export
#' @importFrom rgdal writeGDAL
#' @importFrom sp SpatialPixelsDataFrame
#' @param x classified image as \code{RasterLayer}
#' @param cls two column matrix, where the first column is the class codes 
#' (integers) and the second column is the class names
#' @param outfile the filename to use for the output
#' @examples
#' #TODO: Add examples
color_image <- function(x, cls, outfile) {
    # Reclassify image so it is coded from 0 - length(cls[1]). ENVI and 
    # other classification file formats rely on the codes being sequential, 
    # starting at zero.
    x <- reclassify(x, cbind(cls[, 1], seq(0, length(cls[, 1]) - 1)))
    x.sp <- as(x, SpatialPixelsDataFrame)
    cls_colors <- t(col2rgb(cls[, 1]))
    # Select appropriate data type and missing value tag depending on data 
    # attributes.
    if (max(x.sp$layer) > 254) {
        gdaltype <- 'Int16'
        gdalmvFlag <- -32768
    } else {
        gdaltype <- 'Byte'
        gdalmvFlag <- 255
    }
    writeGDAL(x.sp, outfile, drivername=ENVI, type=gdaltype,
              colorTables=list(cls_colors), catNames=list(as.character(cls[, 2])),
              mvFlag=gdalmvFlag)
}
  #+END_SRC
* compcont.R
 #+BEGIN_SRC R 
#' Calculate a contingency table using the composite operator
#'
#' This function calculates a cross tabulation for a map comparison using the 
#' composite operator recommended by Pontius and Cheuk (2006).
#'
#' @export
#' @return matrix with contingency table
#' @references Pontius, R. G., and M. L. Cheuk. 2006.  A generalized 
#' cross-tabulation matrix to compare soft-classified maps at multiple 
#' resolutions. International Journal of Geographical Information Science 
#' 20:1-30.
compcont <- function() {
    stop('compcont is not yet finished')
    #TODO: finish coding this function
}
  #+END_SRC
* DFPS.R
 #+BEGIN_SRC R 
#' Double-Window Flexible Pace Search (DFPS) threshold determination
#'
#' @export
#' @importFrom rgeos gDifference gBuffer
#' @param chg_polys \code{SpatialPolygonsDataFrame} with polygons of change 
#' areas surrounded by windows of no-change
#' @param chg_mag change magnitude \code{RasterLayer} from \code{CVAPS}
#' @param radius radius of no-change surrounding change area polygons
#' @param delta the minimum difference between Lmax and Lmin that will allow 
#' another pass through the search loop
#' @param m number of potential thresholds per search iteration (see Chen et 
#' al., 2003). Recommended to leave at default value.
#' @param maxiter maximum number of iterations of the main search process
#' @references Chen, J., P. Gong, C. He, R. Pu, and P. Shi. 2003.
#' Land-use/land-cover change detection using improved change-vector analysis.
#' Photogrammetric Engineering and Remote Sensing 69:369-380.
#' 
#' Chen, J., X. Chen, X. Cui, and J. Chen. 2011. Change vector analysis in
#' posterior probability space: a new method for land cover change detection.
#' IEEE Geoscience and Remote Sensing Letters 8:317-321.
DFPS <- function(chg_polys, chg_mag, radius=100, delta=.01, m=10, maxiter=20) {
    chg_pixels <- unlist(extract(chg_mag, chg_polys))
    nochg_polys <- gDifference(gBuffer(chg_polys, width=radius), chg_polys)
    nochg_pixels <- unlist(extract(chg_mag, nochg_polys))
    # Calculate total number of pixels (used later)
    A <- length(chg_pixels) + length(nochg_pixels)
    # Set initial values for Lmax and Lmin that ensure the below while loop 
    # will run.
    Lmax <- 1
    Lmin <- Lmax - 2 * delta
    min_threshold <- min(chg_pixels)
    max_threshold <- max(chg_pixels)
    n <- 0
    while ((Lmax - Lmin) > delta && n < maxiter) {
        p <- (max_threshold - min_threshold) / m
        thresholds <- seq(min_threshold, max_threshold, p)
        L <- c()
        for (threshold in thresholds) {
            A1 <- sum(chg_pixels > threshold)
            A2 <- sum(nochg_pixels > threshold)
            # Calculate A according to equation 4 in Chen et al. 2003
            L <- c(L, ((A1 - A2) * 100) / A)
        }
        kmax <- thresholds[match(max(L), L)]
        min_threshold <- kmax - p
        max_threshold <- kmax + p
        Lmin <- min(L)
        Lmax <- max(L)
        n <- n + 1
    }
    return(kmax)
}
  #+END_SRC
* ee_plot.R
 #+BEGIN_SRC R 
#' Plot EarthExplorer scene list
#'
#' This function can produce two different types of plots from a USGS 
#' EarthExplorer Landsat CDR Surface Reflectance scene list.
#'
#' @export
#' @import ggplot2
#' @importFrom dplyr group_by summarize
#' @importFrom lubridate new_interval %within%
#' @param x a \code{data.frame} with a list of Landsat scenes as output by
#' \code{\link{ee_read}}
#' @param start_date starting date as a \code{Date} object
#' @param end_date end date as a \code{Date} object
#' @param min_clear the minimum percent clear to plot (calculated as 1 - 
#' percent cloud cover). Images with less than \code{min_clear} fraction of the 
#' image area free of clouds will be ignored.
#' @param exclude a list of sensors to exclude (for example, set 
#' \code{exclude=c('LE7', 'LT4')} to exclude Landsat 7 ETM+ and Landsat 4 TM 
#' images.
#' @param normalize if \code{TRUE}, plot as a normalized line plot
#' @param title title for plot (or \code{NULL} for no title)
#' @return used for side effect of producing a plot
ee_plot <- function(x, start_date, end_date, min_clear=.7, exclude=list(), 
                    normalize=FALSE, title=NULL) {
    if (!class(start_date) == 'Date') {
        stop('start_date must be a Date object')
    }
    if (!class(end_date) == 'Date') {
        stop('end_date must be a Date object')
    }
    x <- x[!(x$Sensor %in% exclude), ]
    x$Sensor <- factor(x$Sensor)
    x <- x[order(x$WRS.Path, x$WRS.Row), ]
    x <- x[x$Frac_Clear >= min_clear, ]
    if ((!missing(start_date) && missing(end_date)) ||
        (missing(start_date) && !missing(end_date))) {
        stop('both start_date and end_date must be provided')
    } else if (!missing(start_date) && !missing(end_date)) {
        sel_interval <- new_interval(start_date, end_date)
        x <- x[x$Date.Acquired %within% sel_interval, ]
    }
    if (nrow(x) == 0) {
        stop('no data to plot - try different start/end dates')
    }
    if (!normalize) {
        YearMonth=Month=Cum_Month=Path_Row=Sensor=Frac_Clear=NULL # Keep R CMD CHECK happy
        x <- transform(group_by(x, YearMonth),
                       Cum_Month=cumsum(rep(1, length(Month))))
        p <- ggplot(x, aes(xmin=Month,
                           xmax=Month + 1, 
                           ymin=Cum_Month - 1, 
                           ymax=Cum_Month,
                           colour=Sensor,
                           fill=Path_Row,
                           alpha=Frac_Clear)) +
            geom_rect() + facet_grid(Year ~ ., scales='free_y', space='free_y') +
            xlab('Month') +
            scale_x_continuous(breaks=c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12),
                               labels=c('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
                                        'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec')) +
            theme(panel.grid.minor.y=element_blank(), 
                  panel.grid.major.y=element_blank(),
                  panel.grid.major.x=element_blank()) +
            theme(axis.ticks.y=element_blank(),
                  axis.text.y=element_blank()) +
            scale_colour_brewer(type='qual', palette='Set1', drop=FALSE, name='Sensor') +
            scale_fill_brewer(type='qual', palette='Set2', drop=FALSE, name='Path/Row') +
            scale_alpha(name='Fraction Clear')
    } else {
        # Keep R CMD CHECK happy:
        YearMonth=Path_Row=Year=Month=Max_Frac_Clear=Frac_Clear=Sum_Max_Frac_Clear=NULL
        Frac_Clear_Stats <- summarize(group_by(x, YearMonth, Path_Row),
                                      Year=Year[1], Month=Month[1],
                                      Max_Frac_Clear=max(Frac_Clear))
        Frac_Clear_Stats <- summarize(group_by(Frac_Clear_Stats, YearMonth), 
                                      Year=Year[1], Month=Month[1],
                                      Sum_Max_Frac_Clear=sum(Max_Frac_Clear))
        p <- ggplot(Frac_Clear_Stats, aes(xmin=Month + .05,
                                          xmax=Month + 1-.05, 
                                          ymin=0, 
                                          ymax=Sum_Max_Frac_Clear)) +
            geom_rect() + facet_grid(Year ~ .) +
            xlab('Month') + ylab('Total Fraction Clear') +
            scale_x_continuous(breaks=c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12),
                               labels=c('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
                                        'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec')) +
            theme(panel.grid.minor.y=element_blank(), 
                  panel.grid.major.y=element_blank(),
                  panel.grid.major.x=element_blank()) +
            theme(axis.ticks.y=element_blank(),
                  axis.text.y=element_blank()) +
            geom_hline(yintercept=seq(1,length(unique(x$Path_Row))), colour='white', linetype='dashed')
    }
    if (!is.null(title)) {
        p <- p + ggtitle(title)
    }
    return(p)
}
  #+END_SRC
* ee_read.R
 #+BEGIN_SRC R 
#' Read EarthExplorer CSV format scene list
#'
#' This function reads in a CSV file of Landsat CDR Surface Reflectance images 
#' as output from USGS EarthExplorer. param x a \code{data.frame} with a list 
#' of Landsat scenes as output from the save metadata function on 
#'
#' @export
#' @param x path to a CSV file with a list of Landsat scenes as output from the 
#' save metadata function on http://earthexplorer.usgs.gov
#' @return x a \code{data.frame} with a list of Landsat scenes and their 
#' associated metadata
ee_read <- function(x) {
    scenes <- read.csv(x, stringsAsFactors=FALSE, quote=, 
                          na.strings=c('NA', ' '))
    scenes$Sensor <- substr(scenes$Landsat.Scene.Identifier, 1, 3)
    scenes$Sensor <- factor(scenes$Sensor)
    # Dates are formatted as either: 1999/12/31 or 12/31/1999
    yr_first <- grepl('^[0-9]{4}/', scenes$Date.Acquired)
    yr_last <- grepl('/[0-9]{4}$', scenes$Date.Acquired)
    if ((sum(yr_first) + sum(yr_last)) < nrow(scenes)) {
        stop('unrecognized date format in Date.Acquired column')
    }
    acq_date <- as.Date(scenes$Date.Acquired)
    acq_date[yr_first] <- as.Date(scenes$Date.Acquired[yr_first], '%Y/%m/%d')
    acq_date[yr_last] <- as.Date(scenes$Date.Acquired[yr_last], '%m/%d/%Y')
    scenes$Date.Acquired <- acq_date
    scenes$Year <- as.numeric(format(scenes$Date.Acquired, '%Y'))
    scenes$Month <- as.numeric(format(scenes$Date.Acquired, '%m')) - .5
    scenes$MonthFactor <- factor(format(scenes$Date.Acquired, '%m'))
    scenes$Path_Row <- factor(paste(scenes$WRS.Path, scenes$WRS.Row, sep='/'))
    scenes$YearMonth <- paste(scenes$Year, scenes$MonthFactor, sep='/')
    scenes$Frac_Clear <- (100 - scenes$Cloud.Cover) / 100
    scenes <- scenes[order(scenes$WRS.Path, scenes$WRS.Row), ]
    if (nrow(scenes) == 0) {
        stop(paste0('no data found in', x,
                    ' - is scenes an EarthExplorer CSV export?'))
    }
    # Drop ENGINEERING, TEST, EXCHANGE, and VALIDATION data. See the Landsat 
    # data dictionary at: https://lta.cr.usgs.gov/landsat_dictionary.html
    scenes <- scenes[scenes$Data.Category == 'NOMINAL', ]
    return(scenes)
}
  #+END_SRC
* espa_download.R
 #+BEGIN_SRC R 
verify_download <- function(espa_url, local_path) {
    cksum_file <- tempfile()
    ret_code <- download.file(gsub('\\.tar\\.gz$', '.cksum', espa_url), 
                              cksum_file, mode=w, quiet=TRUE)
    if (ret_code != 0) {
        message(paste('Warning: problem downloading cksum for', local_path))
        return(1)
    } else {
        # TODO: Check return code and handle accordingly
        # The first element is the cksum, second is the expected file size in 
        # bytes, and the third is the filename
        espa_checksum <- scan(cksum_file, what=c('integer', 'integer', 
                                                 'character'), quiet=TRUE)
        unlink(cksum_file)
        local_size <- file.info(local_path)$size
        # TODO: Figure out how to compute a checksum in R that matches the checksum 
        # output ESPA gives. It appears the ESPA checksum is a CRC from 'cksum' 
        # command run on Linux. This is not a CRC-32 checksum, so the R digest 
        # package won't work for computing it. bitops has a function, 'cksum' that 
        # might work.
        #local_crc <- strtoi(digest(, algo=crc32, file=TRUE), base=16L)
        # f = file(local_path,rb)
        # local_crc <- cksum(rawToChar(readBin(f, raw(), n=local_size)))
        # close(f)
        # if (espa_checksum[1] != local_crc) {
        #     return(2)
        # } else if (espa_checksum[2] != local_size) {
        if (espa_checksum[2] != local_size) {
            return(3)
        } else {
            return(0)
        }
    }
}
download_ESPA_file <- function(espa_url, output_path) {
    ret_code <- download.file(espa_url, output_path, mode=wb)
    if (ret_code != 0) {
        message(paste('Warning: problem downloading', output_path))
        return(1)
    } else if (verify_download(espa_url, output_path) != 0) {
        message(paste(Warning: checksum mismatch on, output_path))
        return(2)
    } else {
        return(0)
    }
}
#' Download a completed ESPA order
#'
#' Function to download a set of Landsat images from ESPA given a valid order 
#' ID and the email that placed the ESPA order.
#' 
#' @export
#' @importFrom RCurl getURL getCurlHandle postForm
#' @importFrom stringr str_extract
#' @param email address used to place the order
#' @param order_ID the ESPA order ID
#' @param output_folder the folder to save output data in
#' @param username your USGS EarthExplorer username
#' @param password your USGS EarthExplorer password
#' @return used for the side effect of downloading Landsat scenes
espa_download <- function(email, order_ID, output_folder, username,
                          password) {
    stop(Due to changes in the ESPA system, espa_download is not working as of 7/1/2014)
    email_re <- '^[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,4}$'
    if (!grepl(email_re, email, ignore.case=TRUE)) {
        stop(paste(email, 'does not appear to be a valid email address'))
    }
    # Below is for old format ESPA order IDs (through end of 2013)
    # ESPA generates order IDs with below Python code:
    #    d = datetime.datetime.now()
    #    return '%s-%s%s%s-%s%s%s' % (email,d.month,d.day,d.year,d.hour,d.minute,d.second)
    # Note that the above does not guarantee unique ESPA order IDs. See
    # https://code.google.com/p/espa/issues/detail?id=123 for details.
    mth_re <- '([1-9]|(1[0-2]))'
    day_re <- '([1-9]|([1-2][0-9])|(3[0-1]))'
    yr_re  <- '20[0-9][0-9]'
    hr_re  <- '([0-9]|(1[0-9])|(2[0-3]))'
    min_re <- '([0-9]|([1-5][0-9]))'
    sec_re <- '([0-9]|([1-5][0-9]))'
    order_id_re <- paste0('^', mth_re, day_re, yr_re, '-',  hr_re, min_re, sec_re, '$')
    if (!grepl(order_id_re, order_ID)) {
        stop(paste(order_ID, 'does not appear to be a valid ESPA order ID'))
    }
    
    if (!file_test('-d', output_folder)) {
        stop(paste(output_folder, 'does not appear to be a valid directory'))
    }
    # Parse ESPA page for download links
    # TODO: Rewrite using httr - see http://bit.ly/1m8ZWXf
    email_noat <- gsub('@', '%40', email)
    options(RCurlOptions=list(cainfo=system.file(CurlSSL, cacert.pem, 
                                                 package=RCurl)))
    curl=getCurlHandle()
    login_page <- unlist(strsplit(getURL('https://espa.cr.usgs.gov/login/', curl=curl), '\n'))
    csrfmiddlewaretoken <- login_page[grepl(csrfmiddlewaretoken, login_page)]
    csrfmiddlewaretoken <- gsub((value=)|('), '',
                                str_extract(csrfmiddlewaretoken, 
                                            value='[a-zA-Z0-9]*'))
    params <- list('username'=username,
                   'password'=password,
                   'submit'=Log In,
                   'next'=,
                   'csrfmiddlewaretoken'=csrfmiddlewaretoken)
    post_res <- postForm('https://espa.cr.usgs.gov/login',
                         .params=params, style=POST, curl=curl)
    tryCatch(espa_page <- getURL(paste0(http://espa.cr.usgs.gov/ordering/status/, email_noat, 
                             -, order_ID, curl=curl)),
             error=function(e) stop('error loading order - check order ID and email'))
    url_re <- paste0('http://espa\\.cr\\.usgs\\.gov/orders/', email, '-', 
                     order_ID, '/L[ET][0-9]{14}-SC[0-9]{14}\\.tar\\.gz')
    espa_urls <- espa_page[grepl(url_re, espa_page)]
    espa_urls <- str_extract(espa_urls, url_re)
    if (length(espa_urls) == 0) {
        stop('no download links found')
    }
    successes <- 0
    failures <- 0
    skips <- 0
    message(paste('Found', length(espa_urls), 'ESPA downloads.'))
    for (n in 1:length(espa_urls)) {
        espa_url <- espa_urls[n]
        img_file <- basename(espa_url)
        output_path <- file.path(output_folder, img_file)
        if (file.exists(output_path)) {
            if (verify_download(espa_url, output_path)) {
                message(paste(img_file, 'exists but has bad checksum - re-downloading file'))
            } else {
                message(paste(img_file, 'exists and has good checksum - skipping download'))
                skips <- skips + 1
                next
            }
        }
        if (download_ESPA_file(espa_url, output_path) == 0) {
            successes <- successes + 1
        } else {
            failures <- failures + 1
        }
    }
    message(paste(successes, file(s) succeeded,, skips, file(s) skipped,, 
                failures, file(s) failed.))
}
  #+END_SRC
* espa_extract.R
 #+BEGIN_SRC R 
#' Extract a set of Landsat tarballs into a folder tree organized by image date
#'
#' Each image .tar.gz file will be extracted into a subfolder within 
#' \code{output_folder}. The subfolders will be named according to the year and 
#' Julian date of capture, and the sensor type (LT4, LT5 or LE7 for Landsat 4 
#' TM, Landsat 5 TM, and Landsat 7 ETM+ respectively). For example, 
#' LT50150531986037-SC20130816144215.tar.gz would be extracted into a 
#' subfolder named 1986_037_LT5', for 1986, Julian day 37, and Landsat 5 TM.
#'
#' Zip files for images from sensors not included in \code{sensors}, from 
#' path/rows not included in \code{pathrows} or with acquisition dates
#' outside of the period defined by \code{start_date} and \code{end_date} will 
#' be ignored.
#'
#' @export
#' @importFrom stringr str_extract
#' @param in_folder Path to a folder of .tar.gz Landsat surface reflectance 
#' images
#' @param out_folder output folder
#' @param pathrows a list of paths/rows to include. Each path/row should be 
#' specified as a six digit string. For example, path 231, row 62 would be 
#' specified as 231062.
#' @param start_date start date of period from which images will be extracted
#' to (as \code{Date} object).
#' @param end_date end date of period from which images will be extracted
#' to (as \code{Date} object)
#' @param sensors a list of the sensors to include (can be any of LT4, LT5, 
#' LE7, or LC8)
#' @return nothing (used for side effect of unzipping Landsat CDR tarballs)
#' @examples
#' \dontrun{
#' # Don't filter:
#' espa_extract('D:/Landsat_Originals', 'D:/Landsat_Out')
#'
#' # Filter by start and end date:
#' start_date <- as.Date('2010/1/1')
#' end_date <- as.Date('2010/12/31')
#' espa_extract('D:/Landsat_Originals', 'D:/Landsat_Out',
#'              start_date=start_date, end_date=end_date)
#'
#' # Filter by start and end date, sensor, and pathrow:
#' espa_extract('D:/Landsat_Originals', 'D:/Landsat_Out', 
#'              start_date=start_date, end_date=end_date, sensors='LE7',
#'              pathrows='231062')
#' }
espa_extract <- function(in_folder, out_folder, pathrows=NULL, start_date=NULL, 
                         end_date=NULL, sensors=NULL) {
    if (!file_test('-d', in_folder)) {
        stop(paste(in_folder, 'does not exist'))
    }
    if (!file_test('-d', out_folder)) {
        stop(paste(out_folder, 'does not exist'))
    }

    zipfiles <- dir(in_folder, pattern='^.*.tar.gz(ip)?$')

    # Filter by date
    img_dates <- as.Date(gsub('-', '', str_extract(zipfiles, '[0-9]{7}-')), '%Y%j')
    if (!is.null(start_date)) {
        stopifnot(class(start_date) == 'Date')
        inc_dates <- which(img_dates >= start_date)
        zipfiles <- zipfiles[inc_dates]
        img_dates <- img_dates[inc_dates]
    }
    if (!is.null(end_date)) {
        stopifnot(class(end_date) == 'Date')
        inc_dates <- which(img_dates < end_date)
        zipfiles <- zipfiles[inc_dates]
        img_dates <- img_dates[inc_dates]
    }

    # Filter by pathrow
    img_pathrows <- gsub('(LT[45])|(LE7)|(LC8)', '', str_extract(zipfiles, '((LT[45])|(LE7)|(LC8))[0-9]{6}'))
    if (!is.null(pathrows)) {
        stopifnot(!is.na(str_extract(pathrows, '[0-9]{6}')))
        inc_pathrows <- img_pathrows %in% pathrows
        zipfiles <- zipfiles[inc_pathrows]
        img_pathrows <- img_pathrows[inc_pathrows]
        img_dates <- img_dates[inc_pathrows]
    }

    # Filter by sensor
    img_sensors <- str_extract(zipfiles, '^((LT[45])|(LE7)|(LC8))')
    if (!is.null(sensors)) {
        stopifnot(!is.na(str_extract(sensors, '^((LT[45])|(LE7)|(LC8))$')))
        inc_sensors <- img_sensors %in% sensors
        zipfiles <- zipfiles[inc_sensors]
        img_pathrows <- img_pathrows[inc_sensors]
        img_sensors <- img_sensors[inc_sensors]
        img_dates <- img_dates[inc_sensors]
    }

    img_paths <- str_extract(img_pathrows, '^[0-9]{3}')
    img_rows <- str_extract(img_pathrows, '[0-9]{3}$')

    if (length(zipfiles) == 0) {
        stop('No images found')
    }

    for (n in 1:length(zipfiles)) {
        zipfile_path <- file.path(in_folder, zipfiles[n])
        # Figure out which satellite the image is from
        year <- format(img_dates[n], '%Y')
        julian_day <- format(img_dates[n], '%j')
        this_out_folder <- file.path(out_folder,
                                     paste0(img_paths[n],'-', img_rows[n], '_', year, 
                                            '-', julian_day, '_', img_sensors[n]))
        if (!file_test('-d', this_out_folder)) {
            dir.create(this_out_folder)
        } else {
            message(paste('Skipping', zipfiles[n], '- output dir', 
                          this_out_folder, 'already exists.'))
            next
        }
        message(paste0(n, ' of ', length(zipfiles), '. Extracting ', zipfiles[n], ' to ', this_out_folder))
        ret_code <- untar(zipfile_path, exdir=file.path(this_out_folder))
        if (ret_code != 0) {
            message(paste('WARNING: error extracting', zipfiles[n], '- return 
                          code', ret_code))
        }
    }
}

  #+END_SRC
* espa_scenelist.R
 #+BEGIN_SRC R 
#' Save scenelist from EarthExplorer metadata for upload to ESPA
#'
#' @export
#' @importFrom lubridate new_interval %within%
#' @param x a \code{data.frame} with a list of Landsat scenes as output from 
#' the save metadata function on http://earthexplorer.usgs.gov
#' @param start_date starting date as a \code{Date} object
#' @param end_date end date as a \code{Date} object
#' @param out_file filename for output text file for later upload to ESPA
#' @param min_clear the minimum percent clear to plot (calculated as 1 - 
#' percent cloud cover). Images with less than \code{min_clear} fraction of the 
#' image area free of clouds will be ignored.
#' @param exclude a list of sensors to exclude (for example, set 
#' \code{exclude=c('LE7', 'LT4')} to exclude Landsat 7 ETM+ and Landsat 4 TM 
#' images.
#' @return used for side effect of producing ESPA scene list
espa_scenelist <- function(x, start_date, end_date, out_file, min_clear=.7, 
                         exclude=list()) {
    if (!class(start_date) == 'Date') {
        stop('start_date must be a Date object')
    }
    if (!class(end_date) == 'Date') {
        stop('end_date must be a Date object')
    }
    if ((!missing(start_date) && missing(end_date)) ||
        (missing(start_date) && !missing(end_date))) {
        stop('both start_date and end_date must be provided')
    } else if (!missing(start_date) && !missing(end_date)) {
        sel_interval <- new_interval(start_date, end_date)
        x <- x[x$Date.Acquired %within% sel_interval, ]
    }
    if (nrow(x) == 0) {
        stop('no data to download - try different start/end dates')
    }
    x <- x[!(x$Sensor %in% exclude), ]
    x <- x[x$Frac_Clear >= min_clear, ]
    write.table(x$Landsat.Scene.Identifier, out_file, row.names=FALSE, 
                col.names=FALSE, quote=FALSE, sep='\n')
}
  #+END_SRC
* fill_gaps.R
 #+BEGIN_SRC R 
#' Perform SLC-off gap fill of Landsat 7 ETM+ image
#'
#' Calls GNSPI.pro IDL script by Xiaolin Zhu to fill gaps in SLC-off Landsat 7 
#' ETM+ image. The script requires \code{fill} to be a TM (Landsat 5) image.  
#' \code{slc_off} must be a Landsat 7 SLC-off image.
#'
#' If supplied, \code{timeseries} should be a list of TM images.  Performing 
#' gap fill using SLC-Off ETM+ images as the input is not yet supported.
#' 
#' Pixels in gaps, background, and/or clouds in \code{slc_off}, 
#' \code{input_image}, and the images in \code{timeseries} should be coded as 
#' 0.
#'
#' @importFrom tools file_path_sans_ext
#' @param slc_off the SLC-off Landsat 7 file to gap fill, as a \code{Raster*}
#' @param fill the first TM image to use to fill in the gaps, as a 
#' \code{Raster*}
#' @param timeseries a timeseries of TM images as \code{Raster*} objects to use 
#' as additional inputs to the gap fill algorithm (optional)
#' @param out_base path and base filename for the output file. The script will 
#' save the output files by appending _GNSPI.envi and 
#' _GNSPI_uncertainty.envi to this base filename.
#' @param ext file extension to use when and when saving output rasters 
#' (determines output file format). Must be supported by 
#' \code{\link{writeRaster}}.
#' @param algorithm the algorithm to use, as a string (GNSPI_IDL is currently 
#' the only supported algorithm)
#' @param sample_size the sample size of sample pixels
#' @param size_wind the maximum window size
#' @param class_num the estimated number of classes
#' @param DN_min the minimum DN value of the image
#' @param DN_max the maximum DN value of the image
#' @param patch_long the size of block, to process whole ETM scene, set to 1000
#' @param idl path to the IDL binary
#' @param verbose whether to print detailed status messages
#' @param overwrite whether to overwrite output files if they already exist
#' @return a list of two rasters: 1) filled, the gap filled image, and 2) 
#' uncertainty, the uncertainty image.
#' @export
#' @references Zhu, X., Liu, D., Chen, J., 2012. A new geostatistical approach 
#' for filling gaps in Landsat ETM+ SLC-off images. Remote Sensing of 
#' Environment 124, 49--60.
#' @examples
#' \dontrun{
#' slc_off <- brick(system.file('tests', 'testthat_idl', 'fill_gaps', 
#' 'TM20100429_toaR_gap', package='teamlucc'))
#' fill <- brick(system.file('tests', 'testthat_idl', 'fill_gaps', 
#' 'TM20100515_toaR', package='teamlucc'))
#' timeseries <- c(brick(system.file('tests', 'testthat_idl', 'fill_gaps', 
#' 'TM20100208_toaR', package='teamlucc')))
#' filled <- fill_gaps(slc_off, fill, timeseries)
#' }
fill_gaps <- function(slc_off, fill, timeseries=c(), out_base=NULL, ext=tif,
                      algorithm=GNSPI_IDL, sample_size=20, size_wind=12, 
                      class_num=4, DN_min=0.0, 
                      DN_max=1.0, patch_long=1000,
                      idl=C:/Program Files/Exelis/IDL83/bin/bin.x86_64/idl.exe,
                      verbose=FALSE, overwrite=FALSE) {
    if (!(class(slc_off) %in% c(RasterLayer, RasterStack, RasterBrick))) {
        stop('slc_off must be a Raster* object')
    }
    if (!(class(fill) %in% c(RasterLayer, RasterStack, RasterBrick))) {
        stop('fill must be a Raster* object')
    }
    if (nlayers(slc_off) != nlayers(fill)) {
        stop('number of layers in slc_off must match number of layers in fill')
    }
    compareRaster(slc_off, fill)
    if (!(algorithm %in% c('GNSPI_IDL'))) {
        stop('algorithm must be GNSPI_IDL - no other algorithms are supported')
    }
    ext <- gsub('^[.]', '', ext)
    for (timeseries_img in timeseries) {
        if (!(class(timeseries_img) %in% c(RasterLayer, RasterStack, RasterBrick))) {
            stop('each timeseries image be a Raster* object')
        }
        if (nlayers(slc_off) != nlayers(timeseries_img)) {
            stop('number of layers in slc_off must match number of layers of each image in timeseries')
        }
        compareRaster(slc_off, timeseries_img)
    }
    if (is.null(out_base)) {
        out_base <- file_path_sans_ext(rasterTmpFile())
    } else {
        out_base <- normalizePath(out_base, mustWork=FALSE)
        if (!file_test('-d', dirname(out_base))) {
            stop('output folder does not exist')
        }
        if (!overwrite && file_test('-f', file.path(out_base, paste0('_GNSPI.', ext)))) {
            stop('output file already exists - use a different out_base')
        }
        if (!overwrite && file_test('-f', file.path(out_base, paste0('_GNSPI_uncertainty.', ext)))) {
            stop('output uncertainty file already exists - use a different out_base')
        }
    }
    
    if (algorithm == GNSPI_IDL) {
        filled <- fill_gaps_idl(slc_off, fill, timeseries, out_base, 
                                sample_size, size_wind, class_num, DN_min, 
                                DN_max, patch_long, idl, algorithm, ext, 
                                verbose)
    } else {
        stop(Native R gap filling not yet supported)
    }
    return(filled)
}
fill_gaps_idl <- function(slc_off, fill, timeseries, out_base, sample_size, 
                          size_wind, class_num, DN_min, DN_max, patch_long, 
                          idl, algorithm, ext, verbose) {
    if (verbose) {
        warning('verbose=TRUE not supported when algorithm=GNSPI_IDL')
    }
    script_path <- system.file(idl, GNSPI.pro, package=teamlucc)
    if (!(file_test('-x', idl) || file_test('-f', idl))) {
        stop('IDL not found - check idl parameter')
    }
    if (!check_ENVI_IDL(idl)) {
        stop(Unable to load ENVI in IDL - do you have ENVI and IDL licenses, and ENVI >= 5.0?)
    }
    # Save proj4string and extend to ensure the same proj4string and extent is 
    # returned even if they are changed by IDL
    orig_proj <- proj4string(slc_off)
    orig_ext <- extent(slc_off)
    orig_datatype <- dataType(slc_off)[1]
    # Write in-memory rasters to files for hand off to IDL. The capture.output 
    # line is used to avoid printing the rasterOptions to screen as they are 
    # temporarily reset.
    dummy <- capture.output(def_format <- rasterOptions()$format)
    rasterOptions(format='ENVI')
    slc_off <- writeRaster(slc_off, rasterTmpFile(), datatype=dataType(slc_off)[1])
    slc_off_file <- filename(slc_off)
    fill <- writeRaster(fill, rasterTmpFile(), datatype=dataType(fill)[1])
    fill_file <- filename(fill)
    timeseries_files <- c()
    if (length(timeseries) == 0) {
        # Ensure timeseries_files equals an empty matrix, in IDL format
        timeseries_files <- list()
    } else {
        for (timeseries_img in timeseries) {
            timeseries_img <- writeRaster(timeseries_img, rasterTmpFile(), datatype=dataType(fill)[1])
            timeseries_files <- c(timeseries_files, filename(timeseries_img))
        }
    }
    temp_dir <- tempdir()
    dummy <- capture.output(rasterOptions(format=def_format))
    # Save IDL output to a temp folder - it will be copied over and saved with 
    # writeRaster later to ensure the extents and projection are not modified 
    # from those of the original files.
    temp_out_base <- file_path_sans_ext(rasterTmpFile())
    param_vals <- list(slc_off_file, fill_file, timeseries_files,
                       temp_out_base, sample_size, size_wind, class_num,
                       DN_min, DN_max, patch_long, temp_dir)
    param_names <- list('slc_off_file', 'input_file', 'timeseries_files', 
                        'out_base', 'sample_size', 'size_wind', 'class_num', 
                        'DN_min', 'DN_max', 'patch_long', 'temp_dir')
    idl_params <- mapply(format_IDL_param, param_names, param_vals)
    idl_params <- paste(idl_params, collapse='')
    script_dir <- dirname(script_path)
    idl_script <- tempfile(fileext='.pro')
    idl_cmd <- paste0('CD, ', script_dir, '\n', idl_params, 'GNSPI,', 
                      paste(param_names, collapse=','), '\nexit')
    f <- file(idl_script, 'wt')
    writeLines(idl_cmd, f)
    close(f)
    idl_out <- system(paste(shQuote(idl), shQuote(idl_script)), intern=TRUE)
    log_file <- paste0(out_base, '_GNSPI_idllog.txt')
    idl_out <- gsub('\r', '', idl_out)
    f <- file(log_file, 'wt')
    writeLines(idl_out, f) 
    close(f)
    filled <- brick(paste0(temp_out_base, '_GNSPI.envi'))
    filled_out_file <- paste0(out_base, paste0('_GNSPI.', ext))
    proj4string(filled) <- orig_proj
    extent(filled) <- orig_ext
    filled <- writeRaster(filled, filename=filled_out_file, overwrite=TRUE, 
                          datatype=orig_datatype)
    uncertainty <- brick(paste0(temp_out_base, '_GNSPI_uncertainty.envi'))
    uncertainty_out_file <- paste0(out_base, paste0('_GNSPI_uncertainty.', ext))
    proj4string(uncertainty) <- orig_proj
    extent(uncertainty) <- orig_ext
    uncertainty <- writeRaster(uncertainty, filename=uncertainty_out_file, 
                               overwrite=TRUE, datatype=orig_datatype)
    return(list(filled=filled, uncertainty=uncertainty))
}
  #+END_SRC
* get_band_names_from_hdr.R
 #+BEGIN_SRC R 
#' Extract the band names listed in an ENVI format header file (.hdr)
#'
#' @export
#' @param hdr_file an ENVI format header file with a .hdr extension
#' @return A /code{list} of band names extracted from the /code{hdr_file}
get_band_names_from_hdr <- function(hdr_file) {
    txt <- readLines(hdr_file)
    line_num <- which(grepl('^band names', txt)) + 1
    band_names <- c()
    for (n in line_num:length(txt)) {
        band_names <- c(band_names, gsub('[,}][[:space:]]*$', '', txt[n]))
        if (grepl('}', txt[n])) {
            break
        }
    }
    return(band_names)
}
  #+END_SRC
* get_extent_polys.R
 #+BEGIN_SRC R 
#' Generate a SpatialPolygonDataFrame of raster extents
#'
#' Also includes the filename associated with each raster object. Useful for 
#' providing the \code{dem_extents} argument to the 
#' \code{\link{auto_setup_dem}} function.
#'
#' @export
#' @importFrom maptools spRbind
#' @param rast_list a \code{Raster*} object, or \code{list} of\code{Raster*} objects
#' @return \code{SpatialPolygonDataFrame} with the extent of each raster object 
#' as a polygon, with a filename attribute giving the filename for the raster 
#' object from with each extent is derived.
get_extent_polys <- function(rast_list) {
    if (!is.list(rast_list)) rast_list <- list(rast_list)
    proj4strings <- lapply(rast_list, function(x) proj4string(x))
    if (!all(proj4strings == proj4strings[[1]])) {
        stop('every raster in rast_list must have the same projection')
    }
    extents <- lapply(rast_list, function(x) extent(x))
    filenames <- lapply(rast_list, function(x) filename(x))
    # Convert extents to a list of SpatialPolygons objects
    extent_sps_list <- lapply(extents, function(x) as(x, 'SpatialPolygons'))
    # Convert from list of SpatialPolygons objects to a single SpatialPolygons 
    # object
    extent_sps <- extent_sps_list[[1]]
    if (length(extent_sps_list) > 1) {
        for (n in 2:length(extent_sps_list)) {
            extent_sps <- spRbind(extent_sps, spChFIDs(extent_sps_list[[n]], 
                                                 as.character(n)))
        }
    }
    # Finally convert the SpatialPolygons object into a 
    # SpatialPolygonsDataFrame that also includes the filename of the raster 
    # associated with each extent polygon as an attribute
    extent_polys <- SpatialPolygonsDataFrame(extent_sps, 
                                             data=data.frame(filename=unlist(filenames)))
    proj4string(extent_polys) <- proj4strings[[1]]
    extent_polys$filename <- as.character(extent_polys$filename)
    return(extent_polys)
}
  #+END_SRC
* get_metadata_item.R
 #+BEGIN_SRC R 
#' Extract a metadata item from a metadata file in GDAL PAM format
#'
#' GDAL PAM format metadata files end in .aux.xml.
#'
#' @export
#' @importFrom raster extension
#' @importFrom XML xmlInternalTreeParse xpathApply xmlValue
#' @param x an image file that has an accompanying GDAL PAM format metadata 
#' file (ending in .aux.xml)
#' @param key a string giving the name of the metadata item to extract
#' @return The metadata item (as a string)
get_metadata_item <- function(x, key) {
    metadata_file <- paste0(x, '.aux.xml')
    if (!file.exists(metadata_file)) {
        stop(paste('Could not find metadata file', metadata_file))
    }
    doc <- xmlInternalTreeParse(metadata_file)
    xpath_exp <- paste0(//MDI[@key=', key, '])
    value <- unlist(xpathApply(doc, xpath_exp, xmlValue))
    if (length(value) > 1) {
        stop('multiple elements found')
    }
    if (length(value) == 0) {
        stop('no elements found')
    }
    return(value)
}
  #+END_SRC
* gridsample.R
 #+BEGIN_SRC R 
#' Draw a random sample from a grid laid out on a RasterLayer or matrix
#'
#' This function is used to subsample a \code{RasterLayer} or \code{matrix} by 
#' dividing the dataset into a grid of \code{horizcells} x \code{vertcells}, 
#' and by then drawing a sample of size \code{nsamp} from within each grid 
#' cell.
#'
#' @export
#' @param x a matrix or RasterLayer to draw sample from
#' @param horizcells how many cells to break the raster in horizontally (over 
#' the columns)
#' @param vertcells how many cells to break the raster in vertically (over 
#' the rows)
#' @param nsamp how many samples to draw from each grid cell
#' @param rowmajor whether to return indices in row-major format (default is 
#' column-major). Row-major format is useful in conjunction with \code{Raster*} 
#' objects.
#' @param replace whether to sample with replacement (within each grid cell)
#' @return vector of sample indices
#' @note TODO: Recode in C++ for speed.
#' @examples
#' # Make a 100x100 matrix
#' x <- matrix(1:10000, nrow=100)
#' # Subsample the matrix by breaking it up into a 10 x 10 grid, and take 10
#' # random samples from each grid cell without replacement (these are the
#' # default parameters).
#' y <- gridsample(x)
gridsample <- function(x, horizcells=10, vertcells=10, nsamp=10, 
                       rowmajor=FALSE, replace=FALSE) {
    # horizstart is a vector of column numbers of the first column in each cell 
    # in the grid
    horizstart <- round(seq(1, ncol(x), ncol(x) / horizcells))
    # horizend is a vector of column numbers of the last column in each cell in 
    # the grid
    if (length(horizstart) > 1) {
        horizend <- c((horizstart - 1)[2:length(horizstart)], ncol(x))
    } else {
        horizend <- c(ncol(x))
    }
    # vertstart is a vector of row numbers of the first row in each cell in the 
    # grid
    vertstart <- round(seq(1, nrow(x), nrow(x) / vertcells))
    # vertend is a vector of row numbers of the last row in each cell in the
    # grid
    if (length(vertstart) > 1) {
        vertend <- c((vertstart - 1)[2:length(vertstart)], nrow(x))
    } else {
        vertend <- c(nrow(x))
    }
    # retvalues is a vector to store the sample values chosen from x
    # sampindices is a vector to store the column major indices of the locations 
    # of each sampled value in x
    sampindices <- vector('numeric', horizcells * vertcells * nsamp)
    # retval_index tracks position in the vector storing the sampled values 
    # returned from this function
    retval_index <- 1
    # cell1row is used in calculating the column-major index of the first entry 
    # (top left) of each grid cell
    cell1row <- 1
    for (vertcellnum in 1:length(vertstart)) {
        # cell1col is used in calculating the column-major index of the first 
        # entry (top left) of each grid cell
        cell1col <- 1
        # cell_nrows is the number of rows in this particular cell (cells may 
        # have varying numbers of rows due to rounding)
        cell_nrows <- vertend[vertcellnum] - vertstart[vertcellnum] + 1
        for (horizcellnum in 1:length(horizstart)) {
            # cell1colmajindex is the column-major index of the first (top 
            # left) value in this particular grid cell
            cell1colmajindex <- cell1row + nrow(x) * (cell1col - 1)
            # cell_ncols is the number of rows in this particular cell (cells 
            # may have varying numbers of columns due to rounding)
            cell_ncols <- horizend[horizcellnum] - horizstart[horizcellnum] + 1
            # cell_colmaj_indices is a matrix of column-major indices of the 
            # position of each value in this grid cell, within the larger 
            # matrix x
            cell_colmaj_indices <- matrix(rep(1:cell_nrows, cell_ncols),
                                          nrow=cell_nrows) +
                                   matrix(rep(seq(cell1colmajindex, by=nrow(x), 
                                                  length.out=cell_ncols), 
                                              cell_nrows), nrow=cell_nrows, 
                                          byrow=TRUE) - 1
            samp_indices <- sample(cell_colmaj_indices, nsamp, replace=replace)
            sampindices[retval_index:(retval_index + length(samp_indices) - 1)] <- samp_indices
            retval_index <- retval_index + length(samp_indices)
            cell1col  <- cell1col + cell_ncols
        }
        cell1row <- cell1row + cell_nrows
    }
    if (rowmajor) {
        sampindices <- ((sampindices - 1) %% nrow(x)) * ncol(x) + ((sampindices - 1) %/% nrow(x) + 1)
    }
    return(sampindices)
}
  #+END_SRC
* linear_stretch.R
 #+BEGIN_SRC R 
.do_stretch <- function(x, pct, max_val) {
    lower <- quantile(x, prob=0 + pct/100, na.rm=TRUE)
    upper <- quantile(x, prob=1 - pct/100, na.rm=TRUE)
    x[x > upper] <- upper
    x[x < lower] <- lower
    x <- ((x - lower) / (upper-lower)) * max_val
    return(x)
}
#' Apply a linear stretch to an image
#'
#' Applies a linear stretch to an image (default linear 2% stretch), and 
#' returns the image with each band individually stretched and rescaled to 
#' range between zero and \code{max_val} (default of \code{max_val} is 1).
#'
#' @export
#' @param x image to stretch
#' @param pct percent stretch
#' @param max_val maximum value of final output (image will be rescaled to 
#' range from 0 - \code{max_val})
#' @return image with stretch applied
linear_stretch <- function(x, pct=2, max_val=1) {
    # Applies linear stretch (2 percent by default). Assumes image is arranged 
    # with bands in columns. Returns the image with stretch applied and bands 
    # rescaled to range from 0 - max_val.
    if ((pct < 0) | pct >= 50) {
        stop('pct must be > 0 and < 50')
    }
    if (class(x) %in% c('RasterLayer')) {
        x <- setValues(x, .do_stretch(getValues(x), pct, max_val))
        return(x)
    } else if (class(x) %in% c('RasterStack', 'RasterBrick')) {
        for (n in 1:nlayers(x)) {
            x <- setValues(x, .do_stretch(getValues(raster(x, layer=n)),
                                          pct, max_val), layer=n)
        }
        return(x)
    } else if (is.null(dim(x)) || (length(dim(x)) == 2) || (dim(x)[3] == 1)) {
        # Handle a vector, matrix, or n x m x 1 array
        return(.do_stretch(x, pct, max_val))
    }
}
  #+END_SRC
* ls_catalog.R
 #+BEGIN_SRC R 
#' Catalog a folder of Landsat images
#'
#' This function is used to produce a \code{data.frame} of Landsat images 
#' stored locally after download from the USGS. The images should be in a 
#' series of subfolders named following the naming scheme of 
#' \code{\link{espa_extract}}.
#'
#' @export
#' @importFrom stringr str_extract
#' @param in_folder path to a folder of Landsat surface reflectance images (for 
#' example, as extracted by the \code{espa_extract} function).
#' @return a \code{data.frame} with a list of the Landsat images found within 
#' in_folder
ls_catalog <- function(in_folder) {
    if (!file_test('-d', in_folder)) {
        stop(paste(in_folder, 'does not exist'))
    }
    out <- c()
    for (outer_item in dir(in_folder)) {
        outer_item_full <- file.path(in_folder, outer_item)
        # First cycle through the yearly folders
        if (!file_test('-d', outer_item_full)) {
            next
        }
        for (inner_item in dir(outer_item_full)) {
            inner_item_full <- file.path(outer_item_full, inner_item)
            # Check to ensure inner item is a Landsat HDF file
            if (!file_test('-f', inner_item_full) ||
                !grepl('^(lndsr.)?((LT4)|(LT5)|(LE7)|(LC8))[0-9]{13}[A-Z]{3}[0-9]{2}.hdf$', 
                       inner_item)) {
                next
            }
            metadata_string <- str_extract(inner_item, 
                                           '((LT4)|(LT5)|(LE7)|(LC8))[0-9]{13}')
            if (grepl('^LT4', metadata_string)) {
                sensor <- 'LT4'
            } else if (grepl('^LT5', metadata_string)) {
                sensor <- 'LT5'
            } else if (grepl('^LE7', metadata_string)) {
                sensor <- 'LE7'
            } else if (grepl('^LC8', metadata_string)) {
                sensor <- 'LC8'
            } else {
                message(paste('Skipping', inner_item,
                              '- cannot determine sensor from filename.'))
                next
            }
            year <- substr(metadata_string, 10, 13)
            julian_day <- substr(metadata_string, 14, 16)
            img_path <- substr(metadata_string, 4, 6)
            img_row <- substr(metadata_string, 7, 9)
            img_date <- as.Date(paste0(year, julian_day), '%Y%j')
            out <- c(out, list(list(img_path,
                                    img_row,
                                    year,
                                    julian_day,
                                    sensor,
                                    format(img_date, '%m'), 
                                    format(img_date, '%d'),
                                    inner_item)))
        }
    }
    if (is.null(out)) {
        stop('no images found')
    }
    out <- data.frame(matrix(unlist(out), nrow=length(out), byrow=T))
    names(out) <- c('path', 'row', 'year', 'julian', 'sensor',
                    'month', 'day', 'filename')
    out <- out[order(out$path, out$row, out$year, out$julian, out$sensor), ]
    return(out)
}
  #+END_SRC
* match_rasters.R
 #+BEGIN_SRC R 
#' Match the coordinate system and extent of two rasters
#'
#' @export
#' @param baseimg A /code{Raster*} to use as the base image. This layer will 
#' determine the output coordinate system.
#' @param matchimg A /code{Raster*} to match to the base image. If necessary 
#' the /code{matchimg} will be reprojected to match the coordinate system of 
#' the /code{baseimg}. The /code{matchimg} will then be cropped and extended to 
#' match the extent of the /code{baseimg}.
#' @param filename file on disk to save \code{Raster*} to (optional)
#' @param method the method to use if projection is needed to match image 
#' coordinate systems, or if resampling is needed to align image origins. Can 
#' be ngb for nearest-neighbor, or binlinear for bilinear interpolation
#' @param ... additional arguments to pass to \code{writeRaster} (such as 
#' datatype and filename)
#' @return The /code{matchimg} reprojected (if necessary), cropped, and 
#' extended to match the /code{baseimg}.
#' @details Note that \code{match_rasters} can run in parallel if 
#' \code{beginCluster()} is run prior to running \code{match_rasters}.
#' @examples
#' # Mosaic the two ASTER DEM tiles needed to a Landsat image
#' DEM_mosaic <- mosaic(ASTER_V002_EAST, ASTER_V002_WEST, fun='mean')
#' 
#' # Crop and extend the DEM mosaic to match the Landsat image
#' matched_DEM <- match_rasters(L5TSR_1986, DEM_mosaic)
match_rasters <- function(baseimg, matchimg, filename, method='bilinear',
                          ...) {
    if (projection(baseimg) != projection(matchimg)) {
        matchimg <- projectRaster(from=matchimg, to=baseimg, method=method)
    }
    # Crop overlapping area
    matchimg <- crop(matchimg, baseimg)
    if (any(res(matchimg) != res(baseimg))) {
        matchimg <- resample(matchimg, baseimg, method=method)
    }
    if (extent(matchimg) != extent(baseimg)) {
        matchimg <- extend(matchimg, baseimg)
    }
    if (!missing(filename)) {
            matchimg <- writeRaster(matchimg, filename=filename, ...)
    }
    return(matchimg)
}
  #+END_SRC
* minnaert_samp.R
 #+BEGIN_SRC R 
#' @import mgcv
.calc_k_table <- function(x, IL, slope, sampleindices, slopeclass,
                          coverclass, sunzenith) {
    if (!is.null(sampleindices)) {
        K <- data.frame(x=x[sampleindices],
                        IL=IL[sampleindices], 
                        slope=slope[sampleindices])
        # Remember that the sample indices are row-major (as they were drawn 
        # for a RasterLayer), so the coverclass matrix needs to be transposed 
        # as it is stored in column-major order
        if(!is.null(coverclass)) coverclass <- t(coverclass)[sampleindices]
    } else {
        K <- data.frame(x=getValues(x), IL=getValues(IL), 
                        slope=getValues(slope))
    }
    if(!is.null(coverclass)) {
        K <- K[coverclass, ]
    }
    ## K is between 0 and 1
    # IL can be <=0 under certain conditions
    # but that makes it impossible to take log10 so remove those elements
    K <- K[!apply(K, 1, function(rowvals) any(is.na(rowvals))), ]
    K <- K[K$x > 0, ]
    K <- K[K$IL > 0, ]
    k_table <- data.frame(matrix(0, nrow=length(slopeclass) - 1, ncol=3))
    names(k_table) <- c('midpoint', 'n', 'k')
    k_table$midpoint <- diff(slopeclass)/2 + slopeclass[1:length(slopeclass) - 1]
    # don't use slopes outside slopeclass range
    K.cut <- as.numeric(cut(K$slope, slopeclass))
    if(nrow(k_table) != length(table(K.cut))) {
        stop(slopeclass is inappropriate for these data (empty classes)\n)
    }
    k_table$n <- table(K.cut)
    for(i in sort(unique(K.cut[!is.na(K.cut)]))) {
        k_table$k[i] <- coefficients(lm(log10(K$x)[K.cut == i] ~ 
                                        log10(K$IL/cos(sunzenith))[K.cut == i]))[[2]]
    }
    return(k_table)
}
# Function to combine classes defined by the upper limits 'lims' with their 
# smallest neighbors until each class has at least n members. 'counts' stores 
# the number of members in each class.
clean_intervals <- function(counts, lims, n) {
    while(min(counts) < n) {
        # The rev below is so that the classes at the end are combined first 
        # (as classes with higher slopes are more more likely to be more rare 
        # and therefore have fewer members)
        min_index <- length(counts) - match(TRUE,
                                            rev(counts == min(counts))) + 1
        if (min_index == length(counts)) {
            counts[min_index - 1] <- counts[min_index - 1] + counts[min_index]
            lims <- lims[-(min_index - 1)]
        } else if (min_index == 1) {
            counts[min_index + 1] <- counts[min_index + 1] + counts[min_index]
            lims <- lims[-min_index]
        } else if (counts[min_index - 1] < counts[min_index + 1]) {
            counts[min_index - 1] <- counts[min_index - 1] + counts[min_index]
            lims <- lims[-(min_index - 1)]
        } else {
            # We know counts[min_index - 1] > counts[min_index + 1]
            counts[min_index + 1] <- counts[min_index + 1] + counts[min_index]
            lims <- lims[-min_index]
        }
        counts <- counts[-min_index]
    }
    return(lims)
}
#' Topographic correction for satellite imagery using Minnaert method
#'
#' Perform topographic correction using the Minnaert method. This code is 
#' modified from the code in the \code{landsat} package written by Sarah 
#' Goslee.  This version of the code has been altered from the \code{landsat} 
#' version to allow the option of using a sample of pixels for calculation of k 
#' in the Minnaert correction (useful when dealing with large images).
#' 
#' See the help page for \code{minnaert} in the \code{landsat} package for 
#' additional details on the parameters.
#'
#' @export
#' @param x image as a \code{RasterLayer}
#' @param slope the slope in radians as a \code{RasterLayer}
#' @param aspect the aspect in radians as a \code{RasterLayer}
#' @param sunelev sun elevation in degrees
#' @param sunazimuth sun azimuth in degrees
#' @param IL.epsilon a small amount to add to calculated illumination values 
#' that are equal to zero to avoid division by zero resulting in Inf values
#' @param slopeclass the slope classes to calculate k for (in radians), or 
#' NULL, in which case an algorithm will be used to choose reasonable defaults 
#' for the given image. If provided, \code{slopeclass} should be a list of 
#' slope class limits. For example: c(1, 5, 10, 15, 20, 25, 30, 45) * (pi/180)
#' @param coverclass used to calculate k for specific cover class (optional)
#' as \code{RasterLayer}
#' @param sampleindices (optional) row-major indices of sample pixels to use in 
#' the calculation of k values for the Minnaert correction. See
#' \code{\link{gridsample}}.
#' @param DN_min minimum allowable pixel value after correction (values less 
#' than \code{DN_min} are set to NA)
#' @param DN_max maximum allowable pixel value after correction (values less 
#' than \code{DN_max} are set to NA)
#' @return \code{RasterLayer} with topographically corrected data
#' @author Sarah Goslee and Alex Zvoleff
#' @references
#' Sarah Goslee. Analyzing Remote Sensing Data in {R}: The {landsat} Package.  
#' Journal of Statistical Software, 2011, 43:4, pg 1--25.  
#' http://www.jstatsoft.org/v43/i04/
minnaert_samp <- function(x, slope, aspect, sunelev, sunazimuth,
                          IL.epsilon=0.000001, slopeclass=NULL, 
                          coverclass=NULL, sampleindices=NULL, DN_min=NULL, 
                          DN_max=NULL) {
    if (is.null(slopeclass)) {
        slopeclass <- c(1, 2, 3, 4, 5, 6, 8, 10, 12,
                        15, 20, 25, 30, 45, 75) * (pi/180)
    }
    if (is.null(sampleindices)) {
        counts <- raster::freq(raster::cut(slope, slopeclass,
                                           include.lowest=TRUE), 
                               useNA='no')
        # Eliminate empty bins:
        slopeclass <- slopeclass[c(TRUE, 1:(length(slopeclass) - 1) %in% counts[, 1])]
        counts <- counts[, 2]
    } else {
        counts <- as.numeric(table(cut(slope[sampleindices], slopeclass, 
                                       include.lowest=TRUE), useNA='no'))
    }
    # The [-1] below is because clean_intervals only needs the upper limits
    slopeclass <- clean_intervals(counts, slopeclass[-1], 100)
    if (length(slopeclass) <= 5) {
        stop('insufficient sample size to develop k model - try changing slopeclass or sampleindices')
    }
    slopeclass <- c(1*pi/180, slopeclass)
    stopifnot(all((slopeclass >= 0) & slopeclass <= pi/2))
    # some inputs are in degrees, but we need radians
    stopifnot((sunelev >= 0) & (sunelev <= 90))
    stopifnot((sunazimuth >= 0) & (sunazimuth <= 360))
    sunzenith <- (pi/180) * (90 - sunelev)
    sunazimuth <- (pi/180) * sunazimuth
    IL <- .calc_IL(slope, aspect, sunzenith, sunazimuth, IL.epsilon)
    rm(aspect, sunazimuth)
    k_table <- .calc_k_table(x, IL, slope, sampleindices, slopeclass, 
                             coverclass, sunzenith)
    
    k_model <- with(k_table, bam(k ~ s(midpoint, k=length(midpoint) - 1), data=k_table))
    # If slope is greater than modeled range, use maximum of modeled range. If 
    # slope is less than modeled range, treat it as flat.
    slopeclass_max <- max(slopeclass)
    slopeclass_min <- min(slopeclass)
    slope <- calc(slope,
                  fun=function(vals) {
                      vals[vals > slopeclass_max] <- slopeclass_max
                      vals[vals < slopeclass_min] <- 0
                      return(vals)
                  })
    names(slope) <- 'midpoint'
    K.all <- predict(slope, k_model)
    K.all <- calc(K.all,
                  fun=function(vals) {
                      vals[vals > 1] <- 1
                      vals[vals < 0] <- 0
                      return(vals)
                  })
    # Perform correction
    xout <- overlay(x, IL, K.all,
                    fun=function(x_vals, IL_vals, K.all_vals) {
                        x_vals * (cos(sunzenith)/IL_vals) ^ K.all_vals
                    })
    # Don't correct flat areas
    xout[K.all == 0 & !is.na(K.all)] <- x[K.all == 0 & !is.na(K.all)]
    if ((!is.null(DN_min)) || (!is.null(DN_max))) {
        xout <- calc(xout, fun=function(vals) {
                        if (!is.null(DN_min)) vals[vals < DN_min] <- NA
                        if (!is.null(DN_max)) vals[vals > DN_max] <- NA
                        return(vals)
                     })
    }
    list(classcoef=k_table, model=k_model, minnaert=xout, sampleindices=sampleindices)
}
  #+END_SRC
* normalize.R
 #+BEGIN_SRC R 
#' Normalizes two rasters
#'
#' Performs relative normalization on two rasters using model II regression. 
#' Based on the approach in the \code{relnorm} function in the \code{landsat} 
#' package.
#'
#' This function will run in parallel if a parallel backend is registered with 
#' \code{\link{foreach}}.
#'
#' @export
#' @importFrom iterators iter
#' @import foreach
#' @importFrom lmodel2 lmodel2
#' @param x a \code{Raster*} to use as the base image
#' @param y a \code{Raster*} to normalize to the base image
#' @param msk a \code{RasterLayer} with missing values in \code{x} or in {y} 
#' coded as 1, and all other values coded as 0 (optional)
#' @param method the regression method to use (must be a method recognized by 
#' \code{lmodel2}
#' @param size the number of pixels to use in developing the model
#' @return a \code{Raster*} of \code{y} normalized to \code{x}
#' @examples
#' L5TSR_2001_normed_1 <- normalize(L5TSR_1986, L5TSR_2001)
#' plotRGB(L5TSR_2001_normed_1, stretch='lin')
#'
#' # Use only half as many pixels to calculate the models
#' L5TSR_2001_normed_2 <- normalize(L5TSR_1986, L5TSR_2001, 
#'                                  size=ncell(L5TSR_1986)/2)
#' plotRGB(L5TSR_2001_normed_2, stretch='lin')
#' @references
#' Sarah Goslee. Analyzing Remote Sensing Data in {R}: The {landsat} Package.  
#' Journal of Statistical Software, 2011, 43:4, pg 1--25.  
#' http://www.jstatsoft.org/v43/i04/
normalize <- function(x, y, msk, method=MA, size=ncell(x)) {
    orig_datatype <- dataType(y)[1]
    compareRaster(x, y)
    stopifnot(nlayers(x) == nlayers(y))
    stopifnot(size <= ncell(x))
    if (!missing(msk)) {
        compareRaster(x, msk)
        stopifnot(nlayers(msk) == 1)
    }
    if (size < ncell(x)) {
        # Note that sampleRegular with cells=TRUE returns cell numbers in the 
        # first column
        x_vals <- sampleRegular(x, size=size, cells=TRUE)
        if (!missing(msk)) {
            x_vals <- x_vals[!(msk[x_vals[, 1]]), ]
        }
        y_vals <- y[x_vals[, 1]]
        x_vals <- x_vals[, -1]
    } else {
        x_vals <- getValues(x)
        y_vals <- getValues(y)
        if (!missing(msk)) {
            x_vals <- x_vals[!getValues(msk), ]
            y_vals <- y_vals[!getValues(msk), ]
        }
    }
    names(y_vals) <- names(x_vals)
    if (nlayers(y) > 1) {
        unnormed_layer <- x_sample <- y_sample <- NULL
        normed_y <- foreach(unnormed_layer=unstack(y),
                            x_sample=iter(x_vals, by='column'),
                            y_sample=iter(y_vals, by='column'),
                            .combine='addLayer', .multicombine=TRUE, 
                            .init=raster(),
                            .packages=c('raster', 'lmodel2', 'rgdal')) %dopar% {
            model <- suppressMessages(lmodel2(x_sample ~ y_sample, nperm=0))
            model <- model$regression.results[model$regression.results[, Method] == method, ]
            names(model) <- gsub(^ *, , names(model))
            normed_layer <- model$Slope * unnormed_layer + model$Intercept
        }
    } else {
        model <- suppressMessages(lmodel2(x_vals ~ y_vals, nperm=0))
        model <- model$regression.results[model$regression.results[, Method] == method, ]
        names(model) <- gsub(^ *, , names(model))
        normed_y <- model$Slope * y + model$Intercept
    }
    if (!missing(msk)) {
        # Copy masked values back into the output raster
        normed_y[msk] <- y[msk]
    }
    #dataType(normed_y) <- orig_datatype
    return(normed_y)
}
  #+END_SRC
* overlay_poly.R
 #+BEGIN_SRC R 
#' Make plot of image with overlaid polygon
#'
#' Useful for quick plots showing overlap between an area of interest (AOI) and 
#' a satellite image.
#'
#' @export
#' @importFrom grid unit
#' @import ggplot2
#' @param x image as a \code{Raster*} object
#' @param y polygon to overlay, as \code{SpatialPolygonDataFrame}
#' @param out filename for output image. The extension of this file will 
#' determine the output file format (png, pdf, etc.).
#' @param title title of plot
#' @param width width (in inches) of output image
#' @param height height (in inches) of output image
#' @param dpi DPI for output image
#' @param ... additional arguments to \code{ggsave}
#' @return used for writing plot to a file
overlay_poly <- function(x, y, out, title='', width=4, height=4, dpi=150,
                         ...) {
    stop('overlay_poly is not yet supported')
    if (!(nlayers(x) %in% c(1, 3))) {
        stop('x must be a one or three band image')
    }
    y <- spTransform(y, CRS(proj4string(x)))
    maxpixels <- width*height*dpi^2
    # Below is based on gplot from rasterVis package
    nl <- nlayers(x)
    if (ncell(x)/nl > maxpixels) {
        x <- sampleRegular(x, maxpixels*nl, ext=y, asRaster=TRUE)
    }
    x <- mask(x, y)
    coords <- xyFromCell(x, seq_len(ncell(x)))
    dat <- data.frame(linear_stretch(getValues(x)))
    dat <- dat[complete.cases(dat), ]
    coords <- coords[complete.cases(dat), ]
    if (nlayers(x) == 3) {
        dat <- rgb(dat)
    }
    dat <- data.frame(coords, color=dat)
    color=long=lat=NULL  # fix for R CMD CHECK
    theme_set(theme_bw(base_size=8))
    p <- ggplot(dat) +
        geom_raster(aes(x, y, fill=color)) + coord_fixed() +
        scale_fill_identity() +
        labs(title=title) +
        theme(axis.text.x=element_blank(), axis.text.y=element_blank(),
              axis.title.x=element_blank(), axis.title.y=element_blank(),
              panel.background=element_blank(), panel.border=element_blank(),
              panel.grid.major=element_blank(), panel.grid.minor=element_blank(),
              plot.background=element_blank(), axis.ticks=element_blank(),
              plot.margin=unit(c(.1, .1, .1, .1), 'cm'))
    p
    ggsave(out, width=width, height=height, dpi=dpi, ...)
}
  #+END_SRC
* pixel_data.R
 #+BEGIN_SRC R 
#' A class for representing training or testing data
#'
#' Used to represent training data for a machine learning classifier for image 
#' classificaion, or testing data used for testing a classification.
#'
#' @exportClass pixel_data
#' @rdname pixel_data-class
#' @aliases pixel_data
#' @slot x a \code{data.frame} of independent variables (usually pixel values)
#' @slot y a \code{data.frame} of the dependent variable (usually land cover 
#' classes)
#' @slot pixel_src a data.frame used to link pixels in \code{x} and \code{y} to 
#' an input polygon
#' @slot training_flag a binary vector of length equal to \code{nrow(x)} 
#' indicating each row in x should be used in training (TRUE) or in testing 
#' (FALSE)
#' @slot polys a \code{SpatialPolygonsDataFrame} of the polygons used to choose 
#' the pixels in \code{x} and \code{y}.
#' @import methods
#' @importFrom sp SpatialPolygonsDataFrame
setClass('pixel_data', slots=c(x='data.frame', y='factor', 
                               pixel_src='data.frame', training_flag='logical', 
                               polys='SpatialPolygonsDataFrame')
)
#' @export
#' @method summary pixel_data
summary.pixel_data <- function(object, ...) {
    obj = list()
    obj[['class']] <- class(object)
    obj[['n_classes']] <- nlevels(object)
    obj[['n_sources']] <- length(unique(object@polys$src))
    obj[['n_polys']] <- nrow(object@polys)
    obj[['n_pixels']] <- nrow(object@x)
    training_df <- data.frame(y=object@y,
                              pixel_src=src_name(object), 
                              training_flag=object@training_flag)
    y=pixel_src=training_flag=NULL # Keep R CMD CHECK happy
    class_stats <- summarize(group_by(training_df, y),
                             n_polys=length(unique(pixel_src)),
                             n_train_pixels=sum(training_flag),
                             n_test_pixels=sum(!training_flag),
                             train_frac=round(sum(training_flag) / 
                                              length(training_flag), 2))
    names(class_stats)[names(class_stats) == 'y'] <- 'class'
    obj[['class_stats']]  <- class_stats
    obj[['n_training']] <- sum(object@training_flag == TRUE)
    obj[['n_testing']] <- sum(object@training_flag == FALSE)
    obj[['training_frac']] <- sum(object@training_flag == TRUE) / length(object@training_flag)
    class(obj) <- 'summary.pixel_data'
    obj
}
#' @export
#' @method print summary.pixel_data
print.summary.pixel_data <- function(x, ...) {
    cat(paste('Object of class ', x[['class']], '\n', sep = ''))
    cat('\n')
    cat(paste('Number of classes:\t', x[['n_classes']], '\n', sep=''))
    cat(paste('Number of polygons:\t', x[['n_polys']], '\n', sep=''))
    cat(paste('Number of pixels:\t', x[['n_pixels']], '\n', sep=''))
    cat(paste('Number of sources:\t', x[['n_sources']], '\n', sep=''))
    cat('\n')
    cat('Training data statistics:\n')
    print(x[['class_stats']])
    cat('\n')
    cat(paste('Number of training samples:\t', x[['n_training']], '\n', sep=''))
    cat(paste('Number of testing samples:\t', x[['n_testing']], '\n', sep=''))
    cat(paste('Training fraction:\t\t', round(x[['training_frac']], 2), '\n', sep=''))
    invisible(x)
}
#' @export
#' @method length pixel_data
length.pixel_data <- function(x) {
    return(length(x@y))
}
#' @export
#' @method levels pixel_data
levels.pixel_data <- function(x) {
    return(levels(x@y))
}
#' @export
#' @method print pixel_data
print.pixel_data <- function(x, ...) {
    print(summary(x, ...))
}
#' @export
#' @importFrom maptools spRbind
#' @method rbind pixel_data
rbind.pixel_data <- function(x, ...) {
    for (item in c(...)) {
        x@x <- rbind(x@x, item@x)
        x@y <- factor(c(as.character(x@y), as.character(item@y)))
        x@pixel_src <- rbind(x@pixel_src, item@pixel_src)
        x@training_flag <- c(x@training_flag, item@training_flag)
        if (any(row.names(x@polys) %in% row.names(item@polys)))
            stop('training polygon IDs are not unique - are src_names unique?')
        x@polys <- spRbind(x@polys, item@polys)
    }
    return(x)
}
#' Extract part of pixel_data class
#'
#' @method [ pixel_data
#' @rdname extract-methods
#' @param x a \code{pixel_data} object
#' @param i a class or list of classes to extract
#' @param j unused
#' @param ... additional arguments (none implemented)
setMethod([, signature(x=pixel_data, i='character', j=ANY),
function(x, i, j, ...) {
    if (!(i %in% levels(x@y))) {
        stop(paste0('', i, '', ' is not a class in this pixel_data object'))
    }
    sel_rows <- x@y %in% i
    used_polys <- which(paste(x@polys@data$src, x@polys@data$ID) %in% 
                        with(x@pixel_src[sel_rows, ], paste(src, ID)))
    initialize(x, x=x@x[sel_rows, ], y=x@y[sel_rows], 
               pixel_src=x@pixel_src[sel_rows, ], 
               training_flag=x@training_flag[sel_rows], 
               polys=x@polys[used_polys, ])
})
setMethod(show, signature(object=pixel_data), function(object) 
          print(object))
#' Subsample a pixel_data object
#'
#' @export subsample
#' @param x a \code{pixel_data} object
#' @param size either 1) a number from 0 to 1, indicating \code{size} is the 
#' fraction of pixels to sample, or 2) a number greater than 1, in which case 
#' \code{size} is the number of pixels to sample. Size applies per strata, if 
#' stratification is chosen.
#' @param strata whether to draw samples from within individual classes, nested 
#' within source polygons (\code{strata='sources'}), or from within individual 
#' classes alone (\code{strata='classes'})
#' @param type whether to subsample training data (\code{type='training'}) or 
#' testing data (\code{type='testing'}). Whichever type is chosen, the other 
#' type will be left untouched (for example, if \code{type='testing'}, the 
#' training data will not be changed).
#' @param flag whether to swap training flag on sampled data (for example, flag 
#' sampled training data as testing data, if \code{flag=TRUE} and 
#' \code{type='training'}) or remove sampled data from dataset entirely 
#' (\code{flag=FALSE}).
#' @param classes specifies which classes to sample, defaults to all classes in 
#' \code{x}
#' @rdname subsample
#' @aliases subsample,pixel_data-method
setGeneric(subsample, function(x, size, strata=sources, type=training, 
                                 flag=TRUE, classes=levels(x@y))
    standardGeneric(subsample)
)
#' @rdname subsample
#' @aliases subsample,pixel_data,numeric-method
#' @importFrom dplyr group_by sample_frac
setMethod(subsample, signature(x=pixel_data, size=numeric),
function(x, size, strata, type, flag, classes) {
    row_IDs <- data.frame(y=x@y,
                          pixel_src=paste(x@pixel_src$src, x@pixel_src$ID),
                          row_num=seq(1, length(x@y)))
    stopifnot(size > 0)
    stopifnot(strata %in% c(sources, classes))
    stopifnot(type %in% c(training, testing))
    if (type == training) {
        row_IDs <- row_IDs[x@training_flag, ]
    } else {
        row_IDs <- row_IDs[!x@training_flag, ]
    }
    row_IDs <- row_IDs[row_IDs$y %in% classes, ]
    stopifnot(nrow(row_IDs) > 1)
    if (strata == sources) {
        y=pixel_src=NULL
        row_IDs <- group_by(row_IDs, y, pixel_src)
    } else if (strata == classes) {
        row_IDs <- group_by(row_IDs, y)
    }
    if (size < 1) {
        samp_rows <- dplyr:::sample_frac.grouped_df(row_IDs, size)$row_num
    } else {
        samp_rows <- dplyr:::sample_n.grouped_df(row_IDs, size)$row_num
    }
    if (flag) {
        if (type == 'testing') {
            x@training_flag[samp_rows] <- TRUE
        } else if (type == 'training') {
            x@training_flag[samp_rows] <- FALSE
        }
    } else {
        x@x <- x@x[samp_rows, ]
        x@y <- x@y[samp_rows]
        x@training_flag <- x@training_flag[samp_rows]
        x@pixel_src <- x@pixel_src[samp_rows, ]
    }
    return(x)
})
#' Get or set training_flag for a pixel_data object
#'
#' @export training_flag
#' @param x a \code{pixel_data} object
#' @param classes specifies a subset of classes in \code{x}
#' @aliases training_flag,pixel_data-method
setGeneric(training_flag, function(x, classes=levels(x@y)) {
    standardGeneric(training_flag)
})
#' @rdname training_flag
setMethod(training_flag, signature(x=pixel_data),
function(x, classes) {
    if (identical(classes, levels(x@y))) {
        return(x@training_flag)
    } else {
        return(x@training_flag[x@y %in% classes])
    }
})
#' @rdname training_flag
#' @export
#' @param value training flag to assign for pixels in \code{x}
setGeneric(training_flag<-, function(x, classes=levels(x@y), value) {
    standardGeneric(training_flag<-)
})
#' @rdname training_flag
setMethod(training_flag<-, signature(x=pixel_data),
function(x, classes=levels(x@y), value) {
    if (identical(classes, levels(x@y))) {
        # More efficiently handle special case of reassigning flags for all 
        # classes in x.
        if (length(value) == 1) value <- rep(value, length(x@training_flag))
        stopifnot(length(value) == length(x@training_flag))
        x@training_flag <- value
        return(x)
    } else {
        sel_rows <- which(x@y %in% classes)
        if (length(value) == 1) value <- rep(value, length(sel_rows))
        stopifnot(length(value) == length(sel_rows))
        x@training_flag[sel_rows] <- value
        return(x)
    }
})
#' Get number of testing pixels in a pixel_data object
#'
#' @export n_test
#' @param x a \code{pixel_data} object
#' @param classes specifies a subset of classes in \code{x}
#' @aliases n_test,pixel_data-method
setGeneric(n_test, function(x, classes=levels(x@y)) {
    standardGeneric(n_test)
})
#' @rdname n_test
setMethod(n_test, signature(x=pixel_data),
function(x, classes) {
    if (identical(classes, levels(x@y))) {
        return(sum(!x@training_flag))
    } else {
        return(sum(!x@training_flag[x@y %in% classes]))
    }
})
#' Get number of training pixels in a pixel_data object
#'
#' @export n_train
#' @param x a \code{pixel_data} object
#' @param classes specifies a subset of classes in \code{x}
#' @aliases n_train,pixel_data-method
setGeneric(n_train, function(x, classes=levels(x@y)) {
    standardGeneric(n_train)
})
#' @rdname n_train
setMethod(n_train, signature(x=pixel_data),
function(x, classes) {
    if (identical(classes, levels(x@y))) {
        return(sum(x@training_flag))
    } else {
        return(sum(x@training_flag[x@y %in% classes]))
    }
})
#' Get or set src_name for a pixel_data object
#'
#' @export src_name
#' @param x a \code{pixel_data} object
#' @param classes specifies a subset of classes in \code{x}
#' @aliases src_name,pixel_data-method
setGeneric(src_name, function(x, classes=levels(x@y)) {
    standardGeneric(src_name)
})
#' @method src_name pixel_data
setMethod(src_name, signature(x=pixel_data),
function(x, classes) {
    if (identical(classes, levels(x@y))) {
        return(paste0(x@pixel_src$src, '_', x@pixel_src$ID))
    } else {
        return(with(x@pixel_src[x@y %in% classes, ], paste0(src, '_', ID)))
    }
})
#' @export
#' @rdname src_name
#' @param value a new \code{src_name} to assign for pixels in \code{x}
setGeneric(src_name<-, function(x, value) standardGeneric(src_name<-))
#' @rdname src_name
setMethod(src_name<-, signature(x=pixel_data),
function(x, value) {
    if (length(value) == 1) {
        value <- rep(value, nrow(x@polys))
    } else if (length(value) != nrow(x@polys)) {
        stop('src_name must be equal to 1 or number of polygons in x')
    }
    old_full_polyID <- paste(x@polys$src, x@polys$ID)
    x@polys$src <- value
    row.names(x@polys) <- paste0(x@polys$src, '_', x@polys$ID)
    new_full_polyID <- paste(x@polys$src, x@polys$ID)
    poly_pixel_match <- match(paste(x@pixel_src$src, x@pixel_src$ID), 
                              old_full_polyID)
    x@pixel_src$src <- x@polys$src[poly_pixel_match]
    x@pixel_src$ID <- x@polys$ID[poly_pixel_match]
    return(x)
})
#' Extract observed data for use in a classification (training or testing)
#'
#' @export
#' @param x a \code{Raster*} object from which observed data will be extracted.  
#' The data will be extracted from each layer in a \code{RasterBrick} or 
#' \code{RasterStack}.
#' @param polys a \code{SpatialPolygonsDataFrame} with polygons, each of which 
#' has been assigned to a particular class (using the \code{class_col}
#' @param class_col the name of the column containing the response variable 
#' (for example the land cover type of each pixel)
#' @param training indicator of which polygons to use in training. Can be: 1) a 
#' string giving the name of a column indicating whether each polygon is to be 
#' used in training (rows equal to 1) or in testing (rows equal to FALSE), or 
#' 2) a logical vector of length equal to length(polys), or 3) a number between 
#' 0 and 1 indicating the fraction of the polygons to be randomly selected for 
#'   use in training.
#' @param src name of this data source. Useful when gathering training 
#' data from multiple images.
#' @return a \code{link{pixel_data}} object
#' will contain the the @examples
#' set.seed(1)
#' train_data <- get_pixels(L5TSR_1986, L5TSR_1986_2001_training, class_1986, 
#'                          training=.6)
get_pixels <- function(x, polys, class_col, training=1, src='none') {
    if (projection(x) != projection(polys)) {
        stop('Coordinate systems do not match')
    }
    stopifnot(length(src) == 1)
    if (tolower(class_col) == 'id') {
        stop('class_col cannot be named ID (case insensitive)')
    }
    # Convert class_col from the name of the column to an index
    class_colnum <- grep(paste0('^', class_col, '$'), names(polys))
    if (length(class_colnum) == 0) {
        stop(paste0('', class_col, ' not found in polys'))
    }
    # This is displayed in the dataframe, and should never change
    polys$ID <- row.names(polys)
    polys$src <- src
    row.names(polys) <- paste0(polys$src, '_', polys$ID)
    if (is.character(training)) {
        # Handle case of having column name suppled as 'training'
        training_col_index <- grep(training, names(polys))
        if (length(training_col_index) == 0) {
            stop(paste0('', training,  ' column not found in x'))
        } else if (!is.logical(polys[training_col_index])) {
            stop(paste0('', training,  ' column must be a logical vector'))
        }
        training <- polys[, grep(training, names(polys))]
    } else if (is.numeric(training) && (length(training) == 1) &&
               (training >= 0) && (training <= 1)) {
        # Handle case of having fraction supplied as 'training'
        if ('training_flag' %in% names(polys)) {
            stop('training_flag column already present in polys')
        }
        if (training == 0) {
            # Handle training=0 separately to enable use of quantile function 
            # below.
            polys$training_flag <- FALSE
        } else {
            sample_strata <- function(x) {
                rand_vals <- runif(length(x))
                rand_vals <= quantile(rand_vals, training)
            }
            polys$training_flag <- unlist(tapply(polys@data$ID, 
                                                 polys@data[class_colnum], 
                                                 sample_strata))
        }
    } else if ((length(training) == length(polys)) && is.logical(training)) {
        # Handle case of having vector supplied as 'training'
        polys$training_flag <- training
    } else {
        stop('training must be a column name, vector of same length as polys, or length 1 numeric')
    }
    pixels <- extract(x, polys, small=TRUE, df=TRUE)
    poly_rows <- pixels$ID
    pixels <- pixels[!(names(pixels) == 'ID')]
    # Convert y classes to valid R variable names - if they are not valid R 
    # variable names, the classification algorithm may throw an error
    y <- factor(make.names(polys@data[poly_rows, class_colnum]))
    pixel_src <- data.frame(src=polys@data[poly_rows, ]$src,
                            ID=polys@data[poly_rows, ]$ID, 
                            stringsAsFactors=FALSE)
    return(new(pixel_data, x=pixels, y=y, pixel_src=pixel_src,
               training_flag=polys@data[poly_rows, ]$training_flag,
               polys=polys))
}
  #+END_SRC
* proj4comp.R
 #+BEGIN_SRC R 
#' Performs a rough comparison of two proj4strings to see if they match
#'
#' Compares the proj, ellps, zone (if applicable), units, and datum tags in two 
#' proj4strings to determine if two projections match. Requires proj and ellps 
#' to be present. If present, zone, units, and datum must match in both 
#' strings.
#'
#' @export
#' @importFrom rgdal CRSargs
#' @importFrom stringr str_extract
#' @param x a proj4string to compare with \code{y}
#' @param y a proj4string to compare with \code{x}
#' @return TRUE or FALSE depending on if the projections match
proj4comp <- function(x, y) {
    if (grepl('+init=', x)) {
        x <- CRSargs(CRS(x))
    }
    if (grepl('+init=', y)) {
        y <- CRSargs(CRS(y))
    }
    x_proj <- str_extract(x, '+proj=[a-zA-Z0-9]*')
    y_proj <- str_extract(y, '+proj=[a-zA-Z0-9]*')
    if (sum(is.na(c(x_proj, y_proj))) > 0) {
        stop('proj string is missing')
    } else {
        if (x_proj != y_proj) {
            return(FALSE)
        }
    }
    x_ellps <- str_extract(x, '+ellps=[a-zA-Z0-9]*')
    y_ellps <- str_extract(y, '+ellps=[a-zA-Z0-9]*')
    if (sum(is.na(c(x_ellps, y_ellps))) > 0) {
        stop('ellps string is missing')
    } else if (x_ellps != y_ellps) {
        return(FALSE)
    }
    if (grepl('utm', tolower(x_proj))) {
        x_zone <- str_extract(x, '+zone=[a-zA-Z0-9]*')
        y_zone <- str_extract(y, '+zone=[a-zA-Z0-9]*')
        if (sum(is.na(c(x_zone, y_zone))) > 0) {
            stop('utm zone must be specified for utm projections')
        } else if (x_zone != y_zone) {
            return(FALSE)
        }
        x_south <- str_extract(tolower(x), '+south')
        y_south <- str_extract(tolower(y), '+south')
        if (!is.na(x_south) || !is.na(y_south)) {
            # Get here if one or more of x_south and y_south is not NA
            if (xor(is.na(x_south), is.na(y_south)) || (x_south != y_south)) {
                # Get here if ONLY one of x_south and y_south is NA, or, if 
                # x_south and y_south are both not NA and are both not equal
                return(FALSE)
            }
        }
    }
    x_datum <- str_extract(x, '+datum=[a-zA-Z0-9]*')
    y_datum <- str_extract(y, '+datum=[a-zA-Z0-9]*')
    if ((!is.na(x_datum) && !is.na(y_datum)) & (x_datum != y_datum)) {
        return(FALSE)
    }
    
    x_units <- str_extract(x, '+units=[a-zA-Z0-9]*')
    y_units <- str_extract(y, '+units=[a-zA-Z0-9]*')
    if ((!is.na(x_units) && !is.na(y_units)) & (x_units != y_units)) {
        return(FALSE)
    }
    return(TRUE)
}
  #+END_SRC
* RcppExports.R
 #+BEGIN_SRC R 
# This file was generated by Rcpp::compileAttributes
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393
#' Threshold an image using Huang's fuzzy thresholding method.
#'
#' Implements Huang's fuzzy thresholding method. This function is called by 
#' the \code{\link{threshold}} function. It is not intended to be used 
#' directly.
#'
#' Ported to C++ by from the code in the Auto_threshold imageJ plugin by 
#' Gabriel Landini.
#'
#' See original code at:
#' http://www.mecourse.com/landinig/software/autothreshold/autothreshold.html
#'
#' @param data the input image
#' @return integer threshold value
#' @references Huang, L.-K., and M.-J. J. Wang. 1995. Image thresholding by 
#' minimizing the measures of fuzziness. Pattern recognition 28 (1):41--51.
threshold_Huang <- function(data) {
    .Call('teamlucc_threshold_Huang', PACKAGE = 'teamlucc', data)
}
#' Calculate change direction
#'
#' This code calculate the change direction from two probability images. Not
#' intended to be called directly - see \code{chg_dir}.
#'
#' @export
#' @param t1p time 1 posterior probability matrix (with pixels in rows, bands 
#' in columns)
#' @param t2p time 2 posterior probability matrix (with pixels in rows, bands 
#' in columns)
#' @return vector of change directions
#' @references Chen, J., P. Gong, C. He, R. Pu, and P. Shi. 2003.
#' Land-use/land-cover change detection using improved change-vector analysis.
#' Photogrammetric Engineering and Remote Sensing 69:369-380.
#' 
#' Chen, J., X. Chen, X. Cui, and J. Chen. 2011. Change vector analysis in 
#' posterior probability space: a new method for land cover change detection.  
#' IEEE Geoscience and Remote Sensing Letters 8:317-321.
calc_chg_dir <- function(t1p, t2p) {
    .Call('teamlucc_calc_chg_dir', PACKAGE = 'teamlucc', t1p, t2p)
}
#' Cloud fill using the algorithm developed by Xiaolin Zhu
#'
#' This function is called by the \code{\link{cloud_remove}} function. It is
#' not intended to be used directly.
#'
#' @param cloudy the cloudy image as a matrix, with pixels in columns (in 
#' column-major order) and with number of columns equal to number of bands
#' @param clear the clear image as a matrix, with pixels in columns (in 
#' column-major order) and with number of columns equal to number of bands
#' @param cloud_mask the cloud mask image as a vector (in column-major order), 
#' with clouds coded with unique integer codes starting at 1, and with areas 
#' that are clear in both images  coded as 0. Areas that are missing in the 
#' clear image, should be coded as -1.
#' @param dims the dimensions of the cloudy image as a length 3 vector: (rows, 
#' columns, bands)
#' @param num_class set the estimated number of classes in image
#' @param min_pixel the sample size of similar pixels
#' @param max_pixel the maximum sample size to search for similar pixels
#' @param cloud_nbh the range of cloud neighborhood (in pixels)
#' @param DN_min the minimum valid DN value
#' @param DN_max the maximum valid DN value
#' @param verbose whether to print detailed status messages
#' @return array with cloud filled image with dims: cols, rows, bands
#' parameter, containing the selected textures measures
#' @references Zhu, X., Gao, F., Liu, D., Chen, J., 2012. A modified
#' neighborhood similar pixel interpolator approach for removing thick clouds 
#' in Landsat images. Geoscience and Remote Sensing Letters, IEEE 9, 521--525.
cloud_fill <- function(cloudy, clear, cloud_mask, dims, num_class, min_pixel, max_pixel, cloud_nbh, DN_min, DN_max, verbose = FALSE) {
    .Call('teamlucc_cloud_fill', PACKAGE = 'teamlucc', cloudy, clear, cloud_mask, dims, num_class, min_pixel, max_pixel, cloud_nbh, DN_min, DN_max, verbose)
}
#' Cloud fill using a simple linear model approach
#'
#' This algorithm fills clouds using a simple approach in which the value of 
#' each clouded pixel is calculated using a linear model. The script
#' develops a separate linear model (with slope and intercept) for each band 
#' and each cloud. For each cloud, and each image band, the script finds all 
#' pixels clear in both the cloudy and fill images, and calculates a 
#' regression model in which pixel values in the fill image are the 
#' independent variable, and pixel values in the clouded image are the 
#' dependent variable. The script then uses this model to predict pixel values 
#' for each band in each cloud in the clouded image.
#'
#' This function is called by the \code{\link{cloud_remove}} function. It is
#' not intended to be used directly.
#'
#' @param cloudy the cloudy image as a matrix, with pixels in columns (in 
#' column-major order) and with number of columns equal to number of bands
#' @param clear the clear image as a matrix, with pixels in columns (in 
#' column-major order) and with number of columns equal to number of bands
#' @param cloud_mask the cloud mask image as a vector (in column-major order), 
#' with clouds coded with unique integer codes starting at 1, and with areas 
#' that are clear in both images  coded as 0. Areas that are missing in the 
#' clear image, should be coded as -1.
#' @param dims the dimensions of the cloudy image as a length 3 vector: (rows, 
#' columns, bands)
#' @param num_class set the estimated number of classes in image
#' @param cloud_nbh the range of cloud neighborhood (in pixels)
#' @param DN_min the minimum valid DN value
#' @param DN_max the maximum valid DN value
#' @param verbose whether to print detailed status messages
#' @return array with cloud filled image with dims: cols, rows, bands
#' parameter, containing the selected textures measures
cloud_fill_simple <- function(cloudy, clear, cloud_mask, dims, num_class, cloud_nbh, DN_min, DN_max, verbose = FALSE) {
    .Call('teamlucc_cloud_fill_simple', PACKAGE = 'teamlucc', cloudy, clear, cloud_mask, dims, num_class, cloud_nbh, DN_min, DN_max, verbose)
}
  #+END_SRC
* sample_raster.R
 #+BEGIN_SRC R 
#' Generate random sample polygons from a raster layer, optionally with 
#' stratification
#'
#' Useful for gathering training data for an image classification. With the 
#' default settings, the output polygons will be perfectly aligned with the 
#' pixels in the input raster.
#'
#' @export
#' @importFrom rgdal writeOGR
#' @param x a \code{Raster*}
#' @param size the sample size (number of sample polygons to return)
#' @param side desired length for each side of the sample polygon (units of the 
#' input \code{Raster*}, usually meters)
#' @param strata (optional) a \code{RasterLayer} of integers giving the strata 
#' of each pixel (for example, a classified image)
#' @param fields a list of fields to include in the output 
#' \code{SpatialPolygonsDataFrame} (such as a class field if you will be 
#' digitizing classes).
#' @param na.rm whether to remove pixels with NA values from the sample
#' @param exp multiplier used to draw larger initial sample to account for the 
#' loss of sample polygons lost because they contain NAs, and, for stratified 
#' sampling, to account for classes that occur very infrequently in the data.  
#' Increase this value if the final sample has fewer sample polygons than 
#' desired.
#' @return a \code{SpatialPolygonsDataFrame}
#' @examples
#' \dontrun{
#' set.seed(0)
#' L5TSR_1986_b1 <- raster(L5TSR_1986, layer=1)
#' training_polys <- sample_raster(L5TSR_1986_b1, 30,
#'                                   side=6*xres(L5TSR_1986_b1))
#' plot(L5TSR_1986_b1)
#' plot(training_polys, add=TRUE)
#' }
sample_raster <- function(x, size, strata=NULL, side=xres(x), fields=c(), 
                          na.rm=TRUE, exp=5) {
    if (!is.null(strata)) {
        stratified <- TRUE
        if (proj4string(strata) != proj4string(x)) {
            stop('x and strata must have the same coordinate system')
        }
        if (!identical(extent(strata), extent(x))) {
            stop('x and strata must have the same extent')
        }
        if (!identical(res(strata), res(x))) {
            stop('x and strata must have the same resolution')
        }
        strat_sample <- sampleStratified(strata, size, exp=exp, na.rm=na.rm)
        cell_nums <- strat_sample[, 1]
        strataids <- strat_sample[, 2]
    } else {
        stratified <- FALSE
        cell_nums <- sampleRandom(x, size, exp=exp, na.rm=na.rm)
    }
    xy <- xyFromCell(x, cell_nums)
    # Convert from cell-center coordinates to ul corner of cell coordinates
    xy[, 1] <- xy[, 1] - xres(x)/2
    xy[, 2] <- xy[, 2] + yres(x)/2
    # Coordinate order is: ll, lr, ur, ul, ll. Need to end with ll to close the 
    # polygon
    xcoords <- cbind(xy[, 1],
                     xy[, 1] + side,
                     xy[, 1] + side,
                     xy[, 1],
                     xy[, 1])
    ycoords <- cbind(xy[, 2] - side,
                     xy[, 2] - side,
                     xy[, 2],
                     xy[, 2],
                     xy[, 2] - side)
    xycoords <- array(cbind(xcoords, ycoords),
                      dim=c(nrow(xcoords), ncol(xcoords), 2))
    # TODO: Add check to ensure no polygons overlap, and warn if they fall 
    # outside raster extent
    # Function to make individual polygons for each sample area
    make_Polygon <- function(slice) {
        list(Polygon(cbind(x=slice[, 1], y=slice[, 2])))
    }
    polys <- apply(xycoords, c(1), make_Polygon)
    # Now convert the list of Polygon objects to a list of Polygons objects 
    # (notice the trailing s in Polygons)
    polys <- mapply(function(poly, ID) Polygons(poly, ID=ID),
                    polys, seq(1, length(polys)))
    Sr <- SpatialPolygons(polys, proj4string=CRS(proj4string(x)))
    if (stratified) {
        out_data <- data.frame(ID=names(Sr), strata=strataids)
    } else {
        out_data <- data.frame(ID=names(Sr))
    }
    for (field in fields) {
        out_data <- cbind(out_data, rep('', nrow(out_data)))
        names(out_data)[ncol(out_data)] <- field
    }
    # Finally, convert to a SpatialPolygonsDataFrame
    polys <- SpatialPolygonsDataFrame(Sr, data=out_data)
    if (nrow(polys) < size) {
        warning('length of polys < size. Try increasing exp.')
    }
    return(polys)
}
  #+END_SRC
* scale_raster.R
 #+BEGIN_SRC R 
#' Scales a \code{Raster*} by a power of a given integer and rounds to nearest 
#' integer
#'
#' Useful for scaling and (optionally) rounding a \code{RasterLayer} to integer 
#' so that a layer can be saved as an integer datatype such as INT1U, 
#' INT1S, INT2 or INT2S.
#'
#' This function will run in parallel if a parallel backend is registered with 
#' \code{\link{foreach}}.
#'
#' @export scale_raster
#' @import methods
#' @seealso \code{\link{dataType}}
#' @param x a \code{Raster*} object
#' @param power_of raster will be scaled using the highest possible power of 
#' this number
#' @param max_out the scaling factors will be chosen for each layer to ensure 
#' that the maximum and minimum (if minimum is negative) values of each layer 
#' do not exceed \code{max_out}
#' @param do_scaling perform the scaling and return a \code{Raster*} (if 
#' \code{do_scaling} is TRUE) or return a list of scale factors (if 
#' \code{do_scaling} is FALSE)
#' @param round_output whether to round the output to the nearest integer
#' @return a \code{Raster*} if \code{do_scaling} is TRUE, or a list of scaling 
#' factors if \code{do_scaling} is false.
setGeneric(scale_raster, function(x, power_of=10, max_out=32767, 
                                    round_output=TRUE, do_scaling=TRUE) {
    standardGeneric(scale_raster)
})
scale_layer <- function(x, power_of, max_out, round_output, do_scaling) {
    if (!x@data@haveminmax) {
        warning('no stored minimum and maximum values - running setMinMax')
        x <- setMinMax(x)
    }
    layer_max <- max(abs(c(minValue(x), maxValue(x))))
    scale_factor <- power_of ^ floor(log(max_out / layer_max, base=power_of))
    if (do_scaling) {
        x <- calc(x, function(vals, ...) {
                  vals <- vals * scale_factor
                  if (round_output) vals <- round(vals)
                  vals
                  })
        return(x)
    } else {
        return(scale_factor)
    }
}
#' @rdname scale_raster
#' @aliases scale_raster,RasterLayer,ANY-method
setMethod(scale_raster, signature(x=RasterLayer),
    function(x, power_of, max_out, round_output, do_scaling) {
        ret <- scale_layer(x, power_of, max_out, round_output, do_scaling)
        names(ret) <- names(x)
        return(ret)
    }
)
#' @import foreach
scale_stack_or_brick <- function(x, power_of, max_out, round_output, do_scaling) {
    unscaled_layer=NULL
    if (do_scaling) {
        scale_outputs <- foreach(unscaled_layer=unstack(x), 
                                 .combine='addLayer', .multicombine=TRUE, 
                                 .init=raster(), .packages=c('teamlucc'),
                                 .export=c('scale_layer')) %dopar% {
            scale_output <- scale_layer(unscaled_layer, power_of, max_out, 
                                        round_output, do_scaling)
        }
    } else {
        scale_outputs <- foreach(unscaled_layer=unstack(x), 
                                 .packages=c('raster', 'teamlucc'),
                                 .export=c('scale_layer')) %dopar% {
            scale_output <- scale_layer(unscaled_layer, power_of, max_out, 
                                        round_output, do_scaling)
        }
    }
    names(scale_outputs) <- names(x)
    return(scale_outputs)
}
#' @rdname scale_raster
#' @aliases scale_raster,RasterStack,ANY-method
setMethod(scale_raster, signature(x=RasterStack),
    function(x, power_of, max_out, round_output, do_scaling) {
        ret <- scale_stack_or_brick(x, power_of, max_out, round_output, 
                                    do_scaling)
        return(ret)
    }
)
#' @rdname scale_raster
#' @aliases scale_raster,RasterBrick,ANY-method
setMethod(scale_raster, signature(x=RasterBrick),
    function(x, power_of, max_out, round_output, do_scaling) {
        ret <- scale_stack_or_brick(x, power_of, max_out, round_output, 
                                    do_scaling)
        return(ret)
    }
)
  #+END_SRC
* simplify_polygon.R
 #+BEGIN_SRC R 
#' Count number of vertices in an sp polygon object
#'
#' @param poly_obj an sp polygon object
#' @return the number of vertices in the polygon
#' @examples
#' # TODO: Write an example.
nverts <- function(poly_obj) {
    n_verts <- sapply(poly_obj@polygons, function(y) 
                      nrow(y@Polygons[[1]]@coords))[[1]]
    if (is.null(n_verts)) {
        n_verts <- 0
    } else {
        # Need to subtract one as the starting coordinate and ending coordinate 
        # are identical, but appear twice - at beginning and at end of list.
        n_verts <- n_verts - 1
    }
    return(n_verts)
}
#' Simplify a polygon to contain less than a certain number of vertices
#'
#' Useful for simplifying area of interest (AOI) polygons for use with online 
#' data portals (like USGS EarthExplorer) that limit the number of vertices 
#' allowed in uploaded AOI shapefiles.
#'
#' @export
#' @importFrom rgeos gSimplify
#' @param poly_obj a polygon as an sp object
#' @param max_vertices the maximum number of vertices to allow in the 
#' simplified polygon
#' @param maxit the maximum number of iterations to use to simplify the polygon
#' @param multiplier a number used to increase the tolerance for each 
#' subsequent iteration, if the number of vertices in the simplified polygon is 
#' not less than /code{max_vertices}
#' @param initial_tolerance initial value for tolerance used to remove vertices 
#' from polygon.  If set to the default option, dynamic, the code will 
#' automatically set the \code{initial_tolerance} to .01 * the length of the 
#' diagonal of the bounding box of the polygon. \code{initial_tolerance} can 
#' also be set to an arbitrary value, in the same units as the polygon object.
#' @return polygon with less than \code{max_vertices} vertices
#' @examples
#' # TODO: add an example
simplify_polygon <- function(poly_obj, max_vertices, maxit=100, 
                             multiplier=1.05, initial_tolerance='dynamic') {
    if (class(poly_obj) == 'SpatialPolygonsDataFrame') {
        poly_data <- poly_obj@data
    } else {
        poly_data <- NULL
    }
    n_parts <- sapply(poly_obj@polygons, function(x) length(x))
    if (length(n_parts) > 1)
        stop('poly_obj contains more than one polygon')
    else if (n_parts > 1)
        stop('poly_obj polygon is a multipart polygon')
    if (initial_tolerance == 'dynamic') {
        # Set the initial tolerance as the extent / 100
        ext <- extent(poly_obj)
        diag_seg_length <- sqrt((ext@xmax - ext@xmin)**2 +
                                (ext@ymax - ext@ymin)**2)
        tolerance <- diag_seg_length / 100
    } else {
        tolerance <- initial_tolerance
    }
    # Iterate, increasing tolerance, until polygon has less than maxpts 
    # vertices.
    n_verts <- nverts(poly_obj)
    n <- 0
    while ((n_verts > 0) && (n < maxit) && (n_verts > max_vertices)) {
        poly_obj <- gSimplify(poly_obj, tol=tolerance)
        n_verts <- nverts(poly_obj)
        tolerance <- tolerance * multiplier 
        n <- n + 1
    }
    if (n == maxit)
        warning(paste('Reached maximum iterations (', maxit, ')', sep=''))
    if (n_verts == 0)
        warning(paste('Simplified polygon has no vertices.'))
    else if (n_verts <= 2)
        warning(paste('Simplified polygon has only', n_verts, 'vertices.'))
    if (!is.null(poly_data)) {
        poly_obj <- SpatialPolygonsDataFrame(poly_obj, data=poly_data)
    }
    return(poly_obj)
}
  #+END_SRC
* split_classes.R
 #+BEGIN_SRC R 
#' Split classes in a training dataset using normal mixture modeling
#'
#' This function can be used to aid in classifying spectrally diverse classes 
#' by splitting the input classes into subclasses using a clustering algorithm.  
#' After classification, these subclasses are merged back into their original 
#' parent classes. For example, the training data for an agriculture class 
#' might have both fallow and planted fields in the training data, or fields 
#' planted with different crops that are spectrally dissimilar.  This function 
#' can be used to automatically split the agriculture class into a number of 
#' subclasses.  The classifier is then run on this larger set of classes, and 
#' following classification, these subclasses can all be merged together into a 
#' single overall agriculture class.
#'
#' @export
#' @importFrom mclust Mclust
#' @param train_data a \code{link{pixel_data}} object
#' @param split_levels (optional) a list giving the names of the levels to 
#' split. If missing, all levels will be split.
#' @param verbose whether to report status while running
split_classes <- function(train_data, split_levels, verbose=FALSE) {
    y_reclass <- vector('numeric', nrow(train_data@x))
    if (missing(split_levels)) {
        split_levels <- levels(train_data)
    }
    for (level in split_levels) {
        level_ind <- train_data@y == level
        model <- Mclust(train_data@x[level_ind, ])
        y_reclass[level_ind]  <- paste(train_data@y[level_ind], 
                                       model$classification, sep='_clust')
        if (verbose) print(paste(level, 'split into', model$g, 'classes'))
    }
    y <- factor(y_reclass)
    reclass_mat <- data.frame(split_name=levels(y))
    reclass_mat$split_id <- as.numeric(reclass_mat$split_name)
    reclass_mat$name <- gsub('_clust[0-9]*$', '', reclass_mat$split_name)
    reclass_mat$id <- match(reclass_mat$name, unique(reclass_mat$name))
    return(list(reclass_mat=reclass_mat, y=factor(y_reclass)))
}
  #+END_SRC
* SVIs.R
 #+BEGIN_SRC R 
#' Calculates the Normalized Difference Vegetation Index (NDVI)
#'
#' Calculates the NDVI, defined as: (nir - red) / (nir + red).
#'
#' @export NDVI
#' @param red red
#' @param nir near-infrared
#' @param ... additional arguments as for \code{\link{writeRaster}}
#' @examples
#' NDVI_img <- NDVI(red=raster(L5TSR_1986, layer=3), nir=raster(L5TSR_1986, 
#'                 layer=4))
#' plot(NDVI_img)
setGeneric(NDVI, function(red, nir, ...) {
    standardGeneric(NDVI)
})
NDVI_calc <- function(red, nir) {
    v <- (nir - red) / (nir + red)
    return(v)
}
#' @rdname NDVI
#' @aliases NDVI,numeric,numeric,numeric-method
setMethod(NDVI, signature(red=numeric, nir=numeric),
    function(red, nir) {
        NDVI_calc(red, nir)
    }
)
#' @rdname NDVI
#' @aliases NDVI,matrix,matrix,matrix-method
setMethod(NDVI, signature(red=matrix, nir=matrix),
    function(red, nir) {
        ret <- NDVI_calc(red, nir)
        return(ret)
    }
)
#' @rdname NDVI
#' @aliases NDVI,RasterLayer,RasterLayer,RasterLayer-method
#' @importFrom raster overlay
setMethod(NDVI, signature(red=RasterLayer, nir=RasterLayer),
    function(red, nir, ...) {
        ret  <- overlay(red, nir, fun=function(red, nir) {
                NDVI_calc(red, nir)
            } , ...)
        return(ret)
    }
)
#' Calculates the Enhanced Vegetation Index (EVI)
#'
#' @export EVI
#' @importFrom raster overlay
#' @param blue blue
#' @param red red
#' @param nir near-infrared
#' @param ... additional arguments as for \code{\link{writeRaster}}
#' @references Huete, A. R., HuiQing Liu, and W. J. D. van Leeuwen. 1997. The
#' use of vegetation indices in forested regions: issues of linearity and
#' saturation. Pages 1966-1968 vol.4 Geoscience and Remote Sensing, 1997.
#' IGARSS '97. Remote Sensing - A Scientific Vision for Sustainable
#' Development., 1997 IEEE International.
#' @examples
#' EVI_img <- EVI(blue=raster(L5TSR_1986, layer=1), red=raster(L5TSR_1986, layer=3), 
#'                nir=raster(L5TSR_1986, layer=4))
#' plot(EVI_img)
setGeneric(EVI, function(blue, red, nir, ...) {
    standardGeneric(EVI)
})
EVI_calc <- function(blue, red, nir) {
    v <- (2.5*(nir - red)) / (1 + nir + 6*red - 7.5*blue)
    return(v)
}
#' @rdname EVI
#' @aliases EVI,numeric,numeric,numeric-method
setMethod(EVI, signature(blue=numeric, red=numeric, nir=numeric),
    function(blue, red, nir) {
        EVI_calc(blue, red, nir)
    }
)
#' @rdname EVI
#' @aliases EVI,numeric,numeric,numeric-method
setMethod(EVI, signature(blue=matrix, red=matrix, nir=matrix),
    function(blue, red, nir) {
        ret <- EVI_calc(blue, red, nir)
        return(ret)
    }
)
#' @rdname EVI
#' @aliases EVI,RasterLayer,RasterLayer,RasterLayer-method
#' @importFrom raster overlay
setMethod(EVI, signature(blue=RasterLayer, red=RasterLayer, 
                           nir=RasterLayer),
    function(blue, red, nir, ...) {
        ret <- overlay(blue, red, nir, fun=function(blue, red, nir) {
                EVI_calc(blue, red, nir)
            }, ...)
        return(ret)
    }
)
#' Calculates the Modified Soil-Adjusted Vegetation Index (MSAVI)
#'
#' Note that this avoids the need for calculating L by using the equation for 
#' MSAVI2 from Qi et al. (1994).
#'
#' @export MSAVI2
#' @importFrom raster overlay
#' @param red red
#' @param nir near-infrared
#' @param ... additional arguments as for \code{\link{writeRaster}}
#' @references Qi, J., A. Chehbouni, A. R. Huete, Y. H. Kerr, and S.
#' Sorooshian. 1994. A modified soil adjusted vegetation index. Remote Sensing
#' of Environment 48:119-126.
#' @examples
#' MSAVI_img <- MSAVI2(red=raster(L5TSR_1986, layer=3),
#'                     nir=raster(L5TSR_1986, layer=4))
#' plot(MSAVI_img)
setGeneric(MSAVI2, function(red, nir, ...) {
    standardGeneric(MSAVI2)
})
MSAVI2_calc <- function(red, nir) {
    v <- (2*nir + 1 - sqrt((2*nir + 1)^2 - 8*(nir - red)))/2
    return(v)
}
#' @rdname MSAVI2
#' @aliases MSAVI2,numeric,numeric,numeric-method
setMethod(MSAVI2, signature(red=numeric, nir=numeric),
    function(red, nir) {
        MSAVI2_calc(red, nir)
    }
)
#' @rdname MSAVI2
#' @aliases MSAVI2,matrix,matrix,matrix-method
setMethod(MSAVI2, signature(red=matrix, nir=matrix),
    function(red, nir) {
        ret <- MSAVI2_calc(red, nir)
        return(ret)
    }
)
#' @rdname MSAVI2
#' @aliases MSAVI2,RasterLayer,RasterLayer,RasterLayer-method
#' @importFrom raster overlay
setMethod(MSAVI2, signature(red=RasterLayer, nir=RasterLayer),
    function(red, nir, ...) {
        ret  <- overlay(red, nir, fun=function(red, nir) {
                MSAVI2_calc(red, nir)
            } , ...)
        return(ret)
    }
)
#' Calculates the Atmospherically Resistant Vegetation Index (ARVI)
#'
#' @export ARVI
#' @importFrom raster overlay
#' @param blue blue
#' @param red red
#' @param nir near-infrared
#' @param ... additional arguments as for \code{\link{writeRaster}}
#' @references Kaufman, Y. J., and D. Tanre. 1996. Strategy for direct and
#' indirect methods for correcting the aerosol effect on remote sensing: from
#' AVHRR to EOS-MODIS. Remote Sensing of Environment:65-79.
#' @examples
#' ARVI_img <- ARVI(blue=raster(L5TSR_1986, layer=1), red=raster(L5TSR_1986, 
#'                  layer=3), nir=raster(L5TSR_1986, layer=4))
#' plot(ARVI_img)
setGeneric(ARVI, function(blue, red, nir, ...) {
    standardGeneric(ARVI)
})
ARVI_calc <- function(blue, red, nir) {
    v <- (nir - 2*red - blue) / (nir + 2*red - blue)
    return(v)
}
#' @rdname ARVI
#' @aliases ARVI,numeric,numeric,numeric-method
setMethod(ARVI, signature(blue=numeric, red=numeric, nir=numeric),
    function(blue, red, nir) {
        ARVI_calc(blue, red, nir)
    }
)
#' @rdname ARVI
#' @aliases ARVI,matrix,matrix,matrix-method
setMethod(ARVI, signature(blue=matrix, red=matrix, nir=matrix),
    function(blue, red, nir) {
        ret <- ARVI_calc(blue, red, nir)
        return(ret)
    }
)
#' @rdname ARVI
#' @aliases ARVI,RasterLayer,RasterLayer,RasterLayer-method
#' @importFrom raster overlay
setMethod(ARVI, signature(blue=RasterLayer, red=RasterLayer, 
                            nir=RasterLayer),
    function(blue, red, nir, ...) {
        ret <- overlay(blue, red, nir, fun=function(blue, red, nir) {
                ARVI_calc(blue, red, nir)
            }, ...)
        return(ret)
    }
)
  #+END_SRC
* teamlucc-package.R
 #+BEGIN_SRC R 
#' TEAM land use and cover change data processing toolkit
#'
#' teamlucc is a set of routines to support analyzing land use and cover change 
#' (LUCC) in R. The package was designed to support analyzing LUCC in the Zone 
#' of Interaction (ZOIs) of monitoring sites in the Tropical Ecology Assessment 
#' and Monitoring (TEAM) Network.
#'
#' @name teamlucc-package
#' @aliases teamlucc
#' @docType package
#' @title TEAM Remote Sensing Processing Tools
#' @author Alex Zvoleff, \email{azvoleff@@conservation.org}
#' @keywords package
#' @useDynLib teamlucc
NULL
#' Landsat 5 Surface Reflectance Image from February 6, 1986 (path 15, row 53)
#' 
#' Portion of Landsat 5 Surface Reflectance image from the Landsat Climate Data 
#' Record archive. This subset of the image includes only bands 1-4, and pixel 
#' values have been scaled by 10000 and rounded off.
#'
#' @docType data
#' @name L5TSR_1986
#' @seealso L5TSR_2001
NULL
#' Landsat 5 Surface Reflectance Image from January 14, 2001 (path 15, row 53)
#' 
#' Portion of Landsat 5 Surface Reflectance image from the Landsat Climate Data 
#' Record archive. This subset of the image includes only bands 1-4, and pixel 
#' values have been scaled by 10000 and rounded off.
#'
#' @docType data
#' @name L5TSR_2001
#' @seealso L5TSR_1986
NULL
#' Subset of ASTER Digital Elevation Model V002
#' 
#' @docType data
#' @name ASTER_V002_WEST
#' @seealso ASTER_V002_EAST
NULL
#' Training polygons for 1986 and 2001 Landsat 5 Surface Reflectance images
#' 
#' Polygons digitized from 1986 and 2001 Landsat 5 Surface Reflectance image 
#' from the Landsat Climate Data Record archive. The training polygons can be 
#' used for testing classification algorithms.
#'
#' There are three columns in the dataset. class_1986 is the cover class for 
#' the pixels in the polygon from the 1986 image. class_2001 is the cover class 
#' for the pixels in the polygon from the 2001 image.
#'
#' @docType data
#' @name L5TSR_1986_2001_training
NULL
#' Subset of ASTER Digital Elevation Model V002
#' 
#' @docType data
#' @name ASTER_V002_WEST
#' @seealso ASTER_V002_EAST
NULL
#' Subset of ASTER Digital Elevation Model V002
#' 
#' @docType data
#' @name ASTER_V002_EAST
#' @seealso ASTER_V002_WEST
NULL
  #+END_SRC
* threshold.R
 #+BEGIN_SRC R 
#' Automatically determine value for image thresholding
#'
#' The only method currently implemented is Huang's fuzzy thresholding method.  
#' The code for Huang's method was ported to C++ for \code{teamlucc} from the 
#' code in the Auto_threshold imageJ plugin by Gabriel Landini. See original 
#' code at:
#' http://www.mecourse.com/landinig/software/autothreshold/autothreshold.html
#'
#' This function will run in parallel if a parallel backend is registered with 
#' \code{\link{foreach}}.
#'
#' @export
#' @import foreach
#' @importFrom iterators iter
#' @param x the input image, as a matrix or raster
#' @param method the thresholding method. Currently only huang is 
#' implemented.
#' @param n_bin number of bins to use when calculating histogram
#' @param maxpixels maximum number of pixels size to use when calculating 
#' histogram
#' @return integer threshold value
#' @references Huang, L.-K., and M.-J. J. Wang. 1995. Image thresholding by 
#' minimizing the measures of fuzziness. Pattern recognition 28 (1):41--51.
threshold <- function(x, method=huang, n_bin=1000, maxpixels=5e5) {
    stopifnot(method %in% c(huang))
    if (ncell(x) > maxpixels) {
        x <- sampleRegular(x, maxpixels, useGDAL=TRUE, asRaster=TRUE)
    }
    x <- stack(x)
    x <- setMinMax(x)
    mins <- minValue(x)
    maxs <- maxValue(x)
    bys <- (maxs - mins) / (n_bin)
    bandnum=minval=maxval=NULL
    thresholds <- foreach(bandnum=iter(1:nlayers(x)), minval=iter(mins), 
                          maxval=iter(maxs), by=iter(bys),
                          .packages=c('teamlucc'),
                          .combine=c) %dopar% {
        image_hist <- hist(x[[bandnum]], breaks=seq(minval, maxval+by, by=by), 
                           plot=FALSE, maxpixels=maxpixels)
        threshold_index <- threshold_Huang(image_hist$counts)
        image_hist$breaks[threshold_index]
    }
    return(thresholds)
}
  #+END_SRC
* topocorr_samp.R
 #+BEGIN_SRC R 
.calc_IL_vector <- function(slope, aspect, sunzenith, sunazimuth, IL.epsilon) {
    IL <- cos(slope) * cos(sunzenith) + sin(slope) * sin(sunzenith) * 
        cos(sunazimuth - aspect)
    IL[IL == 0] <- IL.epsilon
    return(IL)
}
.calc_IL <- function(slope, aspect, sunzenith, sunazimuth, IL.epsilon) {
    overlay(slope, aspect,
            fun=function(slope_vals, aspect_vals) {
                .calc_IL_vector(slope_vals, aspect_vals, sunzenith, sunazimuth, 
                               IL.epsilon)
            })
}
#' Topographic correction for satellite imagery
#'
#' Perform topographic correction using a number of different methods. This 
#' code is modified from the code in the \code{landsat} package by Sarah 
#' Goslee.  This version of the code has been altered from the \code{landsat} 
#' version to allow the option of using a sample of pixels for calculation of k 
#' in the Minnaert correction (useful when dealing with large images).
#' 
#' See the help page for \code{topocorr} in the \code{landsat} package for 
#' details on the parameters.
#'
#' @export
#' @param x image as a \code{RasterLayer}
#' @param slope the slope in radians as a \code{RasterLayer}
#' @param aspect the aspect in radians as a \code{RasterLayer}
#' @param sunelev sun elevation in degrees
#' @param sunazimuth sun azimuth in degrees
#' @param method the method to use for the topographic correction:
#' cosine, improvedcosine, minnaert, minslope, ccorrection, gamma, SCS, or 
#' illumination
#' @param na.value the value used to code no data values
#' @param IL.epsilon a small amount to add to calculated illumination values 
#' that are equal to zero to avoid division by zero resulting in Inf values
#' @param sampleindices (optional) row-major indices of sample pixels to use in 
#' regression models used for some topographic correction methods (like 
#' Minnaert). Useful when handling very large images. See
#' \code{\link{gridsample}} for one method of calculating these indices.
#' @param DN_min minimum allowable pixel value after correction (values less 
#' than \code{DN_min} are set to NA)
#' @param DN_max maximum allowable pixel value after correction (values less 
#' than \code{DN_max} are set to NA)
#' @return RasterBrick with two layers: 'slope' and 'aspect'
#' @author Sarah Goslee and Alex Zvoleff
#' @references
#' Sarah Goslee. Analyzing Remote Sensing Data in {R}: The {landsat} Package.  
#' Journal of Statistical Software, 2011, 43:4, pg 1--25.  
#' http://www.jstatsoft.org/v43/i04/
#' @examples
#' #TODO: add examples
topocorr_samp <- function(x, slope, aspect, sunelev, sunazimuth, method=cosine, 
                          na.value=NA, IL.epsilon=0.000001,
                          sampleindices=NULL, DN_min=NULL, DN_max=NULL) {
    # some inputs are in degrees, but we need radians
    stopifnot((sunelev >= 0) & (sunelev <= 90))
    stopifnot((sunazimuth >= 0) & (sunazimuth <= 360))
    sunzenith <- (pi/180) * (90 - sunelev)
    sunazimuth <- (pi/180) * sunazimuth
    x[x == na.value] <- NA
    IL <- .calc_IL(slope, aspect, sunzenith, sunazimuth, IL.epsilon)
    rm(aspect, sunazimuth)
    if (!is.null(sampleindices) && !(method %in% c('minnaert', 'minslope', 
                                                   'ccorrection'))) {
        warning(paste0('sampleindices are not used when method is ', method,
                       '. Ignoring sampleindices.'))
    }
    METHODS <- c(cosine, improvedcosine, minnaert, minslope, 
                 ccorrection, gamma, SCS, illumination)
    method <- pmatch(method, METHODS)
    if (is.na(method)) 
        stop(invalid method)
    if (method == -1) 
        stop(ambiguous method)
    if(method == 1){
        ## Cosine method
        xout <- x * (cos(sunzenith)/IL)
    } else if(method == 2) {
        ## Improved cosine method
        ILmean <- cellStats(IL, stat='mean', na.rm=TRUE)
        xout <- x + (x * (ILmean - IL)/ILmean)
    } else if(method == 3) {
        ## Minnaert
        ## K is between 0 and 1
        ## only use points with greater than 5% slope
        targetslope <- atan(.05)
        if(all(x[slope >= targetslope] < 0, na.rm=TRUE)) {
            K <- 1
        } else {
            if (!is.null(sampleindices)) {
                K <- data.frame(y=x[slope >= targetslope][sampleindices],
                                x=IL[slope >= targetslope][sampleindices]/cos(sunzenith))
            } else {
                K <- data.frame(y=x[slope >= targetslope],
                                x=IL[slope >= targetslope]/cos(sunzenith))
            }
            # IL can be <=0 under certain conditions
            # but that makes it impossible to take log10 so remove those 
            # elements
            K <- K[!apply(K, 1, function(x)any(is.na(x))),]
            K <- K[K$x > 0, ]
            K <- K[K$y > 0, ]
            K <- lm(log10(K$y) ~ log10(K$x))
            K <- coefficients(K)[[2]] # need slope
            if(K > 1) K <- 1
            if(K < 0) K <- 0
        }
        xout <- x * (cos(sunzenith)/IL) ^ K
    } else if(method == 4) {
        ## Minnaert with slope
        ## K is between 0 and 1
        ## only use points with greater than 5% slope
        targetslope <- atan(.05)
        if(all(x[slope >= targetslope] < 0, na.rm=TRUE)) {
            K <- 1
        } else {
            if (!is.null(sampleindices)) {
                K <- data.frame(y=x[slope >= targetslope][sampleindices],
                                x=IL[slope >= targetslope][sampleindices] / cos(sunzenith))
            } else {
                K <- data.frame(y=x[slope >= targetslope], 
                                x=IL[slope >= targetslope]/cos(sunzenith))
            }
            # IL can be <=0 under certain conditions
            # but that makes it impossible to take log10 so remove those elements
            K <- K[!apply(K, 1, function(x) any(is.na(x))),]
            K <- K[K$x > 0, ]
            K <- K[K$y > 0, ]
            K <- lm(log10(K$y) ~ log10(K$x))
            K <- coefficients(K)[[2]] # need slope
            if(K > 1) K <- 1
            if(K < 0) K <- 0
        }
        xout <- x * cos(slope) * (cos(sunzenith) / (IL * cos(slope))) ^ K
    } else if(method == 5) {
        ## C correction
        if (!is.null(sampleindices)) {
            band.lm <- lm(x[sampleindices] ~ IL[sampleindices])
        } else {
            band.lm <- lm(getValues(x) ~ getValues(IL))
        }
        C <- coefficients(band.lm)[[1]]/coefficients(band.lm)[[2]]
        xout <- x * (cos(sunzenith) + C) / (IL + C)
    } else if(method == 6) {
        ## Gamma
        ## assumes zenith viewing angle
        viewterrain <- pi/2 - slope
        xout <- x * (cos(sunzenith) + cos(pi / 2)) / (IL + cos(viewterrain))
    } else if(method == 7) {
        ## SCS method from GZ2009
        xout <- x * (cos(sunzenith) * cos(slope))/IL
    } else if(method == 8) {
        ## illumination only
        xout <- IL
    }
    ## if slope is zero, reflectance does not change
    if(method != 8) 
        xout[slope == 0 & !is.na(slope)] <- x[slope == 0 & !is.na(slope)]
    if ((!is.null(DN_min)) || (!is.null(DN_max))) {
        xout <- calc(xout, fun=function(vals) {
                        if (!is.null(DN_min)) vals[vals < DN_min] <- NA
                        if (!is.null(DN_max)) vals[vals > DN_max] <- NA
                        return(vals)
                     })
    }
    return(xout)
}
  #+END_SRC
* topographic_corr.R
 #+BEGIN_SRC R 
#' Topographically correct a raster
#'
#' Performs topographic correction using code based on \code{topocorr} from the 
#' \code{landsat} package by Sarah Goslee. The code in this package has been 
#' modifed from \code{topocorr} to allow using a subsample of the image for 
#' Minnaert k calculations, and to provide the option of running the 
#' topographic correction in parallel using \code{foreach}.
#'
#' This function will run in parallel if a parallel backend is registered with 
#' \code{\link{foreach}}.
#'
#' @export
#' @import foreach
#' @param x an image to correct
#' @param slopeaspect a \code{RasterBrick} or \code{RasterStack} with two 
#' layers.  The first layer should be the slope, the second layer should be 
#' the aspect. The slope and aspect are defined as in \code{terrain} in the 
#' \code{raster} package, and both should be in radians.
#' @param sunelev sun elevation in degrees
#' @param sunazimuth sun azimuth in degrees
#' @param method the topographic correction method to use. See the help for 
#' \code{topocorr} for more guidance on this.
#' @param sampleindices (optional) row-major indices of sample pixels to use in 
#' regression models used for some topographic correction methods (like 
#' Minnaert). Useful when handling very large images. See
#' \code{\link{gridsample}} for one method of calculating these indices.
#' @param scale_factor factor by which to multiply results. Useful if rounding 
#' results to integers (see \code{asinteger} argument).
#' @param asinteger whether to round results to nearest integer. Can be used to 
#' save space by saving results as, for example, an 'INT2S' \code{raster}.
#' @param DN_min minimum allowable pixel value after correction (values less 
#' than \code{DN_min} are set to NA)
#' @param DN_max maximum allowable pixel value after correction (values less 
#' than \code{DN_max} are set to NA)
#' @param ... additional arguments to pass to \code{minnaert_samp} or 
#' \code{topocorr_samp}, depending on chosen topographic correction method
#' @return The topographically corrected image as a \code{RasterLayer} or 
#' \code{RasterStack}
#' @references
#' Sarah Goslee. Analyzing Remote Sensing Data in {R}: The {landsat} Package.  
#' Journal of Statistical Software, 2011, 43:4, pg 1--25.  
#' http://www.jstatsoft.org/v43/i04/
#' @examples
#' \dontrun{
#' # Mosaic the two ASTER DEM tiles needed to a Landsat image
#' DEM_mosaic <- mosaic(ASTER_V002_EAST, ASTER_V002_WEST, fun='mean')
#' 
#' # Crop and extend the DEM mosaic to match the Landsat image
#' matched_DEM <- match_rasters(L5TSR_1986, DEM_mosaic)
#' slopeaspect <- terrain(matched_DEM, opt=c('slope', 'aspect')
#' 
#' # Apply the topographic correction
#' sunelev <- 90 - 44.97 # From metadata file
#' sunazimuth <- 124.37 # From metadata file
#' L5TSR_1986_topocorr <- topographic_corr(L5TSR_1986, slopeaspect, sunelev, 
#'                                         sunazimuth, method='minslope')
#' 
#' plotRGB(L5TSR_1986, stretch='lin', r=3, g=2, b=1)
#' plotRGB(L5TSR_1986_topocorr, stretch='lin', r=3, g=2, b=1)
#' }
topographic_corr <- function(x, slopeaspect, sunelev, sunazimuth, 
                             method='minnaert_full', sampleindices=NULL, 
                             scale_factor=1, asinteger=FALSE, DN_min=NULL, 
                             DN_max=NULL, ...) {
    if (!(class(x) %in% c('RasterLayer', 'RasterStack', 'RasterBrick'))) {
        stop('x must be a Raster* object')
    }
    if (!(class(slopeaspect) %in% c('RasterBrick', 'RasterStack'))) {
        stop('slopeaspect must be a RasterBrick or RasterStack object')
    }
    slope <- raster(slopeaspect, layer=1)
    aspect <- raster(slopeaspect, layer=2)
    stopifnot((sunelev >= 0) & (sunelev <= 90))
    stopifnot((sunazimuth >= 0) & (sunazimuth <= 360))
    # Set uncorr_layer to NULL to pass R CMD CHECK without notes
    uncorr_layer=NULL
    was_rasterlayer <- class(x) == RasterLayer
    # Convert x to stack so foreach will run
    x <- stack(x)
    corr_img <- foreach(uncorr_layer=unstack(x), .combine='addLayer', 
                        .multicombine=TRUE, .init=raster(), 
                        .packages=c('teamlucc', 'rgdal')) %dopar% {
        if (method == 'minnaert_full') {
            minnaert_data <- minnaert_samp(uncorr_layer, slope, aspect, 
                                           sunelev=sunelev, 
                                           sunazimuth=sunazimuth, 
                                           sampleindices=sampleindices, 
                                           DN_min=DN_min, DN_max=DN_max, ...)
            corr_layer <- minnaert_data$minnaert
        } else {
            corr_layer <- topocorr_samp(uncorr_layer, slope, aspect, 
                                        sunelev=sunelev, sunazimuth=sunazimuth, 
                                        method=method, 
                                        sampleindices=sampleindices,
                                        DN_min=DN_min, DN_max=DN_max, ...)
        }
    }
    if (was_rasterlayer) corr_img <- corr_img[[1]]
    names(corr_img) <- paste0(names(x), 'tc')
    if (scale_factor != 1) {
        corr_img <- corr_img * scale_factor
    }
    if (asinteger) {
        corr_img <- round(corr_img)
    }
    return(corr_img)
}
  #+END_SRC
* tpa2df.R
 #+BEGIN_SRC R 
#' Function to convert TIMESAT .tpa binary format file to an R dataframe.
#'
#' @export
#' @param x A string giving the location of a .tpa file output by 
#' TIMESAT
#' @param max_num_seasons the maximum number of seasons for any of the pixels 
#' in the file
#' @return A data.frame containing 14 columns: row, col, season, start, end, 
#' length, base_value, peak_time, peak_value, amp, left_deriv, right_deriv, 
#' large_integ, and small_integ
#' @examples
#' # TODO: Need to add examples here, and need to include a sample TIMESAT tpa 
#' # file in the package data.
tpa2df <- function(x, max_num_seasons) {
    if (missing(x) || !grepl('[.]tpa$', tolower(x))) {
        stop('must specify a .tpa file')
    }
    if (missing(max_num_seasons) || max_num_seasons < 1) {
        stop('must specify maximum number of seasons represented in tpa file')
    }
    # The number of seasonal indicators output by TIMESAT.
    NUM_SEASONAL_INDICATORS <- 11
    # Number of elements in the tpa file line header (which are normally: row, 
    # column, number of seasons).
    LINE_HEADER_SIZE <- 3
    tpa_file_obj <- file(x, rb)
    raw_vector <- readBin(tpa_file_obj, n=file.info(x)$size, raw())
    close(tpa_file_obj)
    # This function is used to track the offset within the binary vector as readBin 
    # does not track position except for file objects
    offset <- 1
    raw_vec_length <- length(raw_vector)
    offset_readBin <- function(raw_vec, what, n=n, size=size, increment_offset=TRUE, ...) {
        bin_data <- readBin(raw_vec[offset:(n * size + offset)], what, n, size, ...)
        # Use a global variable to track the offset
        if (increment_offset) {assign(offset, offset + (size*n), inherits=TRUE)}
        return(bin_data)
    }
    # File header format is: nyears nptperyear rowstart rowstop colstart colstop
    file_header <- offset_readBin(raw_vector, integer(), n=6, size=4)
    num_years <- file_header[1]
    rowstart <- file_header[3]
    rowstop <- file_header[4]
    colstart <- file_header[5]
    colstop <- file_header[6]
    num_rows <- rowstop - rowstart + 1
    num_cols <- colstop - colstart + 1
    num_pixels <- num_cols * num_rows
    tpa_data <- matrix(nrow=num_pixels*max_num_seasons,
                       ncol=(LINE_HEADER_SIZE + NUM_SEASONAL_INDICATORS))
    # Read the data and save it in the tpa_data matrix
    for (pixelnum in 1:num_pixels) {
        line_header <- offset_readBin(raw_vector, integer(), n=3, size=4)
        # Line header format is: rownum colnum numseasons
        num_seasons <- line_header[3]
        if (num_seasons > max_num_seasons) {
            stop(paste('pixel', pixelnum, 'has', num_seasons,
                       'seasons, but max_num_seasons was set to ', max_num_seasons, 
                       'seasons'))
        }
        if (num_seasons == 0) {
            # No seasons identified for this pixel - skip
            next
        }
        for (seasonnum in 1:max_num_seasons) {
            # Seasons were found for this pixel - read them
            tpa_data_row <- (pixelnum - 1)*num_seasons + seasonnum
            line_data <- offset_readBin(raw_vector, numeric(), 
                                        n=NUM_SEASONAL_INDICATORS, size=4)
            tpa_data[tpa_data_row, ] <- c(line_header[1], line_header[2], 
                                          seasonnum, line_data)
        }
    }
    tpa_data <- data.frame(tpa_data)
    tpa_data <- tpa_data[!(rowSums(is.na(tpa_data)) == ncol(tpa_data)), ]
    names(tpa_data) <- c(row, col, season, start, end, length,
                         base_value, peak_time, peak_value, amp, left_deriv,
                         right_deriv, large_integ, small_integ)
    return(tpa_data)
}
  #+END_SRC
* tpadf2raster.R
 #+BEGIN_SRC R 
#' Function to convert TIMESAT .tpa data.frame to an R raster. 
#'
#' @export
#' @param x A TPA data.frame as output by tpa2df
#' @param base_image A string giving the location of a raster file to use 
#' for georeferencing the output raster. Use one of the original raster files 
#' that was input to TIMESAT.
#' @param variable A string giving the variable name to write to a raster.  Can 
#' be one of: start, end, length, base_value, peak_time, peak_value, amp, 
#' left_deriv, right_deriv, large_integ, and small_integ.
#' @return A raster object
#' @examples
#' # TODO: Need to add examples here, and need to include a sample TIMESAT tpa 
#' # file in the package data.
tpadf2raster <- function(x, base_image, variable) {
    if (missing(x) || !is.data.frame(x)) {
        stop('must specify a tpa data.frame')
    } else if (missing(base_image) || !file.exists(base_image)) {
        stop('must specify a valid base image raster')
    }
    var_col <- grep(paste('^', variable, '$', sep=''), names(x))
    if (length(var_col) == 0) {
        stop(paste(variable, 'not found in tpa dataframe'))
    }
    base_image <- raster(base_image)
    ncol(base_image) * nrow(base_image) * 2
    seasons <- sort(unique(x$season))
    out_rasters <- c()
    for (season in sort(unique(x$season))) {
        season_data <- x[x$season == season, ]
        data_matrix <- matrix(NA, nrow(base_image), ncol(base_image))
        vector_indices <- (nrow(data_matrix) * season_data$col) - 
            (nrow(data_matrix) - season_data$row)
        data_matrix[vector_indices] <- season_data[, var_col]
        out_raster <- raster(data_matrix, template=base_image)
        out_rasters <- c(out_rasters, out_raster)
    }
    out_rasters <- stack(out_rasters)
    names(out_rasters) <- paste0('season_', seasons)
    return(out_rasters)
}
  #+END_SRC
* track_time.R
 #+BEGIN_SRC R 
#' A class for tracking running time of individual sections of an R script
#' @slot timers a \code{data.frame} tracking timer names and start times
#' @slot notify function to use for outputting timers (defaults to 
#' \code{\link{print}}
#' @import methods
#' @importFrom lubridate now
#' @export Track_time
#' @name Track_time-class
setClass('Track_time', slots=c(timers='data.frame', notify=function),
    prototype=list(timers=data.frame(label='Default', starttime=now()), 
                   notify=print)
)
#' Instantiate a new Track_time object
#'
#' Creates a new Track_time object for use in tracking and printing status the 
#' running time of processes in an R script.
#'
#' @export Track_time
#' @import methods
#' @importFrom lubridate now
#' @param notify a function to handle the string output from Track_time.  This 
#' function should accept a string as an argument. Default is the
#' \code{\link{print}} function.
#' @return Track_time object
#' @seealso \code{\link{start_timer}}, \code{\link{stop_timer}}
#' @examples
#' timer <- Track_time()
#' print(timer)
Track_time <- function(notify=print) {
    return(new('Track_time',
               timers=data.frame(label='Default', starttime=now()),
               notify=notify))
}
#' Print a Track_time object
#'
#' @export
#' @importFrom lubridate now as.duration
#' @import methods
#' @param x a Track_time object
#' @param label (optional) selects a specific tracking timer to print
#' @param ... ignored
#' @method print Track_time
print.Track_time <- function(x, label, ...) {
    timers <- x@timers
    if (!missing(label)) {
        if (!(label %in% timers$label)) {
            stop(paste0('', label, '', ' timer not defined'))
        } else {
            timers <- timers[timers$label == label, ] 
        }
    }
    for (n in 1:nrow(timers)) {
       x@notify(paste(timers$label[n], 'timer:',
                round(as.duration(now() - timers$starttime[n]), 3),
                'elapsed'))
    }
}
setMethod(show, signature(object=Track_time), function(object) print(object))
#' @importFrom lubridate now as.duration
.start_timer <- function(x, label) {
    if (!missing(label)) {
        if (label %in% x@timers$label) {
            stop(paste0('', label, '', ' timer already defined'))
        }
        x@timers <- rbind(x@timers, data.frame(label=label, starttime=now()))
        x@notify(paste0(x@timers$starttime[x@timers$label == label], ': started ', label, ''))
    } else {
        x@timers$starttime[x@timers$label == 'Default'] <- now()
        x@notify(paste0(x@timers$starttime[x@timers$label == Default], ': started'))
    }
    return(x)
}
#' Start a tracking timer
#'
#' The \code{label} is optional. If not supplied the default timer (labelled 
#' Default) will be used.
#'
#' @export start_timer
#' @param x a \code{Track_time} object
#' @param label an optional label used to maintain multiple tracking timers
#' @return Track_time object
#' @seealso \code{\link{stop_timer}}
#' @examples
#' timer <- Track_time()
#' print(timer)
#'
#' timer <- start_timer(timer, 'test')
#'
#' print(timer, 'test')
#'
#' timer <- stop_timer(timer, 'test')
#'
#' print(timer)
setGeneric(start_timer, function(x, label) {
    standardGeneric(start_timer)
})
#' @rdname start_timer
#' @aliases start_timer,Track_time-method
setMethod(start_timer, signature(x=Track_time),
    function(x) .start_timer(x)
)
#' @rdname start_timer
#' @aliases start_timer,Track_time,character-method
setMethod(start_timer, signature(x=Track_time, label=character),
    function(x, label) .start_timer(x, label)
)
#' @importFrom lubridate now as.duration
.stop_timer <- function(x, label='Default') {
    if (!(label %in% x@timers$label)) {
        stop(paste0('', label, '', ' timer not defined'))
    }
    elapsed <- as.duration(now() - x@timers$starttime[x@timers$label == label])
    if (label == 'Default') {
        # Never delete the default timer. Only reset it.
        x@timers$starttime[x@timers$label == 'Default'] <- now()
    } else {
        x@timers <- x@timers[x@timers$label != label, ] 
    }
    x@notify(paste0(now(), ': finished ', label, ' (', round(elapsed, 3),' elapsed)'))
    return(x)
}
#' Stop a tracking timer
#'
#' The \code{label} is optional. If not supplied, the default timer, labelled 
#' 'Default' will be used.
#'
#' @export stop_timer
#' @param x a \code{Track_time} object
#' @param label an optional label used to maintain multiple tracking timers
#' @seealso \code{\link{start_timer}}
#' @return Track_time object
#' @examples
#' timer <- Track_time()
#' print(timer)
#'
#' timer <- start_timer(timer, 'test')
#'
#' print(timer, 'test')
#'
#' timer <- stop_timer(timer, 'test')
#'
#' print(timer)
setGeneric(stop_timer, function(x, label='Default') {
    standardGeneric(stop_timer)
})
#' @rdname stop_timer
#' @aliases stop_timer,Track_time-method
setMethod(stop_timer, signature(x=Track_time),
    function(x) .stop_timer(x)
)
#' @rdname stop_timer
#' @aliases stop_timer,Track_time,character-method
setMethod(stop_timer, signature(x=Track_time, label=character),
    function(x, label) .stop_timer(x, label)
)
  #+END_SRC
* train_classifier.R
 #+BEGIN_SRC R 
#' Train a random forest or SVM classifier
#'
#' This function trains a Support Vector Machine (SVM) or Random Forest (RF) 
#' classifier for use in an image classification.
#'
#' For \code{type='svm'}, \code{tunegrid} must be a \code{data.frame} with two 
#' columns: .sigma and .C. For \code{type='rf'}, must be a 
#' \code{data.frame} with one column: '.mtry'.
#'
#' This function will run in parallel if a parallel backend is registered with 
#' \code{\link{foreach}}.
#'
#' @export
#' @import caret randomForest e1071 kernlab
#' @param train_data a \code{link{pixel_data}} object
#' @param type either svm (to fit a support vector machine) or rf (to fit a
#' random forest).
#' @param use_training_flag indicates whether to exclude data flagged as 
#' testing data when training the classifier. For this to work the input 
#' train_data \code{data.frame} must have a column named 'training_flag' that 
#' indicates, for each pixel, whether that pixel is a training pixel (coded as 
#' TRUE) or testing pixel (coded as FALSE).
#' @param train_control default is NULL (reasonable values will be set 
#' automatically).  For details see \code{\link{trainControl}}.
#' @param tune_grid the training grid to be used for training the classifier.  
#' See Details.
#' @param use_rfe whether to use Recursive Feature Extraction (RFE) as 
#' implemented in the \code{caret} package to select a subset of the input 
#' features to be used in the classification. NOT YET SUPPORTED.
#' @param factors a list of character vector giving the names of predictors 
#' (layer names from the images used to build \code{train_data}) that should be 
#' treated as factors, and specifying the levels of each factor. For example, 
#' \code{factors=list(year=c(1990, 1995, 2000, 2005, 2010))}.
#' @param ... additional arguments (such as \code{ntree} for random forest 
#' classifier) to pass to \code{train}
#' @return a trained model (as a \code{train} object from the \code{caret} 
#' package)
#' @examples
#' train_data <- get_pixels(L5TSR_1986, L5TSR_1986_2001_training, class_1986, 
#'                          training=.6)
#' model <- train_classifier(train_data)
train_classifier <- function(train_data, type='rf', use_training_flag=TRUE, 
                             train_control=NULL, tune_grid=NULL,
                             use_rfe=FALSE, factors=list(), ...) {
    stopifnot(type %in% c('svm', 'rf'))
    predictor_names <- names(train_data@x)
    # Convert predictors in training data to factors as necessary
    stopifnot(length(factors) == 0 || all(names(factors) %in% predictor_names))
    stopifnot(length(unique(names(factors))) == length(factors))
    for (factor_var in names(factors)) {
        pred_index <- which(predictor_names == factor_var)
        train_data@x[, pred_index] <- factor(train_data@x[, pred_index], 
                                             levels=factors[[factor_var]])
    }
    # Build the formula, excluding the training flag column (if it exists) from 
    # the model formula
    model_formula <- formula(paste('y ~', paste(predictor_names, collapse=' + ')))
    if (use_rfe) {
        stop('recursive feature extraction not yet supported')
        # This recursive feature elimination procedure follows Algorithm 19.5 
        # in Kuhn and Johnson 2013
        svmFuncs <- caretFuncs
        # First center and scale
        normalization <- preProcess(train_data@x, method='range')
        scaled_predictors <- predict(normalization, train_data@x)
        scaled_predictors <- as.data.frame(scaled_predictors)
        subsets <- c(1:length(predictor_names))
        ctrl <- rfeControl(method=repeatedcv,
                           repeats=5,
                           verbose=TRUE,
                           functions=svmFuncs)
        # For the rfe modeling, extract the training data from the main
        # train_data dataset - no need to pass the testing data to rfe
        rfe_x <- scaled_predictors[train_data@training_flag, ]
        rfe_y <- train_data@y[train_data@training_flag, ]
        rfe_res <- rfe(x=rfe_x, rfe_y,
                       sizes=subsets,
                       metric=ROC,
                       rfeControl=ctrl,
                       method=svmRadial,
                       trControl=train_control,
                       tuneGrid=tune_grid)
        #TODO: Extract best model from rfe_res
    } else {
        rfe_res <- NULL
    }
    train_data <- cbind(y=train_data@y,
                        train_data@x,
                        training_flag=train_data@training_flag,
                        poly_src=train_data@pixel_src$src,
                        poly_ID=train_data@pixel_src$ID)
    if (type == 'rf') {
        if (is.null(train_control)) {
            train_control <- trainControl(method=oob, classProbs=TRUE)
        }
        model <- train(model_formula, data=train_data, method=rf,
                       subset=train_data$training_flag,
                       trControl=train_control, tuneGrid=tune_grid, ...)
    } else if (type == 'svm') {
        if (is.null(train_control)) {
            train_control <- trainControl(method=cv, classProbs=TRUE)
        }
        model <- train(model_formula, data=train_data, method=svmRadial,
                       preProc=c('center', 'scale'), subset=train_data$training_flag,
                       trControl=train_control, tuneGrid=tune_grid, ...)
    } else {
        # should never get here
        stop(model type not recognized)
    }
    return(model)
}
  #+END_SRC
* tts2df.R
 #+BEGIN_SRC R 
#' Function to convert TIMESAT .tts binary format to an R dataframe.
#'
#' @export
#' @param x A .tts file output by TIMESAT
#' @return A data.frame containing 'row' and 'col' columns giving the the row 
#' and column of a pixel in the input image to timesat, and then a number of 
#' columns named 't1', 't2', ...'tn', where n is the total number of image 
#' dates input to TIMESAT.
#' @examples
#' # TODO: Need to add examples here, and need to include a sample TIMESAT tts 
#' # file in the package data.
tts2df <- function(x) {
    if (missing(x) || !grepl('[.]tts$', tolower(x))) {
        stop('must specify a .tts file')
    }
    # Number of elements in the tts file line header (which are normally: row, 
    # column).
    LINE_HEADER_SIZE <- 2
    tts_file_obj <- file(x, rb)
    raw_vector <- readBin(tts_file_obj, n=file.info(x)$size, raw())
    close(tts_file_obj)
    # This function is used to track the offset within the binary vector as readBin 
    # does not track position except for file objects
    offset <- 1
    raw_vec_length <- length(raw_vector)
    offset_readBin <- function(raw_vec, what, n=n, size=size, ...) {
        bin_data <- readBin(raw_vec[offset:(n * size + offset)], what, n, size, ...)
        # Be lazy and use a global variable to track the offset
        assign(offset, offset + (size*n), inherits=TRUE)
        return(bin_data)
    }
    # File header format is: nyears nptperyear rowstart rowstop colstart colstop
    file_header <- offset_readBin(raw_vector, integer(), n=6, size=4)
    num_years <- file_header[1]
    n_pts_per_year <- file_header[2]
    rowstart <- file_header[3]
    rowstop <- file_header[4]
    colstart <- file_header[5]
    colstop <- file_header[6]
    num_pixels <- (colstop - colstart) * (rowstop - rowstart)
    # Include 2 extra columns to code the row and col IDs
    tts_data <- matrix(nrow=num_pixels, ncol=(2 + n_pts_per_year * num_years))
    for (pixelnum in 1:num_pixels) {
        line_header <- offset_readBin(raw_vector, integer(), n=2, size=4)
        # Line header format is: rownum colnum
        tts_data[pixelnum, ] <- c(line_header[1], line_header[2], 
                                  offset_readBin(raw_vector, numeric(), 
                                                 n=n_pts_per_year*num_years, 
                                                 size=4))
    }
    tts_data <- data.frame(tts_data)
    names(tts_data) <- c(row, col,
                         paste('t', seq(1, n_pts_per_year*num_years), sep=''))
    return(tts_data)
}
  #+END_SRC
* ttsdf2raster.R
 #+BEGIN_SRC R 
#' Function to convert TIMESAT tts data.frame an R raster.
#'
#' @export
#' @param x A TTS data.frame as output by tts2df
#' @param base_image A string giving the location of a raster file to use 
#' for georeferencing the output raster. Use one of the original raster files 
#' that was input to TIMESAT.
#' @return A raster object
#' @examples
#' # TODO: Need to add examples here, and need to include a sample TIMESAT tpa 
#' # file in the package data.
ttsdf2raster <- function(x, base_image) {
    if (missing(x) || !is.data.frame(x)) {
        stop('must specify a tts data.frame')
    } else if (missing(base_image) || !file.exists(base_image)) {
        stop('must specify a valid base image raster')
    }
    t_cols <- grep('^t[0-9]{1,4}$', names(x))
    base_image <- raster(base_image)
    out_rasters <- c()
    for (t_col in t_cols) {
        this_time_data <- x[, t_col]
        data_matrix <- matrix(NA, nrow(base_image), ncol(base_image))
        vector_indices <- (nrow(data_matrix) * x$col) - 
            (nrow(data_matrix) - x$row)
        data_matrix[vector_indices] <- this_time_data
        out_raster <- raster(data_matrix, template=base_image)
        out_rasters <- c(out_rasters, out_raster)
    }
    out_rasters <- stack(out_rasters)
    names(out_rasters) <- names(x[t_cols])
    return(out_rasters)
}
  #+END_SRC
* unstack_ledapscdr.R
 #+BEGIN_SRC R 
#' Convert Landsat CDR images from HDF4 to GeoTIFF format
#'
#' This function converts a Landsat surface reflectance (SR) image from the 
#' Landsat Climate Data Record (CDR) archive into a series of single band 
#' images in GeoTIFF format.
#'
#' This function uses \code{gdalUtils}, which requires a local GDAL 
#' installation.  See http://trac.osgeo.org/gdal/wiki/DownloadingGdalBinaries 
#' or http://trac.osgeo.org/osgeo4w/ to download the appropriate installer for 
#' your operating system.
#'
#'
#' @export
#' @importFrom gdalUtils gdal_translate get_subdatasets
#' @importFrom tools file_path_sans_ext
#' @param x input HDF4 file
#' @param output_folder output folder (if \code{NULL}, defaults to input folder)
#' @param overwrite whether to overwrite existing files
#' @param rmhdf whether to remove hdf files after unstacking them
#' @return nothing (used for side effect of converting Landsat CDR HDF files)
#' @examples
#' \dontrun{
#' # Unstack files downloaded from CDR:
#' unstack_ledapscdr('lndsr.LT50150531986021XXX03.hdf')
#' 
#' # Unstack files downloaded from CDR, overwriting any existing files, and 
#' # deleting original HDF files after unstacking:
#' unstack_ledapscdr('lndsr.LT50150531986021XXX03.hdf', overwrite=TRUE, 
#'                   rmhdf=TRUE)
#' }
unstack_ledapscdr <- function(x, output_folder=NULL, overwrite=FALSE, 
                              rmhdf=FALSE) {
    if (is.null(output_folder)) {
        output_folder <- dirname(x)
    }
    if ((!file_test('-f', x)) | (tolower(extension(x)) != '.hdf')) {
        stop('x must be an existing file ending in .hdf')
    }
    if (!file_test('-d', output_folder)) {
        stop('output_folder must be a directory')
    }
    out_basename <- file_path_sans_ext(basename(x))
    sds <- get_subdatasets(x)
    loc <- regexpr('[a-zA-Z0-9_-]*$', sds)
    for (n in 1:length(sds)) {
        start_char <- loc[n]
        stop_char <- start_char + attr(loc, 'match.length')[n]
        band_name <- substr(sds[[n]], start_char, stop_char)
        this_out <- paste0(file.path(output_folder, out_basename), '_', band_name, '.tif')
        if (file.exists(this_out)) {
            if (overwrite) {
                unlink(this_out)
                if (file.exists(extension(this_out, 'hdr'))) 
                    unlink(extension(this_out, 'hdr'))
                if (file.exists(extension(this_out, 'tif.aux.xml'))) 
                    unlink(extension(this_out, 'tif.aux.xml'))
                if (file.exists(extension(this_out, 'tif.enp'))) 
                    unlink(extension(this_out, 'tif.enp'))
            } else {
                warning(paste(this_out, 'already exists - skipping file'))
                next
            }
        }
        out_rast <- gdal_translate(x, of=GTiff, sd_index=n, this_out, 
                                   outRaster=TRUE)
    }
    if (rmhdf) {
        if (file.exists(extension(x, 'hdf'))) 
            unlink(extension(x, 'hdf'))
        if (file.exists(extension(x, 'hdr'))) 
            unlink(extension(x, 'hdr'))
        if (file.exists(paste0(file_path_sans_ext(x), '.hdf.hdr')))
            unlink(paste0(file_path_sans_ext(x), '.hdf.hdr'))
        if (file.exists(extension(x, 'txt'))) 
            unlink(extension(x, 'txt'))
    }
}
  #+END_SRC
* utm_zone.R
 #+BEGIN_SRC R 
#' Given a spatial object, calculate the UTM zone of the centroid
#'
#' For a line or polygon, the UTM zone of the centroid is given, after 
#' reprojecting the object into WGS-84.
#'
#' Based on the code on gis.stackexchange.com at http://bit.ly/17SdcuN.
#'
#' @export utm_zone
#' @import methods
#' @param x a longitude (with western hemisphere longitudes negative), or a 
#' \code{Spatial} object
#' @param y a latitude (with southern hemisphere latitudes negative), or 
#' missing (if x is a \code{Spatial} object)
#' @param proj4string if FALSE (default) return the UTM zone as a string (for 
#' example 34S for UTM Zone 34 South). If TRUE, return a proj4string using 
#' the EPSG code as an initialization string.
#' @examples
#' utm_zone(45, 10)
#' utm_zone(45, -10)
#' utm_zone(45, 10, proj4string=TRUE)
setGeneric(utm_zone, function(x, y, proj4string=FALSE) {
    standardGeneric(utm_zone)
})
utm_zone_calc <- function(x, y, proj4string) {
    if (x < -180 || x > 180) {
        stop(longitude must be between -180 and 180)
    }
    if (y < -90 || y > 90) {
        stop(latitude must be between -90 and 90)
    }
    zone_num <- floor((x + 180)/6) + 1
    if (y >= 56.0 && y < 64.0 && x >= 3.0 && x < 12.0) {
        zone_num <- 32
    }
    # Special zone_nums for Svalbard
    if (y >= 72.0 && y < 84.0) {
        if (x >= 0.0 && x < 9.0) {
            zone_num <- 31
        } else if (x >= 9.0 && x < 21.0) {
            zone_num <- 33
        } else if (x >= 21.0 && x < 33.0) {
            zone_num <- 35
        } else if (x >= 33.0 && x < 42.0) {
            zone_num <- 37
        }
    }
    if (y >= 0) {
        ns <- 'N'
    } else {
        ns <- 'S'
    }
    if (proj4string) {
        if (ns == 'N') {
            return(paste0('+init=epsg:326', sprintf('%02i', zone_num)))
        } else {
            return(paste0('+init=epsg:327', sprintf('%02i', zone_num)))
        }
    } else {
        return(paste0(zone_num, ns))
    }
}
#' @rdname utm_zone
#' @aliases utm_zone,numeric,numeric,logical-method
setMethod(utm_zone, signature(numeric, numeric),
    function(x, y, proj4string) {
        return(utm_zone_calc(x, y, proj4string))
    }
)
#' @rdname utm_zone
#' @importFrom rgeos gCentroid
#' @importFrom sp Spatial coordinates spTransform
#' @aliases utm_zone,Spatial,missing,logical-method
setMethod(utm_zone, signature(x='Spatial', y='missing'),
    function(x, proj4string) {
        x <- spTransform(x, CRS('+init=epsg:4236'))
        centroid <- coordinates(gCentroid(x))
        return(utm_zone_calc(centroid[1], centroid[2], proj4string))
    }
)
  #+END_SRC
